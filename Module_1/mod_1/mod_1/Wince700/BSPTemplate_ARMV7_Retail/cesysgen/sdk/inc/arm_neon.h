//
// Copyright (c) Microsoft Corporation.  All rights reserved.
//
//
// Use of this sample source code is subject to the terms of the Microsoft
// license agreement under which you licensed this sample source code. If
// you did not accept the terms of the license agreement, you are not
// authorized to use this sample source code. For the terms of the license,
// please see the license agreement between you and Microsoft or, if applicable,
// see the LICENSE.RTF on your install media or the root of your tools installation.
// THE SAMPLE SOURCE CODE IS PROVIDED "AS IS", WITH NO WARRANTIES OR INDEMNITIES.
//
#pragma once


#if !defined(_M_ARM)
#error This header is specific to ARM targets
#endif


#if defined(__cplusplus)
extern "C" {
#endif
    
    
///////////////////////////////////////////////////////////////////////////////
//
#if !defined(_ADVSIMD_ALIGN)
#if defined(__midl)
#define _ADVSIMD_ALIGN(x)
#else
#define _ADVSIMD_ALIGN(x) __declspec(align(x))
#endif
#endif


///////////////////////////////////////////////////////////////////////////////
//
// ARM Advanced SIMD 64bit type
//
typedef union __declspec(intrin_type) _ADVSIMD_ALIGN(4) __n64
{
    unsigned __int64    n64_u64;
    unsigned __int32    n64_u32[2];
    unsigned __int16    n64_u16[4];
    unsigned __int8     n64_u8[8];
    __int64             n64_i64;
    __int32             n64_i32[2];
    __int16             n64_i16[4];
    __int8              n64_i8[8];
    float               n64_f32[2];
} __n64;


///////////////////////////////////////////////////////////////////////////////
//
// ARM Advanced SIMD 128bit type
//
typedef union __declspec(intrin_type) _ADVSIMD_ALIGN(4) __n128
{
     unsigned __int64   n128_u64[2];
     unsigned __int32   n128_u32[4];
     unsigned __int16   n128_u16[8];
     unsigned __int8    n128_u8[16];
     __int64            n128_i64[2];
     __int32            n128_i32[4];
     __int16            n128_i16[8];
     __int8             n128_i8[16];
     float              n128_f32[4];
    
    struct
    {
        __n64  low64;
        __n64  high64;
    };
    
} __n128;


///////////////////////////////////////////////////////////////////////////////
//
typedef __int8  int8_t;
typedef __int16 int16_t;
typedef __int32 int32_t;
typedef __int64 int64_t;

typedef unsigned __int8  uint8_t;
typedef unsigned __int16 uint16_t;
typedef unsigned __int32 uint32_t;
typedef unsigned __int64 uint64_t;

typedef unsigned __int8  poly8_t;
typedef unsigned __int16 poly16_t;

typedef float float32_t;


///////////////////////////////////////////////////////////////////////////////
//
// { +++ auto-generated code begins (explicit types)

typedef __n64    float32x2_t;
typedef __n64    int8x8_t;
typedef __n64    int16x4_t;
typedef __n64    int32x2_t;
typedef __n64    int64x1_t;
typedef __n64    poly8x8_t;
typedef __n64    poly16x4_t;
typedef __n64    uint8x8_t;
typedef __n64    uint16x4_t;
typedef __n64    uint32x2_t;
typedef __n64    uint64x1_t;
typedef __n128   float32x4_t;
typedef __n128   int8x16_t;
typedef __n128   int16x8_t;
typedef __n128   int32x4_t;
typedef __n128   int64x2_t;
typedef __n128   poly8x16_t;
typedef __n128   poly16x8_t;
typedef __n128   uint8x16_t;
typedef __n128   uint16x8_t;
typedef __n128   uint32x4_t;
typedef __n128   uint64x2_t;

// } +++ auto-generated code ends (explicit types)


///////////////////////////////////////////////////////////////////////////////
//
// { +++ auto-generated code begins (prototypes)

__n64 __neon_DdDm(unsigned int, __n64);
__n64 __neon_DdDm_acc(unsigned int, __n64, __n64);
__n64 __neon_DdDnDm(unsigned int, __n64, __n64);
__n64 __neon_DdDnDm_acc(unsigned int, __n64, __n64, __n64);
__n64 __neon_DdDnDmx(unsigned int, __n64, __n64);
__n64 __neon_DdDnDmx_acc(unsigned int, __n64, __n64, __n64);
__n64 __neon_DdFt(unsigned int, float);
__n64 __neon_DdFt_acc(unsigned int, __n64, float);
__n64 __neon_D1Adr(unsigned int, const __n64*);
__n64 __neon_D1Adr_acc(unsigned int, __n64, const __n64*);
__n64 __neon_DdQm(unsigned int, __n128);
__n64 __neon_DdQm_high(unsigned int, __n128);
__n64 __neon_DdQm_low(unsigned int, __n128);
__n64 __neon_DdQnDm(unsigned int, __n128, __n64);
__n64 __neon_DdQnDm_acc(unsigned int, __n64, __n128, __n64);
__n64 __neon_DdQnQm(unsigned int, __n128, __n128);
__n64 __neon_DdRt(unsigned int, int);
__n64 __neon_DdRtRt2(unsigned int, __int64);
__n64 __neon_DdRtRt2_acc(unsigned int, __n64, __int64);
__n64 __neon_DdRt_acc(unsigned int, __n64, int);
float __neon_FtDn(unsigned int, __n64);
float __neon_FtQn(unsigned int, __n128);
__n128 __neon_QdDm(unsigned int, __n64);
__n128 __neon_QdDnDm(unsigned int, __n64, __n64);
__n128 __neon_QdDnDm_acc(unsigned int, __n128, __n64, __n64);
__n128 __neon_QdDnDm_merge(unsigned int, __n64, __n64);
__n128 __neon_QdDnDmx(unsigned int, __n64, __n64);
__n128 __neon_QdDnDmx_acc(unsigned int, __n128, __n64, __n64);
__n128 __neon_QdFt(unsigned int, float);
__n128 __neon_QdFt_acc(unsigned int, __n128, float);
__n128 __neon_Q1Adr(unsigned int, const __n128*);
__n128 __neon_Q1Adr_acc(unsigned int, __n128, const __n128*);
__n128 __neon_QdQm(unsigned int, __n128);
__n128 __neon_QdQm_acc(unsigned int, __n128, __n128);
__n128 __neon_QdQnDm(unsigned int, __n128, __n64);
__n128 __neon_QdQnDmx(unsigned int, __n128, __n64);
__n128 __neon_QdQnDmx_acc(unsigned int, __n128, __n128, __n64);
__n128 __neon_QdQnQm(unsigned int, __n128, __n128);
__n128 __neon_QdQnQm_acc(unsigned int, __n128, __n128, __n128);
__n128 __neon_QdRt(unsigned int, int);
__n128 __neon_QdRtRt2_acc(unsigned int, __n128, __int64);
__n128 __neon_QdRtRt2_dup(unsigned int, __int64);
__n128 __neon_QdRt_acc(unsigned int, __n128, int);
__int64 __neon_RtRt2Dm(unsigned int, __n64);
__int64 __neon_RtRt2Qm(unsigned int, __n128);
int __neon_RtDn(unsigned int, __n64);
int __neon_RtQn(unsigned int, __n128);
void __neon_AdrD1(unsigned int, __n64*, __n64);
void __neon_AdrQ1(unsigned int, __n128*, __n128);

// } +++ auto-generated code ends (prototypes)


#if defined(__cplusplus)
}
#endif


///////////////////////////////////////////////////////////////////////////////
//
// VLDx/VSTx alignment specifications
//

#define _NEON_ALIGN1(a)         \
    (                           \
    ((a) == 8) ? 0 :            \
    ((a) == 64) ? 1 :           \
    -1)

#define _NEON_ALIGN2(a)         \
    (                           \
    ((a) == 8) ? 0 :            \
    ((a) == 64) ? 1 :           \
    ((a) == 128) ? 2 :          \
    -1)


///////////////////////////////////////////////////////////////////////////////
//
// { +++ auto-generated code begins (encoding macros)

#define _NENC_0(x)                               ((x) & 0x1)
#define _NENC_11_8(x)                            (((x) << 8) & 0xf00)
#define _NENC_12(x)                              (((x) << 12) & 0x1000)
#define _NENC_16(x)                              (((x) << 16) & 0x10000)
#define _NENC_19(x)                              (((x) << 19) & 0x80000)
#define _NENC_19_17(x)                           (((x) << 17) & 0xe0000)
#define _NENC_19_18(x)                           (((x) << 18) & 0xc0000)
#define _NENC_21(x)                              (((x) << 21) & 0x200000)
#define _NENC_21_16(x)                           (((x) << 16) & 0x3f0000)
#define _NENC_21x6(x)                            (((x) << 6) & 0x40 | ((x) << 20) & 0x200000)
#define _NENC_21x6_5(x)                          (((x) << 5) & 0x60 | ((x) << 19) & 0x200000)
#define _NENC_5(x)                               (((x) << 5) & 0x20)
#define _NENC_5_4(x)                             (((x) << 4) & 0x30)
#define _NENC_5x3(x)                             (((x) << 3) & 0x8 | ((x) << 4) & 0x20)
#define _NENC_7(x)                               (((x) << 7) & 0x80)
#define _NENC_7_5(x)                             (((x) << 5) & 0xe0)
#define _NENC_7_6(x)                             (((x) << 6) & 0xc0)

// } +++ auto-generated code ends (encoding macros)


///////////////////////////////////////////////////////////////////////////////
//
// { +++ auto-generated code begins (Neon macros)

// VABA, VABAL
#define vaba_s16(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf2100710, (Dd), (Dn), (Dm)) )
#define vaba_s32(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf2200710, (Dd), (Dn), (Dm)) )
#define vaba_s8(Dd, Dn, Dm)                      ( __neon_DdDnDm_acc( 0xf2000710, (Dd), (Dn), (Dm)) )
#define vaba_u16(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3100710, (Dd), (Dn), (Dm)) )
#define vaba_u32(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3200710, (Dd), (Dn), (Dm)) )
#define vaba_u8(Dd, Dn, Dm)                      ( __neon_DdDnDm_acc( 0xf3000710, (Dd), (Dn), (Dm)) )
#define vabal_s16(Qd, Dn, Dm)                    ( __neon_QdDnDm_acc( 0xf2900500, (Qd), (Dn), (Dm)) )
#define vabal_s32(Qd, Dn, Dm)                    ( __neon_QdDnDm_acc( 0xf2a00500, (Qd), (Dn), (Dm)) )
#define vabal_s8(Qd, Dn, Dm)                     ( __neon_QdDnDm_acc( 0xf2800500, (Qd), (Dn), (Dm)) )
#define vabal_u16(Qd, Dn, Dm)                    ( __neon_QdDnDm_acc( 0xf3900500, (Qd), (Dn), (Dm)) )
#define vabal_u32(Qd, Dn, Dm)                    ( __neon_QdDnDm_acc( 0xf3a00500, (Qd), (Dn), (Dm)) )
#define vabal_u8(Qd, Dn, Dm)                     ( __neon_QdDnDm_acc( 0xf3800500, (Qd), (Dn), (Dm)) )
#define vabaq_s16(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf2100750, (Qd), (Qn), (Qm)) )
#define vabaq_s32(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf2200750, (Qd), (Qn), (Qm)) )
#define vabaq_s8(Qd, Qn, Qm)                     ( __neon_QdQnQm_acc( 0xf2000750, (Qd), (Qn), (Qm)) )
#define vabaq_u16(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf3100750, (Qd), (Qn), (Qm)) )
#define vabaq_u32(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf3200750, (Qd), (Qn), (Qm)) )
#define vabaq_u8(Qd, Qn, Qm)                     ( __neon_QdQnQm_acc( 0xf3000750, (Qd), (Qn), (Qm)) )

// VABD (floating point)
#define vabd_f32(Dn, Dm)                         ( __neon_DdDnDm( 0xf3200d00, (Dn), (Dm)) )
#define vabdq_f32(Qn, Qm)                        ( __neon_QdQnQm( 0xf3200d40, (Qn), (Qm)) )

// VABD[L] (integer)
#define vabd_s16(Dn, Dm)                         ( __neon_DdDnDm( 0xf2100700, (Dn), (Dm)) )
#define vabd_s32(Dn, Dm)                         ( __neon_DdDnDm( 0xf2200700, (Dn), (Dm)) )
#define vabd_s8(Dn, Dm)                          ( __neon_DdDnDm( 0xf2000700, (Dn), (Dm)) )
#define vabd_u16(Dn, Dm)                         ( __neon_DdDnDm( 0xf3100700, (Dn), (Dm)) )
#define vabd_u32(Dn, Dm)                         ( __neon_DdDnDm( 0xf3200700, (Dn), (Dm)) )
#define vabd_u8(Dn, Dm)                          ( __neon_DdDnDm( 0xf3000700, (Dn), (Dm)) )
#define vabdl_s16(Dn, Dm)                        ( __neon_QdDnDm( 0xf2900700, (Dn), (Dm)) )
#define vabdl_s32(Dn, Dm)                        ( __neon_QdDnDm( 0xf2a00700, (Dn), (Dm)) )
#define vabdl_s8(Dn, Dm)                         ( __neon_QdDnDm( 0xf2800700, (Dn), (Dm)) )
#define vabdl_u16(Dn, Dm)                        ( __neon_QdDnDm( 0xf3900700, (Dn), (Dm)) )
#define vabdl_u32(Dn, Dm)                        ( __neon_QdDnDm( 0xf3a00700, (Dn), (Dm)) )
#define vabdl_u8(Dn, Dm)                         ( __neon_QdDnDm( 0xf3800700, (Dn), (Dm)) )
#define vabdq_s16(Qn, Qm)                        ( __neon_QdQnQm( 0xf2100740, (Qn), (Qm)) )
#define vabdq_s32(Qn, Qm)                        ( __neon_QdQnQm( 0xf2200740, (Qn), (Qm)) )
#define vabdq_s8(Qn, Qm)                         ( __neon_QdQnQm( 0xf2000740, (Qn), (Qm)) )
#define vabdq_u16(Qn, Qm)                        ( __neon_QdQnQm( 0xf3100740, (Qn), (Qm)) )
#define vabdq_u32(Qn, Qm)                        ( __neon_QdQnQm( 0xf3200740, (Qn), (Qm)) )
#define vabdq_u8(Qn, Qm)                         ( __neon_QdQnQm( 0xf3000740, (Qn), (Qm)) )

// VABS, VNEG
#define vabs_f32(Dm)                             ( __neon_DdDm( 0xf3b90700, (Dm)) )
#define vabs_s16(Dm)                             ( __neon_DdDm( 0xf3b50300, (Dm)) )
#define vabs_s32(Dm)                             ( __neon_DdDm( 0xf3b90300, (Dm)) )
#define vabs_s8(Dm)                              ( __neon_DdDm( 0xf3b10300, (Dm)) )
#define vneg_f32(Dm)                             ( __neon_DdDm( 0xf3b90780, (Dm)) )
#define vneg_s16(Dm)                             ( __neon_DdDm( 0xf3b50380, (Dm)) )
#define vneg_s32(Dm)                             ( __neon_DdDm( 0xf3b90380, (Dm)) )
#define vneg_s8(Dm)                              ( __neon_DdDm( 0xf3b10380, (Dm)) )
#define vabsq_f32(Qm)                            ( __neon_QdQm( 0xf3b90740, (Qm)) )
#define vabsq_s16(Qm)                            ( __neon_QdQm( 0xf3b50340, (Qm)) )
#define vabsq_s32(Qm)                            ( __neon_QdQm( 0xf3b90340, (Qm)) )
#define vabsq_s8(Qm)                             ( __neon_QdQm( 0xf3b10340, (Qm)) )
#define vnegq_f32(Qm)                            ( __neon_QdQm( 0xf3b907c0, (Qm)) )
#define vnegq_s16(Qm)                            ( __neon_QdQm( 0xf3b503c0, (Qm)) )
#define vnegq_s32(Qm)                            ( __neon_QdQm( 0xf3b903c0, (Qm)) )
#define vnegq_s8(Qm)                             ( __neon_QdQm( 0xf3b103c0, (Qm)) )

// VACGE, VACGT, VACLE, VACLT
#define vcage_f32(Dn, Dm)                        ( __neon_DdDnDm( 0xf3000e10, (Dn), (Dm)) )
#define vcagt_f32(Dn, Dm)                        ( __neon_DdDnDm( 0xf3200e10, (Dn), (Dm)) )
#define vcale_f32(Dn, Dm)                        ( __neon_DdDnDm( 0xf3000e10, (Dm), (Dn)) )
#define vcalt_f32(Dn, Dm)                        ( __neon_DdDnDm( 0xf3200e10, (Dm), (Dn)) )
#define vcageq_f32(Qn, Qm)                       ( __neon_QdQnQm( 0xf3000e50, (Qn), (Qm)) )
#define vcagtq_f32(Qn, Qm)                       ( __neon_QdQnQm( 0xf3200e50, (Qn), (Qm)) )
#define vcaleq_f32(Qn, Qm)                       ( __neon_QdQnQm( 0xf3000e50, (Qm), (Qn)) )
#define vcaltq_f32(Qn, Qm)                       ( __neon_QdQnQm( 0xf3200e50, (Qm), (Qn)) )

// VADD
#define vadd_f32(Dn, Dm)                         ( __neon_DdDnDm( 0xf2000d00, (Dn), (Dm)) )
#define vadd_s16(Dn, Dm)                         ( __neon_DdDnDm( 0xf2100800, (Dn), (Dm)) )
#define vadd_s32(Dn, Dm)                         ( __neon_DdDnDm( 0xf2200800, (Dn), (Dm)) )
#define vadd_s64(Dn, Dm)                         ( __neon_DdDnDm( 0xf2300800, (Dn), (Dm)) )
#define vadd_s8(Dn, Dm)                          ( __neon_DdDnDm( 0xf2000800, (Dn), (Dm)) )
#define vadd_u16(Dn, Dm)                         ( __neon_DdDnDm( 0xf2100800, (Dn), (Dm)) )
#define vadd_u32(Dn, Dm)                         ( __neon_DdDnDm( 0xf2200800, (Dn), (Dm)) )
#define vadd_u64(Dn, Dm)                         ( __neon_DdDnDm( 0xf2300800, (Dn), (Dm)) )
#define vadd_u8(Dn, Dm)                          ( __neon_DdDnDm( 0xf2000800, (Dn), (Dm)) )
#define vaddq_f32(Qn, Qm)                        ( __neon_QdQnQm( 0xf2000d40, (Qn), (Qm)) )
#define vaddq_s16(Qn, Qm)                        ( __neon_QdQnQm( 0xf2100840, (Qn), (Qm)) )
#define vaddq_s32(Qn, Qm)                        ( __neon_QdQnQm( 0xf2200840, (Qn), (Qm)) )
#define vaddq_s64(Qn, Qm)                        ( __neon_QdQnQm( 0xf2300840, (Qn), (Qm)) )
#define vaddq_s8(Qn, Qm)                         ( __neon_QdQnQm( 0xf2000840, (Qn), (Qm)) )
#define vaddq_u16(Qn, Qm)                        ( __neon_QdQnQm( 0xf2100840, (Qn), (Qm)) )
#define vaddq_u32(Qn, Qm)                        ( __neon_QdQnQm( 0xf2200840, (Qn), (Qm)) )
#define vaddq_u64(Qn, Qm)                        ( __neon_QdQnQm( 0xf2300840, (Qn), (Qm)) )
#define vaddq_u8(Qn, Qm)                         ( __neon_QdQnQm( 0xf2000840, (Qn), (Qm)) )

// VADDHN, VRADDHN
#define vaddhn_s16(Qn, Qm)                       ( __neon_DdQnQm( 0xf2800400, (Qn), (Qm)) )
#define vaddhn_s32(Qn, Qm)                       ( __neon_DdQnQm( 0xf2900400, (Qn), (Qm)) )
#define vaddhn_s64(Qn, Qm)                       ( __neon_DdQnQm( 0xf2a00400, (Qn), (Qm)) )
#define vaddhn_u16(Qn, Qm)                       ( __neon_DdQnQm( 0xf2800400, (Qn), (Qm)) )
#define vaddhn_u32(Qn, Qm)                       ( __neon_DdQnQm( 0xf2900400, (Qn), (Qm)) )
#define vaddhn_u64(Qn, Qm)                       ( __neon_DdQnQm( 0xf2a00400, (Qn), (Qm)) )
#define vraddhn_s16(Qn, Qm)                      ( __neon_DdQnQm( 0xf3800400, (Qn), (Qm)) )
#define vraddhn_s32(Qn, Qm)                      ( __neon_DdQnQm( 0xf3900400, (Qn), (Qm)) )
#define vraddhn_s64(Qn, Qm)                      ( __neon_DdQnQm( 0xf3a00400, (Qn), (Qm)) )
#define vraddhn_u16(Qn, Qm)                      ( __neon_DdQnQm( 0xf3800400, (Qn), (Qm)) )
#define vraddhn_u32(Qn, Qm)                      ( __neon_DdQnQm( 0xf3900400, (Qn), (Qm)) )
#define vraddhn_u64(Qn, Qm)                      ( __neon_DdQnQm( 0xf3a00400, (Qn), (Qm)) )

// VADDL, VADDW
#define vaddl_s16(Dn, Dm)                        ( __neon_QdDnDm( 0xf2900000, (Dn), (Dm)) )
#define vaddl_s32(Dn, Dm)                        ( __neon_QdDnDm( 0xf2a00000, (Dn), (Dm)) )
#define vaddl_s8(Dn, Dm)                         ( __neon_QdDnDm( 0xf2800000, (Dn), (Dm)) )
#define vaddl_u16(Dn, Dm)                        ( __neon_QdDnDm( 0xf3900000, (Dn), (Dm)) )
#define vaddl_u32(Dn, Dm)                        ( __neon_QdDnDm( 0xf3a00000, (Dn), (Dm)) )
#define vaddl_u8(Dn, Dm)                         ( __neon_QdDnDm( 0xf3800000, (Dn), (Dm)) )
#define vaddw_s16(Qn, Dm)                        ( __neon_QdQnDm( 0xf2900100, (Qn), (Dm)) )
#define vaddw_s32(Qn, Dm)                        ( __neon_QdQnDm( 0xf2a00100, (Qn), (Dm)) )
#define vaddw_s8(Qn, Dm)                         ( __neon_QdQnDm( 0xf2800100, (Qn), (Dm)) )
#define vaddw_u16(Qn, Dm)                        ( __neon_QdQnDm( 0xf3900100, (Qn), (Dm)) )
#define vaddw_u32(Qn, Dm)                        ( __neon_QdQnDm( 0xf3a00100, (Qn), (Dm)) )
#define vaddw_u8(Qn, Dm)                         ( __neon_QdQnDm( 0xf3800100, (Qn), (Dm)) )

// VAND, VORR
#define vand_s16(Dn, Dm)                         ( __neon_DdDnDm( 0xf2000110, (Dn), (Dm)) )
#define vand_s32(Dn, Dm)                         ( __neon_DdDnDm( 0xf2000110, (Dn), (Dm)) )
#define vand_s64(Dn, Dm)                         ( __neon_DdDnDm( 0xf2000110, (Dn), (Dm)) )
#define vand_s8(Dn, Dm)                          ( __neon_DdDnDm( 0xf2000110, (Dn), (Dm)) )
#define vand_u16(Dn, Dm)                         ( __neon_DdDnDm( 0xf2000110, (Dn), (Dm)) )
#define vand_u32(Dn, Dm)                         ( __neon_DdDnDm( 0xf2000110, (Dn), (Dm)) )
#define vand_u64(Dn, Dm)                         ( __neon_DdDnDm( 0xf2000110, (Dn), (Dm)) )
#define vand_u8(Dn, Dm)                          ( __neon_DdDnDm( 0xf2000110, (Dn), (Dm)) )
#define vorr_s16(Dn, Dm)                         ( __neon_DdDnDm( 0xf2200110, (Dn), (Dm)) )
#define vorr_s32(Dn, Dm)                         ( __neon_DdDnDm( 0xf2200110, (Dn), (Dm)) )
#define vorr_s64(Dn, Dm)                         ( __neon_DdDnDm( 0xf2200110, (Dn), (Dm)) )
#define vorr_s8(Dn, Dm)                          ( __neon_DdDnDm( 0xf2200110, (Dn), (Dm)) )
#define vorr_u16(Dn, Dm)                         ( __neon_DdDnDm( 0xf2200110, (Dn), (Dm)) )
#define vorr_u32(Dn, Dm)                         ( __neon_DdDnDm( 0xf2200110, (Dn), (Dm)) )
#define vorr_u64(Dn, Dm)                         ( __neon_DdDnDm( 0xf2200110, (Dn), (Dm)) )
#define vorr_u8(Dn, Dm)                          ( __neon_DdDnDm( 0xf2200110, (Dn), (Dm)) )
#define vandq_s16(Qn, Qm)                        ( __neon_QdQnQm( 0xf2000150, (Qn), (Qm)) )
#define vandq_s32(Qn, Qm)                        ( __neon_QdQnQm( 0xf2000150, (Qn), (Qm)) )
#define vandq_s64(Qn, Qm)                        ( __neon_QdQnQm( 0xf2000150, (Qn), (Qm)) )
#define vandq_s8(Qn, Qm)                         ( __neon_QdQnQm( 0xf2000150, (Qn), (Qm)) )
#define vandq_u16(Qn, Qm)                        ( __neon_QdQnQm( 0xf2000150, (Qn), (Qm)) )
#define vandq_u32(Qn, Qm)                        ( __neon_QdQnQm( 0xf2000150, (Qn), (Qm)) )
#define vandq_u64(Qn, Qm)                        ( __neon_QdQnQm( 0xf2000150, (Qn), (Qm)) )
#define vandq_u8(Qn, Qm)                         ( __neon_QdQnQm( 0xf2000150, (Qn), (Qm)) )
#define vorrq_s16(Qn, Qm)                        ( __neon_QdQnQm( 0xf2200150, (Qn), (Qm)) )
#define vorrq_s32(Qn, Qm)                        ( __neon_QdQnQm( 0xf2200150, (Qn), (Qm)) )
#define vorrq_s64(Qn, Qm)                        ( __neon_QdQnQm( 0xf2200150, (Qn), (Qm)) )
#define vorrq_s8(Qn, Qm)                         ( __neon_QdQnQm( 0xf2200150, (Qn), (Qm)) )
#define vorrq_u16(Qn, Qm)                        ( __neon_QdQnQm( 0xf2200150, (Qn), (Qm)) )
#define vorrq_u32(Qn, Qm)                        ( __neon_QdQnQm( 0xf2200150, (Qn), (Qm)) )
#define vorrq_u64(Qn, Qm)                        ( __neon_QdQnQm( 0xf2200150, (Qn), (Qm)) )
#define vorrq_u8(Qn, Qm)                         ( __neon_QdQnQm( 0xf2200150, (Qn), (Qm)) )

// VBIF, VBIT, VBSL
#define vbif_f32(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3300110, (Dd), (Dn), (Dm)) )
#define vbif_p16(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3300110, (Dd), (Dn), (Dm)) )
#define vbif_p8(Dd, Dn, Dm)                      ( __neon_DdDnDm_acc( 0xf3300110, (Dd), (Dn), (Dm)) )
#define vbif_s16(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3300110, (Dd), (Dn), (Dm)) )
#define vbif_s32(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3300110, (Dd), (Dn), (Dm)) )
#define vbif_s64(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3300110, (Dd), (Dn), (Dm)) )
#define vbif_s8(Dd, Dn, Dm)                      ( __neon_DdDnDm_acc( 0xf3300110, (Dd), (Dn), (Dm)) )
#define vbif_u16(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3300110, (Dd), (Dn), (Dm)) )
#define vbif_u32(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3300110, (Dd), (Dn), (Dm)) )
#define vbif_u64(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3300110, (Dd), (Dn), (Dm)) )
#define vbif_u8(Dd, Dn, Dm)                      ( __neon_DdDnDm_acc( 0xf3300110, (Dd), (Dn), (Dm)) )
#define vbit_f32(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3200110, (Dd), (Dn), (Dm)) )
#define vbit_p16(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3200110, (Dd), (Dn), (Dm)) )
#define vbit_p8(Dd, Dn, Dm)                      ( __neon_DdDnDm_acc( 0xf3200110, (Dd), (Dn), (Dm)) )
#define vbit_s16(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3200110, (Dd), (Dn), (Dm)) )
#define vbit_s32(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3200110, (Dd), (Dn), (Dm)) )
#define vbit_s64(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3200110, (Dd), (Dn), (Dm)) )
#define vbit_s8(Dd, Dn, Dm)                      ( __neon_DdDnDm_acc( 0xf3200110, (Dd), (Dn), (Dm)) )
#define vbit_u16(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3200110, (Dd), (Dn), (Dm)) )
#define vbit_u32(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3200110, (Dd), (Dn), (Dm)) )
#define vbit_u64(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3200110, (Dd), (Dn), (Dm)) )
#define vbit_u8(Dd, Dn, Dm)                      ( __neon_DdDnDm_acc( 0xf3200110, (Dd), (Dn), (Dm)) )
#define vbsl_f32(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3100110, (Dd), (Dn), (Dm)) )
#define vbsl_p16(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3100110, (Dd), (Dn), (Dm)) )
#define vbsl_p8(Dd, Dn, Dm)                      ( __neon_DdDnDm_acc( 0xf3100110, (Dd), (Dn), (Dm)) )
#define vbsl_s16(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3100110, (Dd), (Dn), (Dm)) )
#define vbsl_s32(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3100110, (Dd), (Dn), (Dm)) )
#define vbsl_s64(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3100110, (Dd), (Dn), (Dm)) )
#define vbsl_s8(Dd, Dn, Dm)                      ( __neon_DdDnDm_acc( 0xf3100110, (Dd), (Dn), (Dm)) )
#define vbsl_u16(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3100110, (Dd), (Dn), (Dm)) )
#define vbsl_u32(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3100110, (Dd), (Dn), (Dm)) )
#define vbsl_u64(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3100110, (Dd), (Dn), (Dm)) )
#define vbsl_u8(Dd, Dn, Dm)                      ( __neon_DdDnDm_acc( 0xf3100110, (Dd), (Dn), (Dm)) )
#define vbifq_f32(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf3300150, (Qd), (Qn), (Qm)) )
#define vbifq_p16(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf3300150, (Qd), (Qn), (Qm)) )
#define vbifq_p8(Qd, Qn, Qm)                     ( __neon_QdQnQm_acc( 0xf3300150, (Qd), (Qn), (Qm)) )
#define vbifq_s16(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf3300150, (Qd), (Qn), (Qm)) )
#define vbifq_s32(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf3300150, (Qd), (Qn), (Qm)) )
#define vbifq_s64(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf3300150, (Qd), (Qn), (Qm)) )
#define vbifq_s8(Qd, Qn, Qm)                     ( __neon_QdQnQm_acc( 0xf3300150, (Qd), (Qn), (Qm)) )
#define vbifq_u16(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf3300150, (Qd), (Qn), (Qm)) )
#define vbifq_u32(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf3300150, (Qd), (Qn), (Qm)) )
#define vbifq_u64(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf3300150, (Qd), (Qn), (Qm)) )
#define vbifq_u8(Qd, Qn, Qm)                     ( __neon_QdQnQm_acc( 0xf3300150, (Qd), (Qn), (Qm)) )
#define vbitq_f32(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf3200150, (Qd), (Qn), (Qm)) )
#define vbitq_p16(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf3200150, (Qd), (Qn), (Qm)) )
#define vbitq_p8(Qd, Qn, Qm)                     ( __neon_QdQnQm_acc( 0xf3200150, (Qd), (Qn), (Qm)) )
#define vbitq_s16(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf3200150, (Qd), (Qn), (Qm)) )
#define vbitq_s32(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf3200150, (Qd), (Qn), (Qm)) )
#define vbitq_s64(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf3200150, (Qd), (Qn), (Qm)) )
#define vbitq_s8(Qd, Qn, Qm)                     ( __neon_QdQnQm_acc( 0xf3200150, (Qd), (Qn), (Qm)) )
#define vbitq_u16(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf3200150, (Qd), (Qn), (Qm)) )
#define vbitq_u32(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf3200150, (Qd), (Qn), (Qm)) )
#define vbitq_u64(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf3200150, (Qd), (Qn), (Qm)) )
#define vbitq_u8(Qd, Qn, Qm)                     ( __neon_QdQnQm_acc( 0xf3200150, (Qd), (Qn), (Qm)) )
#define vbslq_f32(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf3100150, (Qd), (Qn), (Qm)) )
#define vbslq_p16(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf3100150, (Qd), (Qn), (Qm)) )
#define vbslq_p8(Qd, Qn, Qm)                     ( __neon_QdQnQm_acc( 0xf3100150, (Qd), (Qn), (Qm)) )
#define vbslq_s16(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf3100150, (Qd), (Qn), (Qm)) )
#define vbslq_s32(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf3100150, (Qd), (Qn), (Qm)) )
#define vbslq_s64(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf3100150, (Qd), (Qn), (Qm)) )
#define vbslq_s8(Qd, Qn, Qm)                     ( __neon_QdQnQm_acc( 0xf3100150, (Qd), (Qn), (Qm)) )
#define vbslq_u16(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf3100150, (Qd), (Qn), (Qm)) )
#define vbslq_u32(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf3100150, (Qd), (Qn), (Qm)) )
#define vbslq_u64(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf3100150, (Qd), (Qn), (Qm)) )
#define vbslq_u8(Qd, Qn, Qm)                     ( __neon_QdQnQm_acc( 0xf3100150, (Qd), (Qn), (Qm)) )

// VCEQ (register)
#define vceq_f32(Dn, Dm)                         ( __neon_DdDnDm( 0xf2000e00, (Dn), (Dm)) )
#define vceq_p8(Dn, Dm)                          ( __neon_DdDnDm( 0xf3000810, (Dn), (Dm)) )
#define vceq_s16(Dn, Dm)                         ( __neon_DdDnDm( 0xf3100810, (Dn), (Dm)) )
#define vceq_s32(Dn, Dm)                         ( __neon_DdDnDm( 0xf3200810, (Dn), (Dm)) )
#define vceq_s8(Dn, Dm)                          ( __neon_DdDnDm( 0xf3000810, (Dn), (Dm)) )
#define vceq_u16(Dn, Dm)                         ( __neon_DdDnDm( 0xf3100810, (Dn), (Dm)) )
#define vceq_u32(Dn, Dm)                         ( __neon_DdDnDm( 0xf3200810, (Dn), (Dm)) )
#define vceq_u8(Dn, Dm)                          ( __neon_DdDnDm( 0xf3000810, (Dn), (Dm)) )
#define vceqq_f32(Qn, Qm)                        ( __neon_QdQnQm( 0xf2000e40, (Qn), (Qm)) )
#define vceqq_p8(Qn, Qm)                         ( __neon_QdQnQm( 0xf3000850, (Qn), (Qm)) )
#define vceqq_s16(Qn, Qm)                        ( __neon_QdQnQm( 0xf3100850, (Qn), (Qm)) )
#define vceqq_s32(Qn, Qm)                        ( __neon_QdQnQm( 0xf3200850, (Qn), (Qm)) )
#define vceqq_s8(Qn, Qm)                         ( __neon_QdQnQm( 0xf3000850, (Qn), (Qm)) )
#define vceqq_u16(Qn, Qm)                        ( __neon_QdQnQm( 0xf3100850, (Qn), (Qm)) )
#define vceqq_u32(Qn, Qm)                        ( __neon_QdQnQm( 0xf3200850, (Qn), (Qm)) )
#define vceqq_u8(Qn, Qm)                         ( __neon_QdQnQm( 0xf3000850, (Qn), (Qm)) )

// VCGE, VCLE (register)
#define vcge_f32(Dn, Dm)                         ( __neon_DdDnDm( 0xf3000e00, (Dn), (Dm)) )
#define vcge_s16(Dn, Dm)                         ( __neon_DdDnDm( 0xf2100310, (Dn), (Dm)) )
#define vcge_s32(Dn, Dm)                         ( __neon_DdDnDm( 0xf2200310, (Dn), (Dm)) )
#define vcge_s8(Dn, Dm)                          ( __neon_DdDnDm( 0xf2000310, (Dn), (Dm)) )
#define vcge_u16(Dn, Dm)                         ( __neon_DdDnDm( 0xf3100310, (Dn), (Dm)) )
#define vcge_u32(Dn, Dm)                         ( __neon_DdDnDm( 0xf3200310, (Dn), (Dm)) )
#define vcge_u8(Dn, Dm)                          ( __neon_DdDnDm( 0xf3000310, (Dn), (Dm)) )
#define vcle_f32(Dn, Dm)                         ( __neon_DdDnDm( 0xf3000e00, (Dm), (Dn)) )
#define vcle_s16(Dn, Dm)                         ( __neon_DdDnDm( 0xf2100310, (Dm), (Dn)) )
#define vcle_s32(Dn, Dm)                         ( __neon_DdDnDm( 0xf2200310, (Dm), (Dn)) )
#define vcle_s8(Dn, Dm)                          ( __neon_DdDnDm( 0xf2000310, (Dm), (Dn)) )
#define vcle_u16(Dn, Dm)                         ( __neon_DdDnDm( 0xf3100310, (Dm), (Dn)) )
#define vcle_u32(Dn, Dm)                         ( __neon_DdDnDm( 0xf3200310, (Dm), (Dn)) )
#define vcle_u8(Dn, Dm)                          ( __neon_DdDnDm( 0xf3000310, (Dm), (Dn)) )
#define vcgeq_f32(Qn, Qm)                        ( __neon_QdQnQm( 0xf3000e40, (Qn), (Qm)) )
#define vcgeq_s16(Qn, Qm)                        ( __neon_QdQnQm( 0xf2100350, (Qn), (Qm)) )
#define vcgeq_s32(Qn, Qm)                        ( __neon_QdQnQm( 0xf2200350, (Qn), (Qm)) )
#define vcgeq_s8(Qn, Qm)                         ( __neon_QdQnQm( 0xf2000350, (Qn), (Qm)) )
#define vcgeq_u16(Qn, Qm)                        ( __neon_QdQnQm( 0xf3100350, (Qn), (Qm)) )
#define vcgeq_u32(Qn, Qm)                        ( __neon_QdQnQm( 0xf3200350, (Qn), (Qm)) )
#define vcgeq_u8(Qn, Qm)                         ( __neon_QdQnQm( 0xf3000350, (Qn), (Qm)) )
#define vcleq_f32(Qn, Qm)                        ( __neon_QdQnQm( 0xf3000e40, (Qm), (Qn)) )
#define vcleq_s16(Qn, Qm)                        ( __neon_QdQnQm( 0xf2100350, (Qm), (Qn)) )
#define vcleq_s32(Qn, Qm)                        ( __neon_QdQnQm( 0xf2200350, (Qm), (Qn)) )
#define vcleq_s8(Qn, Qm)                         ( __neon_QdQnQm( 0xf2000350, (Qm), (Qn)) )
#define vcleq_u16(Qn, Qm)                        ( __neon_QdQnQm( 0xf3100350, (Qm), (Qn)) )
#define vcleq_u32(Qn, Qm)                        ( __neon_QdQnQm( 0xf3200350, (Qm), (Qn)) )
#define vcleq_u8(Qn, Qm)                         ( __neon_QdQnQm( 0xf3000350, (Qm), (Qn)) )

// VCGT, VCLT (register)
#define vcgt_f32(Dn, Dm)                         ( __neon_DdDnDm( 0xf3200e00, (Dn), (Dm)) )
#define vcgt_s16(Dn, Dm)                         ( __neon_DdDnDm( 0xf2100300, (Dn), (Dm)) )
#define vcgt_s32(Dn, Dm)                         ( __neon_DdDnDm( 0xf2200300, (Dn), (Dm)) )
#define vcgt_s8(Dn, Dm)                          ( __neon_DdDnDm( 0xf2000300, (Dn), (Dm)) )
#define vcgt_u16(Dn, Dm)                         ( __neon_DdDnDm( 0xf3100300, (Dn), (Dm)) )
#define vcgt_u32(Dn, Dm)                         ( __neon_DdDnDm( 0xf3200300, (Dn), (Dm)) )
#define vcgt_u8(Dn, Dm)                          ( __neon_DdDnDm( 0xf3000300, (Dn), (Dm)) )
#define vclt_f32(Dn, Dm)                         ( __neon_DdDnDm( 0xf3200e00, (Dm), (Dn)) )
#define vclt_s16(Dn, Dm)                         ( __neon_DdDnDm( 0xf2100300, (Dm), (Dn)) )
#define vclt_s32(Dn, Dm)                         ( __neon_DdDnDm( 0xf2200300, (Dm), (Dn)) )
#define vclt_s8(Dn, Dm)                          ( __neon_DdDnDm( 0xf2000300, (Dm), (Dn)) )
#define vclt_u16(Dn, Dm)                         ( __neon_DdDnDm( 0xf3100300, (Dm), (Dn)) )
#define vclt_u32(Dn, Dm)                         ( __neon_DdDnDm( 0xf3200300, (Dm), (Dn)) )
#define vclt_u8(Dn, Dm)                          ( __neon_DdDnDm( 0xf3000300, (Dm), (Dn)) )
#define vcgtq_f32(Qn, Qm)                        ( __neon_QdQnQm( 0xf3200e40, (Qn), (Qm)) )
#define vcgtq_s16(Qn, Qm)                        ( __neon_QdQnQm( 0xf2100340, (Qn), (Qm)) )
#define vcgtq_s32(Qn, Qm)                        ( __neon_QdQnQm( 0xf2200340, (Qn), (Qm)) )
#define vcgtq_s8(Qn, Qm)                         ( __neon_QdQnQm( 0xf2000340, (Qn), (Qm)) )
#define vcgtq_u16(Qn, Qm)                        ( __neon_QdQnQm( 0xf3100340, (Qn), (Qm)) )
#define vcgtq_u32(Qn, Qm)                        ( __neon_QdQnQm( 0xf3200340, (Qn), (Qm)) )
#define vcgtq_u8(Qn, Qm)                         ( __neon_QdQnQm( 0xf3000340, (Qn), (Qm)) )
#define vcltq_f32(Qn, Qm)                        ( __neon_QdQnQm( 0xf3200e40, (Qm), (Qn)) )
#define vcltq_s16(Qn, Qm)                        ( __neon_QdQnQm( 0xf2100340, (Qm), (Qn)) )
#define vcltq_s32(Qn, Qm)                        ( __neon_QdQnQm( 0xf2200340, (Qm), (Qn)) )
#define vcltq_s8(Qn, Qm)                         ( __neon_QdQnQm( 0xf2000340, (Qm), (Qn)) )
#define vcltq_u16(Qn, Qm)                        ( __neon_QdQnQm( 0xf3100340, (Qm), (Qn)) )
#define vcltq_u32(Qn, Qm)                        ( __neon_QdQnQm( 0xf3200340, (Qm), (Qn)) )
#define vcltq_u8(Qn, Qm)                         ( __neon_QdQnQm( 0xf3000340, (Qm), (Qn)) )

// VCLS, VCLZ
#define vcls_s16(Dm)                             ( __neon_DdDm( 0xf3b40400, (Dm)) )
#define vcls_s32(Dm)                             ( __neon_DdDm( 0xf3b80400, (Dm)) )
#define vcls_s8(Dm)                              ( __neon_DdDm( 0xf3b00400, (Dm)) )
#define vclz_s16(Dm)                             ( __neon_DdDm( 0xf3b40480, (Dm)) )
#define vclz_s32(Dm)                             ( __neon_DdDm( 0xf3b80480, (Dm)) )
#define vclz_s8(Dm)                              ( __neon_DdDm( 0xf3b00480, (Dm)) )
#define vclz_u16(Dm)                             ( __neon_DdDm( 0xf3b40480, (Dm)) )
#define vclz_u32(Dm)                             ( __neon_DdDm( 0xf3b80480, (Dm)) )
#define vclz_u8(Dm)                              ( __neon_DdDm( 0xf3b00480, (Dm)) )
#define vclsq_s16(Qm)                            ( __neon_QdQm( 0xf3b40440, (Qm)) )
#define vclsq_s32(Qm)                            ( __neon_QdQm( 0xf3b80440, (Qm)) )
#define vclsq_s8(Qm)                             ( __neon_QdQm( 0xf3b00440, (Qm)) )
#define vclzq_s16(Qm)                            ( __neon_QdQm( 0xf3b404c0, (Qm)) )
#define vclzq_s32(Qm)                            ( __neon_QdQm( 0xf3b804c0, (Qm)) )
#define vclzq_s8(Qm)                             ( __neon_QdQm( 0xf3b004c0, (Qm)) )
#define vclzq_u16(Qm)                            ( __neon_QdQm( 0xf3b404c0, (Qm)) )
#define vclzq_u32(Qm)                            ( __neon_QdQm( 0xf3b804c0, (Qm)) )
#define vclzq_u8(Qm)                             ( __neon_QdQm( 0xf3b004c0, (Qm)) )

// VCNT
#define vcnt_p8(Dm)                              ( __neon_DdDm( 0xf3b00500, (Dm)) )
#define vcnt_s8(Dm)                              ( __neon_DdDm( 0xf3b00500, (Dm)) )
#define vcnt_u8(Dm)                              ( __neon_DdDm( 0xf3b00500, (Dm)) )
#define vcntq_p8(Qm)                             ( __neon_QdQm( 0xf3b00540, (Qm)) )
#define vcntq_s8(Qm)                             ( __neon_QdQm( 0xf3b00540, (Qm)) )
#define vcntq_u8(Qm)                             ( __neon_QdQm( 0xf3b00540, (Qm)) )

// VCOMBINE (combine 2x64bit into a 128bit register)
#define vcombine_f32(Dn, Dm)                     ( __neon_QdDnDm_merge( 0x00000000, (Dn), (Dm)) )
#define vcombine_p16(Dn, Dm)                     ( __neon_QdDnDm_merge( 0x00000000, (Dn), (Dm)) )
#define vcombine_p8(Dn, Dm)                      ( __neon_QdDnDm_merge( 0x00000000, (Dn), (Dm)) )
#define vcombine_s16(Dn, Dm)                     ( __neon_QdDnDm_merge( 0x00000000, (Dn), (Dm)) )
#define vcombine_s32(Dn, Dm)                     ( __neon_QdDnDm_merge( 0x00000000, (Dn), (Dm)) )
#define vcombine_s64(Dn, Dm)                     ( __neon_QdDnDm_merge( 0x00000000, (Dn), (Dm)) )
#define vcombine_s8(Dn, Dm)                      ( __neon_QdDnDm_merge( 0x00000000, (Dn), (Dm)) )
#define vcombine_u16(Dn, Dm)                     ( __neon_QdDnDm_merge( 0x00000000, (Dn), (Dm)) )
#define vcombine_u32(Dn, Dm)                     ( __neon_QdDnDm_merge( 0x00000000, (Dn), (Dm)) )
#define vcombine_u64(Dn, Dm)                     ( __neon_QdDnDm_merge( 0x00000000, (Dn), (Dm)) )
#define vcombine_u8(Dn, Dm)                      ( __neon_QdDnDm_merge( 0x00000000, (Dn), (Dm)) )

// VCREATE (ARM core register pair to Neon 64bit register)
#define vcreate_f32(R64t)                        ( __neon_DdRtRt2( 0xec400b10, (R64t)) )
#define vcreate_p16(R64t)                        ( __neon_DdRtRt2( 0xec400b10, (R64t)) )
#define vcreate_p8(R64t)                         ( __neon_DdRtRt2( 0xec400b10, (R64t)) )
#define vcreate_s16(R64t)                        ( __neon_DdRtRt2( 0xec400b10, (R64t)) )
#define vcreate_s32(R64t)                        ( __neon_DdRtRt2( 0xec400b10, (R64t)) )
#define vcreate_s64(R64t)                        ( __neon_DdRtRt2( 0xec400b10, (R64t)) )
#define vcreate_s8(R64t)                         ( __neon_DdRtRt2( 0xec400b10, (R64t)) )
#define vcreate_u16(R64t)                        ( __neon_DdRtRt2( 0xec400b10, (R64t)) )
#define vcreate_u32(R64t)                        ( __neon_DdRtRt2( 0xec400b10, (R64t)) )
#define vcreate_u64(R64t)                        ( __neon_DdRtRt2( 0xec400b10, (R64t)) )
#define vcreate_u8(R64t)                         ( __neon_DdRtRt2( 0xec400b10, (R64t)) )

// VCVT (between floating-point and fixed-point)
#define vcvt_n_f32_s32(Dm, fbits)                ( __static_assert((fbits) >= 1 && (fbits) <= 32, "invalid fbits value"), __neon_DdDm( 0xf2800e10 | _NENC_21_16(64 - (fbits)), (Dm)) )
#define vcvt_n_f32_u32(Dm, fbits)                ( __static_assert((fbits) >= 1 && (fbits) <= 32, "invalid fbits value"), __neon_DdDm( 0xf3800e10 | _NENC_21_16(64 - (fbits)), (Dm)) )
#define vcvt_n_s32_f32(Dm, fbits)                ( __static_assert((fbits) >= 1 && (fbits) <= 32, "invalid fbits value"), __neon_DdDm( 0xf2800f10 | _NENC_21_16(64 - (fbits)), (Dm)) )
#define vcvt_n_u32_f32(Dm, fbits)                ( __static_assert((fbits) >= 1 && (fbits) <= 32, "invalid fbits value"), __neon_DdDm( 0xf3800f10 | _NENC_21_16(64 - (fbits)), (Dm)) )
#define vcvtq_n_f32_s32(Qm, fbits)               ( __static_assert((fbits) >= 1 && (fbits) <= 32, "invalid fbits value"), __neon_QdQm( 0xf2800e50 | _NENC_21_16(64 - (fbits)), (Qm)) )
#define vcvtq_n_f32_u32(Qm, fbits)               ( __static_assert((fbits) >= 1 && (fbits) <= 32, "invalid fbits value"), __neon_QdQm( 0xf3800e50 | _NENC_21_16(64 - (fbits)), (Qm)) )
#define vcvtq_n_s32_f32(Qm, fbits)               ( __static_assert((fbits) >= 1 && (fbits) <= 32, "invalid fbits value"), __neon_QdQm( 0xf2800f50 | _NENC_21_16(64 - (fbits)), (Qm)) )
#define vcvtq_n_u32_f32(Qm, fbits)               ( __static_assert((fbits) >= 1 && (fbits) <= 32, "invalid fbits value"), __neon_QdQm( 0xf3800f50 | _NENC_21_16(64 - (fbits)), (Qm)) )

// VCVT (between floating-point and integer)
#define vcvt_f32_s32(Dm)                         ( __neon_DdDm( 0xf3bb0600, (Dm)) )
#define vcvt_f32_u32(Dm)                         ( __neon_DdDm( 0xf3bb0680, (Dm)) )
#define vcvt_s32_f32(Dm)                         ( __neon_DdDm( 0xf3bb0700, (Dm)) )
#define vcvt_u32_f32(Dm)                         ( __neon_DdDm( 0xf3bb0780, (Dm)) )
#define vcvtq_f32_s32(Qm)                        ( __neon_QdQm( 0xf3bb0640, (Qm)) )
#define vcvtq_f32_u32(Qm)                        ( __neon_QdQm( 0xf3bb06c0, (Qm)) )
#define vcvtq_s32_f32(Qm)                        ( __neon_QdQm( 0xf3bb0740, (Qm)) )
#define vcvtq_u32_f32(Qm)                        ( __neon_QdQm( 0xf3bb07c0, (Qm)) )

// VDUP (scalar)
#define vdup_lane_f32(Dm, lane)                  ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_DdDm( 0xf3b40c00 | _NENC_19(lane), (Dm)) )
#define vdup_lane_p16(Dm, lane)                  ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_DdDm( 0xf3b20c00 | _NENC_19_18(lane), (Dm)) )
#define vdup_lane_p8(Dm, lane)                   ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_DdDm( 0xf3b10c00 | _NENC_19_17(lane), (Dm)) )
#define vdup_lane_s16(Dm, lane)                  ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_DdDm( 0xf3b20c00 | _NENC_19_18(lane), (Dm)) )
#define vdup_lane_s32(Dm, lane)                  ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_DdDm( 0xf3b40c00 | _NENC_19(lane), (Dm)) )
#define vdup_lane_s8(Dm, lane)                   ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_DdDm( 0xf3b10c00 | _NENC_19_17(lane), (Dm)) )
#define vdup_lane_u16(Dm, lane)                  ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_DdDm( 0xf3b20c00 | _NENC_19_18(lane), (Dm)) )
#define vdup_lane_u32(Dm, lane)                  ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_DdDm( 0xf3b40c00 | _NENC_19(lane), (Dm)) )
#define vdup_lane_u8(Dm, lane)                   ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_DdDm( 0xf3b10c00 | _NENC_19_17(lane), (Dm)) )
#define vdupq_lane_f32(Dm, lane)                 ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_QdDm( 0xf3b40c40 | _NENC_19(lane), (Dm)) )
#define vdupq_lane_p16(Dm, lane)                 ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_QdDm( 0xf3b20c40 | _NENC_19_18(lane), (Dm)) )
#define vdupq_lane_p8(Dm, lane)                  ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_QdDm( 0xf3b10c40 | _NENC_19_17(lane), (Dm)) )
#define vdupq_lane_s16(Dm, lane)                 ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_QdDm( 0xf3b20c40 | _NENC_19_18(lane), (Dm)) )
#define vdupq_lane_s32(Dm, lane)                 ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_QdDm( 0xf3b40c40 | _NENC_19(lane), (Dm)) )
#define vdupq_lane_s8(Dm, lane)                  ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_QdDm( 0xf3b10c40 | _NENC_19_17(lane), (Dm)) )
#define vdupq_lane_u16(Dm, lane)                 ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_QdDm( 0xf3b20c40 | _NENC_19_18(lane), (Dm)) )
#define vdupq_lane_u32(Dm, lane)                 ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_QdDm( 0xf3b40c40 | _NENC_19(lane), (Dm)) )
#define vdupq_lane_u8(Dm, lane)                  ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_QdDm( 0xf3b10c40 | _NENC_19_17(lane), (Dm)) )

// VDUP, VMOV (ARM core register to Neon register)
#define vdup_n_f32(Ft)                           ( __neon_DdFt( 0xee800b10, (Ft)) )
#define vmov_n_f32(Ft)                           ( __neon_DdFt( 0xee800b10, (Ft)) )
#define vdup_n_p16(Rt)                           ( __neon_DdRt( 0xee800b30, (Rt)) )
#define vdup_n_p8(Rt)                            ( __neon_DdRt( 0xeec00b10, (Rt)) )
#define vdup_n_s16(Rt)                           ( __neon_DdRt( 0xee800b30, (Rt)) )
#define vdup_n_s32(Rt)                           ( __neon_DdRt( 0xee800b10, (Rt)) )
#define vdup_n_s8(Rt)                            ( __neon_DdRt( 0xeec00b10, (Rt)) )
#define vdup_n_u16(Rt)                           ( __neon_DdRt( 0xee800b30, (Rt)) )
#define vdup_n_u32(Rt)                           ( __neon_DdRt( 0xee800b10, (Rt)) )
#define vdup_n_u8(Rt)                            ( __neon_DdRt( 0xeec00b10, (Rt)) )
#define vmov_n_p16(Rt)                           ( __neon_DdRt( 0xee800b30, (Rt)) )
#define vmov_n_p8(Rt)                            ( __neon_DdRt( 0xeec00b10, (Rt)) )
#define vmov_n_s16(Rt)                           ( __neon_DdRt( 0xee800b30, (Rt)) )
#define vmov_n_s32(Rt)                           ( __neon_DdRt( 0xee800b10, (Rt)) )
#define vmov_n_s8(Rt)                            ( __neon_DdRt( 0xeec00b10, (Rt)) )
#define vmov_n_u16(Rt)                           ( __neon_DdRt( 0xee800b30, (Rt)) )
#define vmov_n_u32(Rt)                           ( __neon_DdRt( 0xee800b10, (Rt)) )
#define vmov_n_u8(Rt)                            ( __neon_DdRt( 0xeec00b10, (Rt)) )
#define vdupq_n_f32(Ft)                          ( __neon_QdFt( 0xeea00b10, (Ft)) )
#define vmovq_n_f32(Ft)                          ( __neon_QdFt( 0xeea00b10, (Ft)) )
#define vdupq_n_p16(Rt)                          ( __neon_QdRt( 0xeea00b30, (Rt)) )
#define vdupq_n_p8(Rt)                           ( __neon_QdRt( 0xeee00b10, (Rt)) )
#define vdupq_n_s16(Rt)                          ( __neon_QdRt( 0xeea00b30, (Rt)) )
#define vdupq_n_s32(Rt)                          ( __neon_QdRt( 0xeea00b10, (Rt)) )
#define vdupq_n_s8(Rt)                           ( __neon_QdRt( 0xeee00b10, (Rt)) )
#define vdupq_n_u16(Rt)                          ( __neon_QdRt( 0xeea00b30, (Rt)) )
#define vdupq_n_u32(Rt)                          ( __neon_QdRt( 0xeea00b10, (Rt)) )
#define vdupq_n_u8(Rt)                           ( __neon_QdRt( 0xeee00b10, (Rt)) )
#define vmovq_n_p16(Rt)                          ( __neon_QdRt( 0xeea00b30, (Rt)) )
#define vmovq_n_p8(Rt)                           ( __neon_QdRt( 0xeee00b10, (Rt)) )
#define vmovq_n_s16(Rt)                          ( __neon_QdRt( 0xeea00b30, (Rt)) )
#define vmovq_n_s32(Rt)                          ( __neon_QdRt( 0xeea00b10, (Rt)) )
#define vmovq_n_s8(Rt)                           ( __neon_QdRt( 0xeee00b10, (Rt)) )
#define vmovq_n_u16(Rt)                          ( __neon_QdRt( 0xeea00b30, (Rt)) )
#define vmovq_n_u32(Rt)                          ( __neon_QdRt( 0xeea00b10, (Rt)) )
#define vmovq_n_u8(Rt)                           ( __neon_QdRt( 0xeee00b10, (Rt)) )

// VDUP.64, VMOV.64 (ARM core register pair to Neon registers)
#define vdup_n_s64(R64t)                         ( __neon_DdRtRt2( 0xec400b10, (R64t)) )
#define vdup_n_u64(R64t)                         ( __neon_DdRtRt2( 0xec400b10, (R64t)) )
#define vmov_n_s64(R64t)                         ( __neon_DdRtRt2( 0xec400b10, (R64t)) )
#define vmov_n_u64(R64t)                         ( __neon_DdRtRt2( 0xec400b10, (R64t)) )
#define vdupq_n_s64(R64t)                        ( __neon_QdRtRt2_dup( 0xec400b10, (R64t)) )
#define vdupq_n_u64(R64t)                        ( __neon_QdRtRt2_dup( 0xec400b10, (R64t)) )
#define vmovq_n_s64(R64t)                        ( __neon_QdRtRt2_dup( 0xec400b10, (R64t)) )
#define vmovq_n_u64(R64t)                        ( __neon_QdRtRt2_dup( 0xec400b10, (R64t)) )

// VEOR, VBIC, VORN
#define vbic_s16(Dn, Dm)                         ( __neon_DdDnDm( 0xf2100110, (Dn), (Dm)) )
#define vbic_s32(Dn, Dm)                         ( __neon_DdDnDm( 0xf2100110, (Dn), (Dm)) )
#define vbic_s64(Dn, Dm)                         ( __neon_DdDnDm( 0xf2100110, (Dn), (Dm)) )
#define vbic_s8(Dn, Dm)                          ( __neon_DdDnDm( 0xf2100110, (Dn), (Dm)) )
#define vbic_u16(Dn, Dm)                         ( __neon_DdDnDm( 0xf2100110, (Dn), (Dm)) )
#define vbic_u32(Dn, Dm)                         ( __neon_DdDnDm( 0xf2100110, (Dn), (Dm)) )
#define vbic_u64(Dn, Dm)                         ( __neon_DdDnDm( 0xf2100110, (Dn), (Dm)) )
#define vbic_u8(Dn, Dm)                          ( __neon_DdDnDm( 0xf2100110, (Dn), (Dm)) )
#define veor_s16(Dn, Dm)                         ( __neon_DdDnDm( 0xf3000110, (Dn), (Dm)) )
#define veor_s32(Dn, Dm)                         ( __neon_DdDnDm( 0xf3000110, (Dn), (Dm)) )
#define veor_s64(Dn, Dm)                         ( __neon_DdDnDm( 0xf3000110, (Dn), (Dm)) )
#define veor_s8(Dn, Dm)                          ( __neon_DdDnDm( 0xf3000110, (Dn), (Dm)) )
#define veor_u16(Dn, Dm)                         ( __neon_DdDnDm( 0xf3000110, (Dn), (Dm)) )
#define veor_u32(Dn, Dm)                         ( __neon_DdDnDm( 0xf3000110, (Dn), (Dm)) )
#define veor_u64(Dn, Dm)                         ( __neon_DdDnDm( 0xf3000110, (Dn), (Dm)) )
#define veor_u8(Dn, Dm)                          ( __neon_DdDnDm( 0xf3000110, (Dn), (Dm)) )
#define vorn_s16(Dn, Dm)                         ( __neon_DdDnDm( 0xf2300110, (Dn), (Dm)) )
#define vorn_s32(Dn, Dm)                         ( __neon_DdDnDm( 0xf2300110, (Dn), (Dm)) )
#define vorn_s64(Dn, Dm)                         ( __neon_DdDnDm( 0xf2300110, (Dn), (Dm)) )
#define vorn_s8(Dn, Dm)                          ( __neon_DdDnDm( 0xf2300110, (Dn), (Dm)) )
#define vorn_u16(Dn, Dm)                         ( __neon_DdDnDm( 0xf2300110, (Dn), (Dm)) )
#define vorn_u32(Dn, Dm)                         ( __neon_DdDnDm( 0xf2300110, (Dn), (Dm)) )
#define vorn_u64(Dn, Dm)                         ( __neon_DdDnDm( 0xf2300110, (Dn), (Dm)) )
#define vorn_u8(Dn, Dm)                          ( __neon_DdDnDm( 0xf2300110, (Dn), (Dm)) )
#define vbicq_s16(Qn, Qm)                        ( __neon_QdQnQm( 0xf2100150, (Qn), (Qm)) )
#define vbicq_s32(Qn, Qm)                        ( __neon_QdQnQm( 0xf2100150, (Qn), (Qm)) )
#define vbicq_s64(Qn, Qm)                        ( __neon_QdQnQm( 0xf2100150, (Qn), (Qm)) )
#define vbicq_s8(Qn, Qm)                         ( __neon_QdQnQm( 0xf2100150, (Qn), (Qm)) )
#define vbicq_u16(Qn, Qm)                        ( __neon_QdQnQm( 0xf2100150, (Qn), (Qm)) )
#define vbicq_u32(Qn, Qm)                        ( __neon_QdQnQm( 0xf2100150, (Qn), (Qm)) )
#define vbicq_u64(Qn, Qm)                        ( __neon_QdQnQm( 0xf2100150, (Qn), (Qm)) )
#define vbicq_u8(Qn, Qm)                         ( __neon_QdQnQm( 0xf2100150, (Qn), (Qm)) )
#define veorq_s16(Qn, Qm)                        ( __neon_QdQnQm( 0xf3000150, (Qn), (Qm)) )
#define veorq_s32(Qn, Qm)                        ( __neon_QdQnQm( 0xf3000150, (Qn), (Qm)) )
#define veorq_s64(Qn, Qm)                        ( __neon_QdQnQm( 0xf3000150, (Qn), (Qm)) )
#define veorq_s8(Qn, Qm)                         ( __neon_QdQnQm( 0xf3000150, (Qn), (Qm)) )
#define veorq_u16(Qn, Qm)                        ( __neon_QdQnQm( 0xf3000150, (Qn), (Qm)) )
#define veorq_u32(Qn, Qm)                        ( __neon_QdQnQm( 0xf3000150, (Qn), (Qm)) )
#define veorq_u64(Qn, Qm)                        ( __neon_QdQnQm( 0xf3000150, (Qn), (Qm)) )
#define veorq_u8(Qn, Qm)                         ( __neon_QdQnQm( 0xf3000150, (Qn), (Qm)) )
#define vornq_s16(Qn, Qm)                        ( __neon_QdQnQm( 0xf2300150, (Qn), (Qm)) )
#define vornq_s32(Qn, Qm)                        ( __neon_QdQnQm( 0xf2300150, (Qn), (Qm)) )
#define vornq_s64(Qn, Qm)                        ( __neon_QdQnQm( 0xf2300150, (Qn), (Qm)) )
#define vornq_s8(Qn, Qm)                         ( __neon_QdQnQm( 0xf2300150, (Qn), (Qm)) )
#define vornq_u16(Qn, Qm)                        ( __neon_QdQnQm( 0xf2300150, (Qn), (Qm)) )
#define vornq_u32(Qn, Qm)                        ( __neon_QdQnQm( 0xf2300150, (Qn), (Qm)) )
#define vornq_u64(Qn, Qm)                        ( __neon_QdQnQm( 0xf2300150, (Qn), (Qm)) )
#define vornq_u8(Qn, Qm)                         ( __neon_QdQnQm( 0xf2300150, (Qn), (Qm)) )

// VEXT
#define vext_f32(Dn, Dm, pos)                    ( __static_assert((pos) >= 0 && (pos) < 2, "invalid position value"), __neon_DdDnDm( 0xf2b00000 | _NENC_11_8((pos) * 4), (Dn), (Dm)) )
#define vext_p16(Dn, Dm, pos)                    ( __static_assert((pos) >= 0 && (pos) < 4, "invalid position value"), __neon_DdDnDm( 0xf2b00000 | _NENC_11_8((pos) * 2), (Dn), (Dm)) )
#define vext_p8(Dn, Dm, pos)                     ( __static_assert((pos) >= 0 && (pos) < 8, "invalid position value"), __neon_DdDnDm( 0xf2b00000 | _NENC_11_8(pos), (Dn), (Dm)) )
#define vext_s16(Dn, Dm, pos)                    ( __static_assert((pos) >= 0 && (pos) < 4, "invalid position value"), __neon_DdDnDm( 0xf2b00000 | _NENC_11_8((pos) * 2), (Dn), (Dm)) )
#define vext_s32(Dn, Dm, pos)                    ( __static_assert((pos) >= 0 && (pos) < 2, "invalid position value"), __neon_DdDnDm( 0xf2b00000 | _NENC_11_8((pos) * 4), (Dn), (Dm)) )
#define vext_s64(Dn, Dm, pos)                    ( __static_assert((pos) >= 0 && (pos) < 1, "invalid position value"), __neon_DdDnDm( 0xf2b00000 | _NENC_11_8((pos) * 8), (Dn), (Dm)) )
#define vext_s8(Dn, Dm, pos)                     ( __static_assert((pos) >= 0 && (pos) < 8, "invalid position value"), __neon_DdDnDm( 0xf2b00000 | _NENC_11_8(pos), (Dn), (Dm)) )
#define vext_u16(Dn, Dm, pos)                    ( __static_assert((pos) >= 0 && (pos) < 4, "invalid position value"), __neon_DdDnDm( 0xf2b00000 | _NENC_11_8((pos) * 2), (Dn), (Dm)) )
#define vext_u32(Dn, Dm, pos)                    ( __static_assert((pos) >= 0 && (pos) < 2, "invalid position value"), __neon_DdDnDm( 0xf2b00000 | _NENC_11_8((pos) * 4), (Dn), (Dm)) )
#define vext_u64(Dn, Dm, pos)                    ( __static_assert((pos) >= 0 && (pos) < 1, "invalid position value"), __neon_DdDnDm( 0xf2b00000 | _NENC_11_8((pos) * 8), (Dn), (Dm)) )
#define vext_u8(Dn, Dm, pos)                     ( __static_assert((pos) >= 0 && (pos) < 8, "invalid position value"), __neon_DdDnDm( 0xf2b00000 | _NENC_11_8(pos), (Dn), (Dm)) )
#define vextq_f32(Qn, Qm, pos)                   ( __static_assert((pos) >= 0 && (pos) < 4, "invalid position value"), __neon_QdQnQm( 0xf2b00040 | _NENC_11_8((pos) * 4), (Qn), (Qm)) )
#define vextq_p16(Qn, Qm, pos)                   ( __static_assert((pos) >= 0 && (pos) < 8, "invalid position value"), __neon_QdQnQm( 0xf2b00040 | _NENC_11_8((pos) * 2), (Qn), (Qm)) )
#define vextq_p8(Qn, Qm, pos)                    ( __static_assert((pos) >= 0 && (pos) < 16, "invalid position value"), __neon_QdQnQm( 0xf2b00040 | _NENC_11_8(pos), (Qn), (Qm)) )
#define vextq_s16(Qn, Qm, pos)                   ( __static_assert((pos) >= 0 && (pos) < 8, "invalid position value"), __neon_QdQnQm( 0xf2b00040 | _NENC_11_8((pos) * 2), (Qn), (Qm)) )
#define vextq_s32(Qn, Qm, pos)                   ( __static_assert((pos) >= 0 && (pos) < 4, "invalid position value"), __neon_QdQnQm( 0xf2b00040 | _NENC_11_8((pos) * 4), (Qn), (Qm)) )
#define vextq_s64(Qn, Qm, pos)                   ( __static_assert((pos) >= 0 && (pos) < 2, "invalid position value"), __neon_QdQnQm( 0xf2b00040 | _NENC_11_8((pos) * 8), (Qn), (Qm)) )
#define vextq_s8(Qn, Qm, pos)                    ( __static_assert((pos) >= 0 && (pos) < 16, "invalid position value"), __neon_QdQnQm( 0xf2b00040 | _NENC_11_8(pos), (Qn), (Qm)) )
#define vextq_u16(Qn, Qm, pos)                   ( __static_assert((pos) >= 0 && (pos) < 8, "invalid position value"), __neon_QdQnQm( 0xf2b00040 | _NENC_11_8((pos) * 2), (Qn), (Qm)) )
#define vextq_u32(Qn, Qm, pos)                   ( __static_assert((pos) >= 0 && (pos) < 4, "invalid position value"), __neon_QdQnQm( 0xf2b00040 | _NENC_11_8((pos) * 4), (Qn), (Qm)) )
#define vextq_u64(Qn, Qm, pos)                   ( __static_assert((pos) >= 0 && (pos) < 2, "invalid position value"), __neon_QdQnQm( 0xf2b00040 | _NENC_11_8((pos) * 8), (Qn), (Qm)) )
#define vextq_u8(Qn, Qm, pos)                    ( __static_assert((pos) >= 0 && (pos) < 16, "invalid position value"), __neon_QdQnQm( 0xf2b00040 | _NENC_11_8(pos), (Qn), (Qm)) )

// VGET (access the 64bit high/low part of a 128bit register)
#define vget_high_f32(Qm)                        ( __neon_DdQm_high( 0x00000000, (Qm)) )
#define vget_high_p16(Qm)                        ( __neon_DdQm_high( 0x00000000, (Qm)) )
#define vget_high_p8(Qm)                         ( __neon_DdQm_high( 0x00000000, (Qm)) )
#define vget_high_s16(Qm)                        ( __neon_DdQm_high( 0x00000000, (Qm)) )
#define vget_high_s32(Qm)                        ( __neon_DdQm_high( 0x00000000, (Qm)) )
#define vget_high_s64(Qm)                        ( __neon_DdQm_high( 0x00000000, (Qm)) )
#define vget_high_s8(Qm)                         ( __neon_DdQm_high( 0x00000000, (Qm)) )
#define vget_high_u16(Qm)                        ( __neon_DdQm_high( 0x00000000, (Qm)) )
#define vget_high_u32(Qm)                        ( __neon_DdQm_high( 0x00000000, (Qm)) )
#define vget_high_u64(Qm)                        ( __neon_DdQm_high( 0x00000000, (Qm)) )
#define vget_high_u8(Qm)                         ( __neon_DdQm_high( 0x00000000, (Qm)) )
#define vget_low_f32(Qm)                         ( __neon_DdQm_low( 0x00000000, (Qm)) )
#define vget_low_p16(Qm)                         ( __neon_DdQm_low( 0x00000000, (Qm)) )
#define vget_low_p8(Qm)                          ( __neon_DdQm_low( 0x00000000, (Qm)) )
#define vget_low_s16(Qm)                         ( __neon_DdQm_low( 0x00000000, (Qm)) )
#define vget_low_s32(Qm)                         ( __neon_DdQm_low( 0x00000000, (Qm)) )
#define vget_low_s64(Qm)                         ( __neon_DdQm_low( 0x00000000, (Qm)) )
#define vget_low_s8(Qm)                          ( __neon_DdQm_low( 0x00000000, (Qm)) )
#define vget_low_u16(Qm)                         ( __neon_DdQm_low( 0x00000000, (Qm)) )
#define vget_low_u32(Qm)                         ( __neon_DdQm_low( 0x00000000, (Qm)) )
#define vget_low_u64(Qm)                         ( __neon_DdQm_low( 0x00000000, (Qm)) )
#define vget_low_u8(Qm)                          ( __neon_DdQm_low( 0x00000000, (Qm)) )

// VHADD, VRHADD, VHSUB
#define vhadd_s16(Dn, Dm)                        ( __neon_DdDnDm( 0xf2100000, (Dn), (Dm)) )
#define vhadd_s32(Dn, Dm)                        ( __neon_DdDnDm( 0xf2200000, (Dn), (Dm)) )
#define vhadd_s8(Dn, Dm)                         ( __neon_DdDnDm( 0xf2000000, (Dn), (Dm)) )
#define vhadd_u16(Dn, Dm)                        ( __neon_DdDnDm( 0xf3100000, (Dn), (Dm)) )
#define vhadd_u32(Dn, Dm)                        ( __neon_DdDnDm( 0xf3200000, (Dn), (Dm)) )
#define vhadd_u8(Dn, Dm)                         ( __neon_DdDnDm( 0xf3000000, (Dn), (Dm)) )
#define vhsub_s16(Dn, Dm)                        ( __neon_DdDnDm( 0xf2100200, (Dn), (Dm)) )
#define vhsub_s32(Dn, Dm)                        ( __neon_DdDnDm( 0xf2200200, (Dn), (Dm)) )
#define vhsub_s8(Dn, Dm)                         ( __neon_DdDnDm( 0xf2000200, (Dn), (Dm)) )
#define vhsub_u16(Dn, Dm)                        ( __neon_DdDnDm( 0xf3100200, (Dn), (Dm)) )
#define vhsub_u32(Dn, Dm)                        ( __neon_DdDnDm( 0xf3200200, (Dn), (Dm)) )
#define vhsub_u8(Dn, Dm)                         ( __neon_DdDnDm( 0xf3000200, (Dn), (Dm)) )
#define vrhadd_s16(Dn, Dm)                       ( __neon_DdDnDm( 0xf2100100, (Dn), (Dm)) )
#define vrhadd_s32(Dn, Dm)                       ( __neon_DdDnDm( 0xf2200100, (Dn), (Dm)) )
#define vrhadd_s8(Dn, Dm)                        ( __neon_DdDnDm( 0xf2000100, (Dn), (Dm)) )
#define vrhadd_u16(Dn, Dm)                       ( __neon_DdDnDm( 0xf3100100, (Dn), (Dm)) )
#define vrhadd_u32(Dn, Dm)                       ( __neon_DdDnDm( 0xf3200100, (Dn), (Dm)) )
#define vrhadd_u8(Dn, Dm)                        ( __neon_DdDnDm( 0xf3000100, (Dn), (Dm)) )
#define vhaddq_s16(Qn, Qm)                       ( __neon_QdQnQm( 0xf2100040, (Qn), (Qm)) )
#define vhaddq_s32(Qn, Qm)                       ( __neon_QdQnQm( 0xf2200040, (Qn), (Qm)) )
#define vhaddq_s8(Qn, Qm)                        ( __neon_QdQnQm( 0xf2000040, (Qn), (Qm)) )
#define vhaddq_u16(Qn, Qm)                       ( __neon_QdQnQm( 0xf3100040, (Qn), (Qm)) )
#define vhaddq_u32(Qn, Qm)                       ( __neon_QdQnQm( 0xf3200040, (Qn), (Qm)) )
#define vhaddq_u8(Qn, Qm)                        ( __neon_QdQnQm( 0xf3000040, (Qn), (Qm)) )
#define vhsubq_s16(Qn, Qm)                       ( __neon_QdQnQm( 0xf2100240, (Qn), (Qm)) )
#define vhsubq_s32(Qn, Qm)                       ( __neon_QdQnQm( 0xf2200240, (Qn), (Qm)) )
#define vhsubq_s8(Qn, Qm)                        ( __neon_QdQnQm( 0xf2000240, (Qn), (Qm)) )
#define vhsubq_u16(Qn, Qm)                       ( __neon_QdQnQm( 0xf3100240, (Qn), (Qm)) )
#define vhsubq_u32(Qn, Qm)                       ( __neon_QdQnQm( 0xf3200240, (Qn), (Qm)) )
#define vhsubq_u8(Qn, Qm)                        ( __neon_QdQnQm( 0xf3000240, (Qn), (Qm)) )
#define vrhaddq_s16(Qn, Qm)                      ( __neon_QdQnQm( 0xf2100140, (Qn), (Qm)) )
#define vrhaddq_s32(Qn, Qm)                      ( __neon_QdQnQm( 0xf2200140, (Qn), (Qm)) )
#define vrhaddq_s8(Qn, Qm)                       ( __neon_QdQnQm( 0xf2000140, (Qn), (Qm)) )
#define vrhaddq_u16(Qn, Qm)                      ( __neon_QdQnQm( 0xf3100140, (Qn), (Qm)) )
#define vrhaddq_u32(Qn, Qm)                      ( __neon_QdQnQm( 0xf3200140, (Qn), (Qm)) )
#define vrhaddq_u8(Qn, Qm)                       ( __neon_QdQnQm( 0xf3000140, (Qn), (Qm)) )

// VLD1 (multiple single elements)
#define vld1_f32(pcD)                            ( __neon_D1Adr( 0xf420078f, (pcD)) )
#define vld1_p16(pcD)                            ( __neon_D1Adr( 0xf420074f, (pcD)) )
#define vld1_p8(pcD)                             ( __neon_D1Adr( 0xf420070f, (pcD)) )
#define vld1_s16(pcD)                            ( __neon_D1Adr( 0xf420074f, (pcD)) )
#define vld1_s32(pcD)                            ( __neon_D1Adr( 0xf420078f, (pcD)) )
#define vld1_s64(pcD)                            ( __neon_D1Adr( 0xf42007cf, (pcD)) )
#define vld1_s8(pcD)                             ( __neon_D1Adr( 0xf420070f, (pcD)) )
#define vld1_u16(pcD)                            ( __neon_D1Adr( 0xf420074f, (pcD)) )
#define vld1_u32(pcD)                            ( __neon_D1Adr( 0xf420078f, (pcD)) )
#define vld1_u64(pcD)                            ( __neon_D1Adr( 0xf42007cf, (pcD)) )
#define vld1_u8(pcD)                             ( __neon_D1Adr( 0xf420070f, (pcD)) )
#define vld1_f32_ex(pcD, align)                  ( __static_assert(_NEON_ALIGN1(align) >= 0, "invalid align"), __neon_D1Adr( 0xf420078f | _NENC_5_4(_NEON_ALIGN1(align)), (pcD)) )
#define vld1_p16_ex(pcD, align)                  ( __static_assert(_NEON_ALIGN1(align) >= 0, "invalid align"), __neon_D1Adr( 0xf420074f | _NENC_5_4(_NEON_ALIGN1(align)), (pcD)) )
#define vld1_p8_ex(pcD, align)                   ( __static_assert(_NEON_ALIGN1(align) >= 0, "invalid align"), __neon_D1Adr( 0xf420070f | _NENC_5_4(_NEON_ALIGN1(align)), (pcD)) )
#define vld1_s16_ex(pcD, align)                  ( __static_assert(_NEON_ALIGN1(align) >= 0, "invalid align"), __neon_D1Adr( 0xf420074f | _NENC_5_4(_NEON_ALIGN1(align)), (pcD)) )
#define vld1_s32_ex(pcD, align)                  ( __static_assert(_NEON_ALIGN1(align) >= 0, "invalid align"), __neon_D1Adr( 0xf420078f | _NENC_5_4(_NEON_ALIGN1(align)), (pcD)) )
#define vld1_s64_ex(pcD, align)                  ( __static_assert(_NEON_ALIGN1(align) >= 0, "invalid align"), __neon_D1Adr( 0xf42007cf | _NENC_5_4(_NEON_ALIGN1(align)), (pcD)) )
#define vld1_s8_ex(pcD, align)                   ( __static_assert(_NEON_ALIGN1(align) >= 0, "invalid align"), __neon_D1Adr( 0xf420070f | _NENC_5_4(_NEON_ALIGN1(align)), (pcD)) )
#define vld1_u16_ex(pcD, align)                  ( __static_assert(_NEON_ALIGN1(align) >= 0, "invalid align"), __neon_D1Adr( 0xf420074f | _NENC_5_4(_NEON_ALIGN1(align)), (pcD)) )
#define vld1_u32_ex(pcD, align)                  ( __static_assert(_NEON_ALIGN1(align) >= 0, "invalid align"), __neon_D1Adr( 0xf420078f | _NENC_5_4(_NEON_ALIGN1(align)), (pcD)) )
#define vld1_u64_ex(pcD, align)                  ( __static_assert(_NEON_ALIGN1(align) >= 0, "invalid align"), __neon_D1Adr( 0xf42007cf | _NENC_5_4(_NEON_ALIGN1(align)), (pcD)) )
#define vld1_u8_ex(pcD, align)                   ( __static_assert(_NEON_ALIGN1(align) >= 0, "invalid align"), __neon_D1Adr( 0xf420070f | _NENC_5_4(_NEON_ALIGN1(align)), (pcD)) )
#define vld1q_f32(pcQ)                           ( __neon_Q1Adr( 0xf4200a8f, (pcQ)) )
#define vld1q_p16(pcQ)                           ( __neon_Q1Adr( 0xf4200a4f, (pcQ)) )
#define vld1q_p8(pcQ)                            ( __neon_Q1Adr( 0xf4200a0f, (pcQ)) )
#define vld1q_s16(pcQ)                           ( __neon_Q1Adr( 0xf4200a4f, (pcQ)) )
#define vld1q_s32(pcQ)                           ( __neon_Q1Adr( 0xf4200a8f, (pcQ)) )
#define vld1q_s64(pcQ)                           ( __neon_Q1Adr( 0xf4200acf, (pcQ)) )
#define vld1q_s8(pcQ)                            ( __neon_Q1Adr( 0xf4200a0f, (pcQ)) )
#define vld1q_u16(pcQ)                           ( __neon_Q1Adr( 0xf4200a4f, (pcQ)) )
#define vld1q_u32(pcQ)                           ( __neon_Q1Adr( 0xf4200a8f, (pcQ)) )
#define vld1q_u64(pcQ)                           ( __neon_Q1Adr( 0xf4200acf, (pcQ)) )
#define vld1q_u8(pcQ)                            ( __neon_Q1Adr( 0xf4200a0f, (pcQ)) )
#define vld1q_f32_ex(pcQ, align)                 ( __static_assert(_NEON_ALIGN2(align) >= 0, "invalid align"), __neon_Q1Adr( 0xf4200a8f | _NENC_5_4(_NEON_ALIGN2(align)), (pcQ)) )
#define vld1q_p16_ex(pcQ, align)                 ( __static_assert(_NEON_ALIGN2(align) >= 0, "invalid align"), __neon_Q1Adr( 0xf4200a4f | _NENC_5_4(_NEON_ALIGN2(align)), (pcQ)) )
#define vld1q_p8_ex(pcQ, align)                  ( __static_assert(_NEON_ALIGN2(align) >= 0, "invalid align"), __neon_Q1Adr( 0xf4200a0f | _NENC_5_4(_NEON_ALIGN2(align)), (pcQ)) )
#define vld1q_s16_ex(pcQ, align)                 ( __static_assert(_NEON_ALIGN2(align) >= 0, "invalid align"), __neon_Q1Adr( 0xf4200a4f | _NENC_5_4(_NEON_ALIGN2(align)), (pcQ)) )
#define vld1q_s32_ex(pcQ, align)                 ( __static_assert(_NEON_ALIGN2(align) >= 0, "invalid align"), __neon_Q1Adr( 0xf4200a8f | _NENC_5_4(_NEON_ALIGN2(align)), (pcQ)) )
#define vld1q_s64_ex(pcQ, align)                 ( __static_assert(_NEON_ALIGN2(align) >= 0, "invalid align"), __neon_Q1Adr( 0xf4200acf | _NENC_5_4(_NEON_ALIGN2(align)), (pcQ)) )
#define vld1q_s8_ex(pcQ, align)                  ( __static_assert(_NEON_ALIGN2(align) >= 0, "invalid align"), __neon_Q1Adr( 0xf4200a0f | _NENC_5_4(_NEON_ALIGN2(align)), (pcQ)) )
#define vld1q_u16_ex(pcQ, align)                 ( __static_assert(_NEON_ALIGN2(align) >= 0, "invalid align"), __neon_Q1Adr( 0xf4200a4f | _NENC_5_4(_NEON_ALIGN2(align)), (pcQ)) )
#define vld1q_u32_ex(pcQ, align)                 ( __static_assert(_NEON_ALIGN2(align) >= 0, "invalid align"), __neon_Q1Adr( 0xf4200a8f | _NENC_5_4(_NEON_ALIGN2(align)), (pcQ)) )
#define vld1q_u64_ex(pcQ, align)                 ( __static_assert(_NEON_ALIGN2(align) >= 0, "invalid align"), __neon_Q1Adr( 0xf4200acf | _NENC_5_4(_NEON_ALIGN2(align)), (pcQ)) )
#define vld1q_u8_ex(pcQ, align)                  ( __static_assert(_NEON_ALIGN2(align) >= 0, "invalid align"), __neon_Q1Adr( 0xf4200a0f | _NENC_5_4(_NEON_ALIGN2(align)), (pcQ)) )

// VLD1 (single element to all lanes)
#define vld1_dup_f32(pcD)                        ( __neon_D1Adr( 0xf4a00c8f, (pcD)) )
#define vld1_dup_f32_ex(pcD)                     ( __neon_D1Adr( 0xf4a00c9f, (pcD)) )
#define vld1_dup_p16(pcD)                        ( __neon_D1Adr( 0xf4a00c4f, (pcD)) )
#define vld1_dup_p16_ex(pcD)                     ( __neon_D1Adr( 0xf4a00c5f, (pcD)) )
#define vld1_dup_p8(pcD)                         ( __neon_D1Adr( 0xf4a00c0f, (pcD)) )
#define vld1_dup_p8_ex(pcD)                      ( __neon_D1Adr( 0xf4a00c1f, (pcD)) )
#define vld1_dup_s16(pcD)                        ( __neon_D1Adr( 0xf4a00c4f, (pcD)) )
#define vld1_dup_s16_ex(pcD)                     ( __neon_D1Adr( 0xf4a00c5f, (pcD)) )
#define vld1_dup_s32(pcD)                        ( __neon_D1Adr( 0xf4a00c8f, (pcD)) )
#define vld1_dup_s32_ex(pcD)                     ( __neon_D1Adr( 0xf4a00c9f, (pcD)) )
#define vld1_dup_s8(pcD)                         ( __neon_D1Adr( 0xf4a00c0f, (pcD)) )
#define vld1_dup_s8_ex(pcD)                      ( __neon_D1Adr( 0xf4a00c1f, (pcD)) )
#define vld1_dup_u16(pcD)                        ( __neon_D1Adr( 0xf4a00c4f, (pcD)) )
#define vld1_dup_u16_ex(pcD)                     ( __neon_D1Adr( 0xf4a00c5f, (pcD)) )
#define vld1_dup_u32(pcD)                        ( __neon_D1Adr( 0xf4a00c8f, (pcD)) )
#define vld1_dup_u32_ex(pcD)                     ( __neon_D1Adr( 0xf4a00c9f, (pcD)) )
#define vld1_dup_u8(pcD)                         ( __neon_D1Adr( 0xf4a00c0f, (pcD)) )
#define vld1_dup_u8_ex(pcD)                      ( __neon_D1Adr( 0xf4a00c1f, (pcD)) )
#define vld1q_dup_f32(pcQ)                       ( __neon_Q1Adr( 0xf4a00caf, (pcQ)) )
#define vld1q_dup_f32_ex(pcQ)                    ( __neon_Q1Adr( 0xf4a00cbf, (pcQ)) )
#define vld1q_dup_p16(pcQ)                       ( __neon_Q1Adr( 0xf4a00c6f, (pcQ)) )
#define vld1q_dup_p16_ex(pcQ)                    ( __neon_Q1Adr( 0xf4a00c7f, (pcQ)) )
#define vld1q_dup_p8(pcQ)                        ( __neon_Q1Adr( 0xf4a00c2f, (pcQ)) )
#define vld1q_dup_p8_ex(pcQ)                     ( __neon_Q1Adr( 0xf4a00c3f, (pcQ)) )
#define vld1q_dup_s16(pcQ)                       ( __neon_Q1Adr( 0xf4a00c6f, (pcQ)) )
#define vld1q_dup_s16_ex(pcQ)                    ( __neon_Q1Adr( 0xf4a00c7f, (pcQ)) )
#define vld1q_dup_s32(pcQ)                       ( __neon_Q1Adr( 0xf4a00caf, (pcQ)) )
#define vld1q_dup_s32_ex(pcQ)                    ( __neon_Q1Adr( 0xf4a00cbf, (pcQ)) )
#define vld1q_dup_s8(pcQ)                        ( __neon_Q1Adr( 0xf4a00c2f, (pcQ)) )
#define vld1q_dup_s8_ex(pcQ)                     ( __neon_Q1Adr( 0xf4a00c3f, (pcQ)) )
#define vld1q_dup_u16(pcQ)                       ( __neon_Q1Adr( 0xf4a00c6f, (pcQ)) )
#define vld1q_dup_u16_ex(pcQ)                    ( __neon_Q1Adr( 0xf4a00c7f, (pcQ)) )
#define vld1q_dup_u32(pcQ)                       ( __neon_Q1Adr( 0xf4a00caf, (pcQ)) )
#define vld1q_dup_u32_ex(pcQ)                    ( __neon_Q1Adr( 0xf4a00cbf, (pcQ)) )
#define vld1q_dup_u8(pcQ)                        ( __neon_Q1Adr( 0xf4a00c2f, (pcQ)) )
#define vld1q_dup_u8_ex(pcQ)                     ( __neon_Q1Adr( 0xf4a00c3f, (pcQ)) )

// VLD1 (single element to one lane)
#define vld1_lane_f32(pcD, Dd, lane)             ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_D1Adr_acc( 0xf4a0080f | _NENC_7(lane), (Dd), (pcD)) )
#define vld1_lane_p16(pcD, Dd, lane)             ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_D1Adr_acc( 0xf4a0040f | _NENC_7_6(lane), (Dd), (pcD)) )
#define vld1_lane_p8(pcD, Dd, lane)              ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_D1Adr_acc( 0xf4a0000f | _NENC_7_5(lane), (Dd), (pcD)) )
#define vld1_lane_s16(pcD, Dd, lane)             ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_D1Adr_acc( 0xf4a0040f | _NENC_7_6(lane), (Dd), (pcD)) )
#define vld1_lane_s32(pcD, Dd, lane)             ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_D1Adr_acc( 0xf4a0080f | _NENC_7(lane), (Dd), (pcD)) )
#define vld1_lane_s8(pcD, Dd, lane)              ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_D1Adr_acc( 0xf4a0000f | _NENC_7_5(lane), (Dd), (pcD)) )
#define vld1_lane_u16(pcD, Dd, lane)             ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_D1Adr_acc( 0xf4a0040f | _NENC_7_6(lane), (Dd), (pcD)) )
#define vld1_lane_u32(pcD, Dd, lane)             ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_D1Adr_acc( 0xf4a0080f | _NENC_7(lane), (Dd), (pcD)) )
#define vld1_lane_u8(pcD, Dd, lane)              ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_D1Adr_acc( 0xf4a0000f | _NENC_7_5(lane), (Dd), (pcD)) )
#define vld1q_lane_f32(pcQ, Qd, lane)            ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_Q1Adr_acc( 0xf4a0080f | _NENC_7((lane) % 2) | _NENC_12((lane) >= 2 ? 1 : 0), (Qd), (pcQ)) )
#define vld1q_lane_p16(pcQ, Qd, lane)            ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_Q1Adr_acc( 0xf4a0040f | _NENC_7_6((lane) % 4) | _NENC_12((lane) >= 4 ? 1 : 0), (Qd), (pcQ)) )
#define vld1q_lane_p8(pcQ, Qd, lane)             ( __static_assert((lane) >= 0 && (lane) < 16, "invalid lane index"), __neon_Q1Adr_acc( 0xf4a0000f | _NENC_7_5((lane) % 8) | _NENC_12((lane) >= 8 ? 1 : 0), (Qd), (pcQ)) )
#define vld1q_lane_s16(pcQ, Qd, lane)            ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_Q1Adr_acc( 0xf4a0040f | _NENC_7_6((lane) % 4) | _NENC_12((lane) >= 4 ? 1 : 0), (Qd), (pcQ)) )
#define vld1q_lane_s32(pcQ, Qd, lane)            ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_Q1Adr_acc( 0xf4a0080f | _NENC_7((lane) % 2) | _NENC_12((lane) >= 2 ? 1 : 0), (Qd), (pcQ)) )
#define vld1q_lane_s8(pcQ, Qd, lane)             ( __static_assert((lane) >= 0 && (lane) < 16, "invalid lane index"), __neon_Q1Adr_acc( 0xf4a0000f | _NENC_7_5((lane) % 8) | _NENC_12((lane) >= 8 ? 1 : 0), (Qd), (pcQ)) )
#define vld1q_lane_u16(pcQ, Qd, lane)            ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_Q1Adr_acc( 0xf4a0040f | _NENC_7_6((lane) % 4) | _NENC_12((lane) >= 4 ? 1 : 0), (Qd), (pcQ)) )
#define vld1q_lane_u32(pcQ, Qd, lane)            ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_Q1Adr_acc( 0xf4a0080f | _NENC_7((lane) % 2) | _NENC_12((lane) >= 2 ? 1 : 0), (Qd), (pcQ)) )
#define vld1q_lane_u8(pcQ, Qd, lane)             ( __static_assert((lane) >= 0 && (lane) < 16, "invalid lane index"), __neon_Q1Adr_acc( 0xf4a0000f | _NENC_7_5((lane) % 8) | _NENC_12((lane) >= 8 ? 1 : 0), (Qd), (pcQ)) )

// VLD1 (single element to one lane, aligned)
#define vld1_lane_f32_ex(pcD, Dd, lane)          ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_D1Adr_acc( 0xf4a0083f | _NENC_7(lane), (Dd), (pcD)) )
#define vld1_lane_p16_ex(pcD, Dd, lane)          ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_D1Adr_acc( 0xf4a0041f | _NENC_7_6(lane), (Dd), (pcD)) )
#define vld1_lane_p8_ex(pcD, Dd, lane)           ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_D1Adr_acc( 0xf4a0000f | _NENC_7_5(lane), (Dd), (pcD)) )
#define vld1_lane_s16_ex(pcD, Dd, lane)          ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_D1Adr_acc( 0xf4a0041f | _NENC_7_6(lane), (Dd), (pcD)) )
#define vld1_lane_s32_ex(pcD, Dd, lane)          ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_D1Adr_acc( 0xf4a0083f | _NENC_7(lane), (Dd), (pcD)) )
#define vld1_lane_s8_ex(pcD, Dd, lane)           ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_D1Adr_acc( 0xf4a0000f | _NENC_7_5(lane), (Dd), (pcD)) )
#define vld1_lane_u16_ex(pcD, Dd, lane)          ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_D1Adr_acc( 0xf4a0041f | _NENC_7_6(lane), (Dd), (pcD)) )
#define vld1_lane_u32_ex(pcD, Dd, lane)          ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_D1Adr_acc( 0xf4a0083f | _NENC_7(lane), (Dd), (pcD)) )
#define vld1_lane_u8_ex(pcD, Dd, lane)           ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_D1Adr_acc( 0xf4a0000f | _NENC_7_5(lane), (Dd), (pcD)) )
#define vld1q_lane_f32_ex(pcQ, Qd, lane)         ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_Q1Adr_acc( 0xf4a0083f | _NENC_7((lane) % 2) | _NENC_12((lane) >= 2 ? 1 : 0), (Qd), (pcQ)) )
#define vld1q_lane_p16_ex(pcQ, Qd, lane)         ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_Q1Adr_acc( 0xf4a0041f | _NENC_7_6((lane) % 4) | _NENC_12((lane) >= 4 ? 1 : 0), (Qd), (pcQ)) )
#define vld1q_lane_p8_ex(pcQ, Qd, lane)          ( __static_assert((lane) >= 0 && (lane) < 16, "invalid lane index"), __neon_Q1Adr_acc( 0xf4a0000f | _NENC_7_5((lane) % 8) | _NENC_12((lane) >= 8 ? 1 : 0), (Qd), (pcQ)) )
#define vld1q_lane_s16_ex(pcQ, Qd, lane)         ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_Q1Adr_acc( 0xf4a0041f | _NENC_7_6((lane) % 4) | _NENC_12((lane) >= 4 ? 1 : 0), (Qd), (pcQ)) )
#define vld1q_lane_s32_ex(pcQ, Qd, lane)         ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_Q1Adr_acc( 0xf4a0083f | _NENC_7((lane) % 2) | _NENC_12((lane) >= 2 ? 1 : 0), (Qd), (pcQ)) )
#define vld1q_lane_s8_ex(pcQ, Qd, lane)          ( __static_assert((lane) >= 0 && (lane) < 16, "invalid lane index"), __neon_Q1Adr_acc( 0xf4a0000f | _NENC_7_5((lane) % 8) | _NENC_12((lane) >= 8 ? 1 : 0), (Qd), (pcQ)) )
#define vld1q_lane_u16_ex(pcQ, Qd, lane)         ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_Q1Adr_acc( 0xf4a0041f | _NENC_7_6((lane) % 4) | _NENC_12((lane) >= 4 ? 1 : 0), (Qd), (pcQ)) )
#define vld1q_lane_u32_ex(pcQ, Qd, lane)         ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_Q1Adr_acc( 0xf4a0083f | _NENC_7((lane) % 2) | _NENC_12((lane) >= 2 ? 1 : 0), (Qd), (pcQ)) )
#define vld1q_lane_u8_ex(pcQ, Qd, lane)          ( __static_assert((lane) >= 0 && (lane) < 16, "invalid lane index"), __neon_Q1Adr_acc( 0xf4a0000f | _NENC_7_5((lane) % 8) | _NENC_12((lane) >= 8 ? 1 : 0), (Qd), (pcQ)) )

// VMAX, VMIN (floating point)
#define vmax_f32(Dn, Dm)                         ( __neon_DdDnDm( 0xf2000f00, (Dn), (Dm)) )
#define vmin_f32(Dn, Dm)                         ( __neon_DdDnDm( 0xf2200f00, (Dn), (Dm)) )
#define vmaxq_f32(Qn, Qm)                        ( __neon_QdQnQm( 0xf2000f40, (Qn), (Qm)) )
#define vminq_f32(Qn, Qm)                        ( __neon_QdQnQm( 0xf2200f40, (Qn), (Qm)) )

// VMAX, VMIN (integer)
#define vmax_s16(Dn, Dm)                         ( __neon_DdDnDm( 0xf2100600, (Dn), (Dm)) )
#define vmax_s32(Dn, Dm)                         ( __neon_DdDnDm( 0xf2200600, (Dn), (Dm)) )
#define vmax_s8(Dn, Dm)                          ( __neon_DdDnDm( 0xf2000600, (Dn), (Dm)) )
#define vmax_u16(Dn, Dm)                         ( __neon_DdDnDm( 0xf3100600, (Dn), (Dm)) )
#define vmax_u32(Dn, Dm)                         ( __neon_DdDnDm( 0xf3200600, (Dn), (Dm)) )
#define vmax_u8(Dn, Dm)                          ( __neon_DdDnDm( 0xf3000600, (Dn), (Dm)) )
#define vmin_s16(Dn, Dm)                         ( __neon_DdDnDm( 0xf2100610, (Dn), (Dm)) )
#define vmin_s32(Dn, Dm)                         ( __neon_DdDnDm( 0xf2200610, (Dn), (Dm)) )
#define vmin_s8(Dn, Dm)                          ( __neon_DdDnDm( 0xf2000610, (Dn), (Dm)) )
#define vmin_u16(Dn, Dm)                         ( __neon_DdDnDm( 0xf3100610, (Dn), (Dm)) )
#define vmin_u32(Dn, Dm)                         ( __neon_DdDnDm( 0xf3200610, (Dn), (Dm)) )
#define vmin_u8(Dn, Dm)                          ( __neon_DdDnDm( 0xf3000610, (Dn), (Dm)) )
#define vmaxq_s16(Qn, Qm)                        ( __neon_QdQnQm( 0xf2100640, (Qn), (Qm)) )
#define vmaxq_s32(Qn, Qm)                        ( __neon_QdQnQm( 0xf2200640, (Qn), (Qm)) )
#define vmaxq_s8(Qn, Qm)                         ( __neon_QdQnQm( 0xf2000640, (Qn), (Qm)) )
#define vmaxq_u16(Qn, Qm)                        ( __neon_QdQnQm( 0xf3100640, (Qn), (Qm)) )
#define vmaxq_u32(Qn, Qm)                        ( __neon_QdQnQm( 0xf3200640, (Qn), (Qm)) )
#define vmaxq_u8(Qn, Qm)                         ( __neon_QdQnQm( 0xf3000640, (Qn), (Qm)) )
#define vminq_s16(Qn, Qm)                        ( __neon_QdQnQm( 0xf2100650, (Qn), (Qm)) )
#define vminq_s32(Qn, Qm)                        ( __neon_QdQnQm( 0xf2200650, (Qn), (Qm)) )
#define vminq_s8(Qn, Qm)                         ( __neon_QdQnQm( 0xf2000650, (Qn), (Qm)) )
#define vminq_u16(Qn, Qm)                        ( __neon_QdQnQm( 0xf3100650, (Qn), (Qm)) )
#define vminq_u32(Qn, Qm)                        ( __neon_QdQnQm( 0xf3200650, (Qn), (Qm)) )
#define vminq_u8(Qn, Qm)                         ( __neon_QdQnQm( 0xf3000650, (Qn), (Qm)) )

// VMLA, VMLS (by scalar)
#define vmla_lane_f32(Dd, Dn, Dm, lane)          ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_DdDnDmx_acc( 0xf2a00140 | _NENC_5(lane), (Dd), (Dn), (Dm)) )
#define vmla_lane_s16(Dd, Dn, Dm, lane)          ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_DdDnDmx_acc( 0xf2900040 | _NENC_5x3(lane), (Dd), (Dn), (Dm)) )
#define vmla_lane_s32(Dd, Dn, Dm, lane)          ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_DdDnDmx_acc( 0xf2a00040 | _NENC_5(lane), (Dd), (Dn), (Dm)) )
#define vmla_lane_u16(Dd, Dn, Dm, lane)          ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_DdDnDmx_acc( 0xf2900040 | _NENC_5x3(lane), (Dd), (Dn), (Dm)) )
#define vmla_lane_u32(Dd, Dn, Dm, lane)          ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_DdDnDmx_acc( 0xf2a00040 | _NENC_5(lane), (Dd), (Dn), (Dm)) )
#define vmls_lane_f32(Dd, Dn, Dm, lane)          ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_DdDnDmx_acc( 0xf2a00540 | _NENC_5(lane), (Dd), (Dn), (Dm)) )
#define vmls_lane_s16(Dd, Dn, Dm, lane)          ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_DdDnDmx_acc( 0xf2900440 | _NENC_5x3(lane), (Dd), (Dn), (Dm)) )
#define vmls_lane_s32(Dd, Dn, Dm, lane)          ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_DdDnDmx_acc( 0xf2a00440 | _NENC_5(lane), (Dd), (Dn), (Dm)) )
#define vmls_lane_u16(Dd, Dn, Dm, lane)          ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_DdDnDmx_acc( 0xf2900440 | _NENC_5x3(lane), (Dd), (Dn), (Dm)) )
#define vmls_lane_u32(Dd, Dn, Dm, lane)          ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_DdDnDmx_acc( 0xf2a00440 | _NENC_5(lane), (Dd), (Dn), (Dm)) )
#define vmlaq_lane_f32(Qd, Qn, Dm, lane)         ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_QdQnDmx_acc( 0xf3a00140 | _NENC_5(lane), (Qd), (Qn), (Dm)) )
#define vmlaq_lane_s16(Qd, Qn, Dm, lane)         ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_QdQnDmx_acc( 0xf3900040 | _NENC_5x3(lane), (Qd), (Qn), (Dm)) )
#define vmlaq_lane_s32(Qd, Qn, Dm, lane)         ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_QdQnDmx_acc( 0xf3a00040 | _NENC_5(lane), (Qd), (Qn), (Dm)) )
#define vmlaq_lane_u16(Qd, Qn, Dm, lane)         ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_QdQnDmx_acc( 0xf3900040 | _NENC_5x3(lane), (Qd), (Qn), (Dm)) )
#define vmlaq_lane_u32(Qd, Qn, Dm, lane)         ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_QdQnDmx_acc( 0xf3a00040 | _NENC_5(lane), (Qd), (Qn), (Dm)) )
#define vmlsq_lane_f32(Qd, Qn, Dm, lane)         ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_QdQnDmx_acc( 0xf3a00540 | _NENC_5(lane), (Qd), (Qn), (Dm)) )
#define vmlsq_lane_s16(Qd, Qn, Dm, lane)         ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_QdQnDmx_acc( 0xf3900440 | _NENC_5x3(lane), (Qd), (Qn), (Dm)) )
#define vmlsq_lane_s32(Qd, Qn, Dm, lane)         ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_QdQnDmx_acc( 0xf3a00440 | _NENC_5(lane), (Qd), (Qn), (Dm)) )
#define vmlsq_lane_u16(Qd, Qn, Dm, lane)         ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_QdQnDmx_acc( 0xf3900440 | _NENC_5x3(lane), (Qd), (Qn), (Dm)) )
#define vmlsq_lane_u32(Qd, Qn, Dm, lane)         ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_QdQnDmx_acc( 0xf3a00440 | _NENC_5(lane), (Qd), (Qn), (Dm)) )

// VMLA, VMLS (floating point)
#define vmla_f32(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf2000d10, (Dd), (Dn), (Dm)) )
#define vmls_f32(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf2200d10, (Dd), (Dn), (Dm)) )
#define vmlaq_f32(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf2000d50, (Qd), (Qn), (Qm)) )
#define vmlsq_f32(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf2200d50, (Qd), (Qn), (Qm)) )

// VMLA, VMLS (integer)
#define vmla_s16(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf2100900, (Dd), (Dn), (Dm)) )
#define vmla_s32(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf2200900, (Dd), (Dn), (Dm)) )
#define vmla_s8(Dd, Dn, Dm)                      ( __neon_DdDnDm_acc( 0xf2000900, (Dd), (Dn), (Dm)) )
#define vmla_u16(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf2100900, (Dd), (Dn), (Dm)) )
#define vmla_u32(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf2200900, (Dd), (Dn), (Dm)) )
#define vmla_u8(Dd, Dn, Dm)                      ( __neon_DdDnDm_acc( 0xf2000900, (Dd), (Dn), (Dm)) )
#define vmls_s16(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3100900, (Dd), (Dn), (Dm)) )
#define vmls_s32(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3200900, (Dd), (Dn), (Dm)) )
#define vmls_s8(Dd, Dn, Dm)                      ( __neon_DdDnDm_acc( 0xf3000900, (Dd), (Dn), (Dm)) )
#define vmls_u16(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3100900, (Dd), (Dn), (Dm)) )
#define vmls_u32(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3200900, (Dd), (Dn), (Dm)) )
#define vmls_u8(Dd, Dn, Dm)                      ( __neon_DdDnDm_acc( 0xf3000900, (Dd), (Dn), (Dm)) )
#define vmlaq_s16(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf2100940, (Qd), (Qn), (Qm)) )
#define vmlaq_s32(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf2200940, (Qd), (Qn), (Qm)) )
#define vmlaq_s8(Qd, Qn, Qm)                     ( __neon_QdQnQm_acc( 0xf2000940, (Qd), (Qn), (Qm)) )
#define vmlaq_u16(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf2100940, (Qd), (Qn), (Qm)) )
#define vmlaq_u32(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf2200940, (Qd), (Qn), (Qm)) )
#define vmlaq_u8(Qd, Qn, Qm)                     ( __neon_QdQnQm_acc( 0xf2000940, (Qd), (Qn), (Qm)) )
#define vmlsq_s16(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf3100940, (Qd), (Qn), (Qm)) )
#define vmlsq_s32(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf3200940, (Qd), (Qn), (Qm)) )
#define vmlsq_s8(Qd, Qn, Qm)                     ( __neon_QdQnQm_acc( 0xf3000940, (Qd), (Qn), (Qm)) )
#define vmlsq_u16(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf3100940, (Qd), (Qn), (Qm)) )
#define vmlsq_u32(Qd, Qn, Qm)                    ( __neon_QdQnQm_acc( 0xf3200940, (Qd), (Qn), (Qm)) )
#define vmlsq_u8(Qd, Qn, Qm)                     ( __neon_QdQnQm_acc( 0xf3000940, (Qd), (Qn), (Qm)) )

// VMLAL, VMLSL
#define vmlal_s16(Qd, Dn, Dm)                    ( __neon_QdDnDm_acc( 0xf2900800, (Qd), (Dn), (Dm)) )
#define vmlal_s32(Qd, Dn, Dm)                    ( __neon_QdDnDm_acc( 0xf2a00800, (Qd), (Dn), (Dm)) )
#define vmlal_s8(Qd, Dn, Dm)                     ( __neon_QdDnDm_acc( 0xf2800800, (Qd), (Dn), (Dm)) )
#define vmlal_u16(Qd, Dn, Dm)                    ( __neon_QdDnDm_acc( 0xf3900800, (Qd), (Dn), (Dm)) )
#define vmlal_u32(Qd, Dn, Dm)                    ( __neon_QdDnDm_acc( 0xf3a00800, (Qd), (Dn), (Dm)) )
#define vmlal_u8(Qd, Dn, Dm)                     ( __neon_QdDnDm_acc( 0xf3800800, (Qd), (Dn), (Dm)) )
#define vmlsl_s16(Qd, Dn, Dm)                    ( __neon_QdDnDm_acc( 0xf2900a00, (Qd), (Dn), (Dm)) )
#define vmlsl_s32(Qd, Dn, Dm)                    ( __neon_QdDnDm_acc( 0xf2a00a00, (Qd), (Dn), (Dm)) )
#define vmlsl_s8(Qd, Dn, Dm)                     ( __neon_QdDnDm_acc( 0xf2800a00, (Qd), (Dn), (Dm)) )
#define vmlsl_u16(Qd, Dn, Dm)                    ( __neon_QdDnDm_acc( 0xf3900a00, (Qd), (Dn), (Dm)) )
#define vmlsl_u32(Qd, Dn, Dm)                    ( __neon_QdDnDm_acc( 0xf3a00a00, (Qd), (Dn), (Dm)) )
#define vmlsl_u8(Qd, Dn, Dm)                     ( __neon_QdDnDm_acc( 0xf3800a00, (Qd), (Dn), (Dm)) )

// VMLAL, VMLSL (by scalar)
#define vmlal_lane_s16(Qd, Dn, Dm, lane)         ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_QdDnDmx_acc( 0xf2900240 | _NENC_5x3(lane), (Qd), (Dn), (Dm)) )
#define vmlal_lane_s32(Qd, Dn, Dm, lane)         ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_QdDnDmx_acc( 0xf2a00240 | _NENC_5(lane), (Qd), (Dn), (Dm)) )
#define vmlal_lane_u16(Qd, Dn, Dm, lane)         ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_QdDnDmx_acc( 0xf3900240 | _NENC_5x3(lane), (Qd), (Dn), (Dm)) )
#define vmlal_lane_u32(Qd, Dn, Dm, lane)         ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_QdDnDmx_acc( 0xf3a00240 | _NENC_5(lane), (Qd), (Dn), (Dm)) )
#define vmlsl_lane_s16(Qd, Dn, Dm, lane)         ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_QdDnDmx_acc( 0xf2900640 | _NENC_5x3(lane), (Qd), (Dn), (Dm)) )
#define vmlsl_lane_s32(Qd, Dn, Dm, lane)         ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_QdDnDmx_acc( 0xf2a00640 | _NENC_5(lane), (Qd), (Dn), (Dm)) )
#define vmlsl_lane_u16(Qd, Dn, Dm, lane)         ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_QdDnDmx_acc( 0xf3900640 | _NENC_5x3(lane), (Qd), (Dn), (Dm)) )
#define vmlsl_lane_u32(Qd, Dn, Dm, lane)         ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_QdDnDmx_acc( 0xf3a00640 | _NENC_5(lane), (Qd), (Dn), (Dm)) )

// VMOV (ARM core register to scalar)
#define vset_lane_f32(Ft, Dd, lane)              ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_DdFt_acc( 0xee000b10 | _NENC_21(lane), (Dd), (Ft)) )
#define vset_lane_p16(Rt, Dd, lane)              ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_DdRt_acc( 0xee000b30 | _NENC_21x6(lane), (Dd), (Rt)) )
#define vset_lane_p8(Rt, Dd, lane)               ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_DdRt_acc( 0xee400b10 | _NENC_21x6_5(lane), (Dd), (Rt)) )
#define vset_lane_s16(Rt, Dd, lane)              ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_DdRt_acc( 0xee000b30 | _NENC_21x6(lane), (Dd), (Rt)) )
#define vset_lane_s32(Rt, Dd, lane)              ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_DdRt_acc( 0xee000b10 | _NENC_21(lane), (Dd), (Rt)) )
#define vset_lane_s8(Rt, Dd, lane)               ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_DdRt_acc( 0xee400b10 | _NENC_21x6_5(lane), (Dd), (Rt)) )
#define vset_lane_u16(Rt, Dd, lane)              ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_DdRt_acc( 0xee000b30 | _NENC_21x6(lane), (Dd), (Rt)) )
#define vset_lane_u32(Rt, Dd, lane)              ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_DdRt_acc( 0xee000b10 | _NENC_21(lane), (Dd), (Rt)) )
#define vset_lane_u8(Rt, Dd, lane)               ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_DdRt_acc( 0xee400b10 | _NENC_21x6_5(lane), (Dd), (Rt)) )

// VMOV (scalar to ARM core register)
#define vget_lane_f32(Dm, lane)                  ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_FtDn( 0xee100b10 | _NENC_21(lane), (Dm)) )
#define vget_lane_p16(Dm, lane)                  ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_RtDn( 0xee900b30 | _NENC_21x6(lane), (Dm)) )
#define vget_lane_p8(Dm, lane)                   ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_RtDn( 0xeed00b10 | _NENC_21x6_5(lane), (Dm)) )
#define vget_lane_s16(Dm, lane)                  ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_RtDn( 0xee100b30 | _NENC_21x6(lane), (Dm)) )
#define vget_lane_s8(Dm, lane)                   ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_RtDn( 0xee500b10 | _NENC_21x6_5(lane), (Dm)) )
#define vget_lane_s32(Dm, lane)                  ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_RtDn( 0xee100b10 | _NENC_21(lane), (Dm)) )
#define vget_lane_u16(Dm, lane)                  ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_RtDn( 0xee900b30 | _NENC_21x6(lane), (Dm)) )
#define vget_lane_u8(Dm, lane)                   ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_RtDn( 0xeed00b10 | _NENC_21x6_5(lane), (Dm)) )
#define vget_lane_u32(Dm, lane)                  ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_RtDn( 0xee100b10 | _NENC_21(lane), (Dm)) )

// VMOV.64 (ARM core register pair to scalar)
#define vset_lane_s64(R64t, Dd, lane)            ( __static_assert((lane) >= 0 && (lane) < 1, "invalid lane index"), __neon_DdRtRt2_acc( 0xec400b10, (Dd), (R64t)) )
#define vset_lane_u64(R64t, Dd, lane)            ( __static_assert((lane) >= 0 && (lane) < 1, "invalid lane index"), __neon_DdRtRt2_acc( 0xec400b10, (Dd), (R64t)) )
#define vsetq_lane_s64(R64t, Qd, lane)           ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_QdRtRt2_acc( 0xec400b10 | _NENC_0(lane), (Qd), (R64t)) )
#define vsetq_lane_u64(R64t, Qd, lane)           ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_QdRtRt2_acc( 0xec400b10 | _NENC_0(lane), (Qd), (R64t)) )

// VMOV.64 (scalar to ARM core register pair)
#define vget_lane_s64(Dm, lane)                  ( __static_assert((lane) >= 0 && (lane) < 1, "invalid lane index"), __neon_RtRt2Dm( 0xec500b10, (Dm)) )
#define vget_lane_u64(Dm, lane)                  ( __static_assert((lane) >= 0 && (lane) < 1, "invalid lane index"), __neon_RtRt2Dm( 0xec500b10, (Dm)) )
#define vgetq_lane_s64(Qm, lane)                 ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_RtRt2Qm( 0xec500b10 | _NENC_0(lane), (Qm)) )
#define vgetq_lane_u64(Qm, lane)                 ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_RtRt2Qm( 0xec500b10 | _NENC_0(lane), (Qm)) )

// VMOV.Q (ARM core register to scalar)
#define vsetq_lane_f32(Ft, Qd, lane)             ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_QdFt_acc( 0xee000b10 | _NENC_16((lane) >= 2 ? 1 : 0) | _NENC_21((lane) % 2), (Qd), (Ft)) )
#define vsetq_lane_p16(Rt, Qd, lane)             ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_QdRt_acc( 0xee000b30 | _NENC_16((lane) >= 4 ? 1 : 0) | _NENC_21x6((lane) % 4), (Qd), (Rt)) )
#define vsetq_lane_p8(Rt, Qd, lane)              ( __static_assert((lane) >= 0 && (lane) < 16, "invalid lane index"), __neon_QdRt_acc( 0xee400b10 | _NENC_16((lane) >= 8 ? 1 : 0) | _NENC_21x6_5((lane) % 8), (Qd), (Rt)) )
#define vsetq_lane_s16(Rt, Qd, lane)             ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_QdRt_acc( 0xee000b30 | _NENC_16((lane) >= 4 ? 1 : 0) | _NENC_21x6((lane) % 4), (Qd), (Rt)) )
#define vsetq_lane_s32(Rt, Qd, lane)             ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_QdRt_acc( 0xee000b10 | _NENC_16((lane) >= 2 ? 1 : 0) | _NENC_21((lane) % 2), (Qd), (Rt)) )
#define vsetq_lane_s8(Rt, Qd, lane)              ( __static_assert((lane) >= 0 && (lane) < 16, "invalid lane index"), __neon_QdRt_acc( 0xee400b10 | _NENC_16((lane) >= 8 ? 1 : 0) | _NENC_21x6_5((lane) % 8), (Qd), (Rt)) )
#define vsetq_lane_u16(Rt, Qd, lane)             ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_QdRt_acc( 0xee000b30 | _NENC_16((lane) >= 4 ? 1 : 0) | _NENC_21x6((lane) % 4), (Qd), (Rt)) )
#define vsetq_lane_u32(Rt, Qd, lane)             ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_QdRt_acc( 0xee000b10 | _NENC_16((lane) >= 2 ? 1 : 0) | _NENC_21((lane) % 2), (Qd), (Rt)) )
#define vsetq_lane_u8(Rt, Qd, lane)              ( __static_assert((lane) >= 0 && (lane) < 16, "invalid lane index"), __neon_QdRt_acc( 0xee400b10 | _NENC_16((lane) >= 8 ? 1 : 0) | _NENC_21x6_5((lane) % 8), (Qd), (Rt)) )

// VMOV.Q (scalar to ARM core register)
#define vgetq_lane_f32(Qm, lane)                 ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_FtQn( 0xee100b10 | _NENC_16((lane) >= 2 ? 1 : 0) | _NENC_21((lane) % 2), (Qm)) )
#define vgetq_lane_p16(Qm, lane)                 ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_RtQn( 0xee900b30 | _NENC_16((lane) >= 4 ? 1 : 0) | _NENC_21x6((lane) % 4), (Qm)) )
#define vgetq_lane_p8(Qm, lane)                  ( __static_assert((lane) >= 0 && (lane) < 16, "invalid lane index"), __neon_RtQn( 0xeed00b10 | _NENC_16((lane) >= 8 ? 1 : 0) | _NENC_21x6_5((lane) % 8), (Qm)) )
#define vgetq_lane_s16(Qm, lane)                 ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_RtQn( 0xee100b30 | _NENC_16((lane) >= 4 ? 1 : 0) | _NENC_21x6((lane) % 4), (Qm)) )
#define vgetq_lane_s8(Qm, lane)                  ( __static_assert((lane) >= 0 && (lane) < 16, "invalid lane index"), __neon_RtQn( 0xee500b10 | _NENC_16((lane) >= 8 ? 1 : 0) | _NENC_21x6_5((lane) % 8), (Qm)) )
#define vgetq_lane_s32(Qm, lane)                 ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_RtQn( 0xee100b10 | _NENC_16((lane) >= 2 ? 1 : 0) | _NENC_21((lane) % 2), (Qm)) )
#define vgetq_lane_u16(Qm, lane)                 ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_RtQn( 0xee900b30 | _NENC_16((lane) >= 4 ? 1 : 0) | _NENC_21x6((lane) % 4), (Qm)) )
#define vgetq_lane_u8(Qm, lane)                  ( __static_assert((lane) >= 0 && (lane) < 16, "invalid lane index"), __neon_RtQn( 0xeed00b10 | _NENC_16((lane) >= 8 ? 1 : 0) | _NENC_21x6_5((lane) % 8), (Qm)) )
#define vgetq_lane_u32(Qm, lane)                 ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_RtQn( 0xee100b10 | _NENC_16((lane) >= 2 ? 1 : 0) | _NENC_21((lane) % 2), (Qm)) )

// VMOVL
#define vmovl_s16(Dm)                            ( __neon_QdDm( 0xf2900a10, (Dm)) )
#define vmovl_s32(Dm)                            ( __neon_QdDm( 0xf2a00a10, (Dm)) )
#define vmovl_s8(Dm)                             ( __neon_QdDm( 0xf2880a10, (Dm)) )
#define vmovl_u16(Dm)                            ( __neon_QdDm( 0xf3900a10, (Dm)) )
#define vmovl_u32(Dm)                            ( __neon_QdDm( 0xf3a00a10, (Dm)) )
#define vmovl_u8(Dm)                             ( __neon_QdDm( 0xf3880a10, (Dm)) )

// VMOVN
#define vmovn_s16(Qm)                            ( __neon_DdQm( 0xf3b20200, (Qm)) )
#define vmovn_s32(Qm)                            ( __neon_DdQm( 0xf3b60200, (Qm)) )
#define vmovn_s64(Qm)                            ( __neon_DdQm( 0xf3ba0200, (Qm)) )
#define vmovn_u16(Qm)                            ( __neon_DdQm( 0xf3b20200, (Qm)) )
#define vmovn_u32(Qm)                            ( __neon_DdQm( 0xf3b60200, (Qm)) )
#define vmovn_u64(Qm)                            ( __neon_DdQm( 0xf3ba0200, (Qm)) )

// VMUL
#define vmul_f32(Dn, Dm)                         ( __neon_DdDnDm( 0xf3000d10, (Dn), (Dm)) )
#define vmul_p8(Dn, Dm)                          ( __neon_DdDnDm( 0xf3000910, (Dn), (Dm)) )
#define vmul_s16(Dn, Dm)                         ( __neon_DdDnDm( 0xf2100910, (Dn), (Dm)) )
#define vmul_s32(Dn, Dm)                         ( __neon_DdDnDm( 0xf2200910, (Dn), (Dm)) )
#define vmul_s8(Dn, Dm)                          ( __neon_DdDnDm( 0xf2000910, (Dn), (Dm)) )
#define vmul_u16(Dn, Dm)                         ( __neon_DdDnDm( 0xf2100910, (Dn), (Dm)) )
#define vmul_u32(Dn, Dm)                         ( __neon_DdDnDm( 0xf2200910, (Dn), (Dm)) )
#define vmul_u8(Dn, Dm)                          ( __neon_DdDnDm( 0xf2000910, (Dn), (Dm)) )
#define vmulq_f32(Qn, Qm)                        ( __neon_QdQnQm( 0xf3000d50, (Qn), (Qm)) )
#define vmulq_p8(Qn, Qm)                         ( __neon_QdQnQm( 0xf3000950, (Qn), (Qm)) )
#define vmulq_s16(Qn, Qm)                        ( __neon_QdQnQm( 0xf2100950, (Qn), (Qm)) )
#define vmulq_s32(Qn, Qm)                        ( __neon_QdQnQm( 0xf2200950, (Qn), (Qm)) )
#define vmulq_s8(Qn, Qm)                         ( __neon_QdQnQm( 0xf2000950, (Qn), (Qm)) )
#define vmulq_u16(Qn, Qm)                        ( __neon_QdQnQm( 0xf2100950, (Qn), (Qm)) )
#define vmulq_u32(Qn, Qm)                        ( __neon_QdQnQm( 0xf2200950, (Qn), (Qm)) )
#define vmulq_u8(Qn, Qm)                         ( __neon_QdQnQm( 0xf2000950, (Qn), (Qm)) )

// VMUL (by scalar)
#define vmul_lane_f32(Dn, Dm, lane)              ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_DdDnDmx( 0xf2a00940 | _NENC_5(lane), (Dn), (Dm)) )
#define vmul_lane_s16(Dn, Dm, lane)              ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_DdDnDmx( 0xf2900840 | _NENC_5x3(lane), (Dn), (Dm)) )
#define vmul_lane_s32(Dn, Dm, lane)              ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_DdDnDmx( 0xf2a00840 | _NENC_5(lane), (Dn), (Dm)) )
#define vmul_lane_u16(Dn, Dm, lane)              ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_DdDnDmx( 0xf2900840 | _NENC_5x3(lane), (Dn), (Dm)) )
#define vmul_lane_u32(Dn, Dm, lane)              ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_DdDnDmx( 0xf2a00840 | _NENC_5(lane), (Dn), (Dm)) )
#define vmulq_lane_f32(Qn, Dm, lane)             ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_QdQnDmx( 0xf3a00940 | _NENC_5(lane), (Qn), (Dm)) )
#define vmulq_lane_s16(Qn, Dm, lane)             ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_QdQnDmx( 0xf3900840 | _NENC_5x3(lane), (Qn), (Dm)) )
#define vmulq_lane_s32(Qn, Dm, lane)             ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_QdQnDmx( 0xf3a00840 | _NENC_5(lane), (Qn), (Dm)) )
#define vmulq_lane_u16(Qn, Dm, lane)             ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_QdQnDmx( 0xf3900840 | _NENC_5x3(lane), (Qn), (Dm)) )
#define vmulq_lane_u32(Qn, Dm, lane)             ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_QdQnDmx( 0xf3a00840 | _NENC_5(lane), (Qn), (Dm)) )

// VMULL
#define vmull_p8(Dn, Dm)                         ( __neon_QdDnDm( 0xf2800e00, (Dn), (Dm)) )
#define vmull_s16(Dn, Dm)                        ( __neon_QdDnDm( 0xf2900c00, (Dn), (Dm)) )
#define vmull_s32(Dn, Dm)                        ( __neon_QdDnDm( 0xf2a00c00, (Dn), (Dm)) )
#define vmull_s8(Dn, Dm)                         ( __neon_QdDnDm( 0xf2800c00, (Dn), (Dm)) )
#define vmull_u16(Dn, Dm)                        ( __neon_QdDnDm( 0xf3900c00, (Dn), (Dm)) )
#define vmull_u32(Dn, Dm)                        ( __neon_QdDnDm( 0xf3a00c00, (Dn), (Dm)) )
#define vmull_u8(Dn, Dm)                         ( __neon_QdDnDm( 0xf3800c00, (Dn), (Dm)) )

// VMULL (by scalar)
#define vmull_lane_s16(Dn, Dm, lane)             ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_QdDnDmx( 0xf2900a40 | _NENC_5x3(lane), (Dn), (Dm)) )
#define vmull_lane_s32(Dn, Dm, lane)             ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_QdDnDmx( 0xf2a00a40 | _NENC_5(lane), (Dn), (Dm)) )
#define vmull_lane_u16(Dn, Dm, lane)             ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_QdDnDmx( 0xf3900a40 | _NENC_5x3(lane), (Dn), (Dm)) )
#define vmull_lane_u32(Dn, Dm, lane)             ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_QdDnDmx( 0xf3a00a40 | _NENC_5(lane), (Dn), (Dm)) )

// VMVN
#define vmvn_p16(Dm)                             ( __neon_DdDm( 0xf3b00580, (Dm)) )
#define vmvn_p8(Dm)                              ( __neon_DdDm( 0xf3b00580, (Dm)) )
#define vmvn_s16(Dm)                             ( __neon_DdDm( 0xf3b00580, (Dm)) )
#define vmvn_s32(Dm)                             ( __neon_DdDm( 0xf3b00580, (Dm)) )
#define vmvn_s8(Dm)                              ( __neon_DdDm( 0xf3b00580, (Dm)) )
#define vmvn_u16(Dm)                             ( __neon_DdDm( 0xf3b00580, (Dm)) )
#define vmvn_u32(Dm)                             ( __neon_DdDm( 0xf3b00580, (Dm)) )
#define vmvn_u8(Dm)                              ( __neon_DdDm( 0xf3b00580, (Dm)) )
#define vmvnq_p16(Qm)                            ( __neon_QdQm( 0xf3b005c0, (Qm)) )
#define vmvnq_p8(Qm)                             ( __neon_QdQm( 0xf3b005c0, (Qm)) )
#define vmvnq_s16(Qm)                            ( __neon_QdQm( 0xf3b005c0, (Qm)) )
#define vmvnq_s32(Qm)                            ( __neon_QdQm( 0xf3b005c0, (Qm)) )
#define vmvnq_s8(Qm)                             ( __neon_QdQm( 0xf3b005c0, (Qm)) )
#define vmvnq_u16(Qm)                            ( __neon_QdQm( 0xf3b005c0, (Qm)) )
#define vmvnq_u32(Qm)                            ( __neon_QdQm( 0xf3b005c0, (Qm)) )
#define vmvnq_u8(Qm)                             ( __neon_QdQm( 0xf3b005c0, (Qm)) )

// VPADAL
#define vpadal_s16(Dd, Dm)                       ( __neon_DdDm_acc( 0xf3b40600, (Dd), (Dm)) )
#define vpadal_s32(Dd, Dm)                       ( __neon_DdDm_acc( 0xf3b80600, (Dd), (Dm)) )
#define vpadal_s8(Dd, Dm)                        ( __neon_DdDm_acc( 0xf3b00600, (Dd), (Dm)) )
#define vpadal_u16(Dd, Dm)                       ( __neon_DdDm_acc( 0xf3b40680, (Dd), (Dm)) )
#define vpadal_u32(Dd, Dm)                       ( __neon_DdDm_acc( 0xf3b80680, (Dd), (Dm)) )
#define vpadal_u8(Dd, Dm)                        ( __neon_DdDm_acc( 0xf3b00680, (Dd), (Dm)) )
#define vpadalq_s16(Qd, Qm)                      ( __neon_QdQm_acc( 0xf3b40640, (Qd), (Qm)) )
#define vpadalq_s32(Qd, Qm)                      ( __neon_QdQm_acc( 0xf3b80640, (Qd), (Qm)) )
#define vpadalq_s8(Qd, Qm)                       ( __neon_QdQm_acc( 0xf3b00640, (Qd), (Qm)) )
#define vpadalq_u16(Qd, Qm)                      ( __neon_QdQm_acc( 0xf3b406c0, (Qd), (Qm)) )
#define vpadalq_u32(Qd, Qm)                      ( __neon_QdQm_acc( 0xf3b806c0, (Qd), (Qm)) )
#define vpadalq_u8(Qd, Qm)                       ( __neon_QdQm_acc( 0xf3b006c0, (Qd), (Qm)) )

// VPADD (floating point)
#define vpadd_f32(Dn, Dm)                        ( __neon_DdDnDm( 0xf3000d00, (Dn), (Dm)) )

// VPADD (integer)
#define vpadd_s16(Dn, Dm)                        ( __neon_DdDnDm( 0xf2100b10, (Dn), (Dm)) )
#define vpadd_s32(Dn, Dm)                        ( __neon_DdDnDm( 0xf2200b10, (Dn), (Dm)) )
#define vpadd_s8(Dn, Dm)                         ( __neon_DdDnDm( 0xf2000b10, (Dn), (Dm)) )
#define vpadd_u16(Dn, Dm)                        ( __neon_DdDnDm( 0xf2100b10, (Dn), (Dm)) )
#define vpadd_u32(Dn, Dm)                        ( __neon_DdDnDm( 0xf2200b10, (Dn), (Dm)) )
#define vpadd_u8(Dn, Dm)                         ( __neon_DdDnDm( 0xf2000b10, (Dn), (Dm)) )

// VPADDL
#define vpaddl_s16(Dm)                           ( __neon_DdDm( 0xf3b40200, (Dm)) )
#define vpaddl_s32(Dm)                           ( __neon_DdDm( 0xf3b80200, (Dm)) )
#define vpaddl_s8(Dm)                            ( __neon_DdDm( 0xf3b00200, (Dm)) )
#define vpaddl_u16(Dm)                           ( __neon_DdDm( 0xf3b40280, (Dm)) )
#define vpaddl_u32(Dm)                           ( __neon_DdDm( 0xf3b80280, (Dm)) )
#define vpaddl_u8(Dm)                            ( __neon_DdDm( 0xf3b00280, (Dm)) )
#define vpaddlq_s16(Qm)                          ( __neon_QdQm( 0xf3b40240, (Qm)) )
#define vpaddlq_s32(Qm)                          ( __neon_QdQm( 0xf3b80240, (Qm)) )
#define vpaddlq_s8(Qm)                           ( __neon_QdQm( 0xf3b00240, (Qm)) )
#define vpaddlq_u16(Qm)                          ( __neon_QdQm( 0xf3b402c0, (Qm)) )
#define vpaddlq_u32(Qm)                          ( __neon_QdQm( 0xf3b802c0, (Qm)) )
#define vpaddlq_u8(Qm)                           ( __neon_QdQm( 0xf3b002c0, (Qm)) )

// VPMAX, VPMIN (floating point)
#define vpmax_f32(Dn, Dm)                        ( __neon_DdDnDm( 0xf3000f00, (Dn), (Dm)) )
#define vpmin_f32(Dn, Dm)                        ( __neon_DdDnDm( 0xf3200f00, (Dn), (Dm)) )

// VPMAX, VPMIN (integer)
#define vpmax_s16(Dn, Dm)                        ( __neon_DdDnDm( 0xf2100a00, (Dn), (Dm)) )
#define vpmax_s32(Dn, Dm)                        ( __neon_DdDnDm( 0xf2200a00, (Dn), (Dm)) )
#define vpmax_s8(Dn, Dm)                         ( __neon_DdDnDm( 0xf2000a00, (Dn), (Dm)) )
#define vpmax_u16(Dn, Dm)                        ( __neon_DdDnDm( 0xf3100a00, (Dn), (Dm)) )
#define vpmax_u32(Dn, Dm)                        ( __neon_DdDnDm( 0xf3200a00, (Dn), (Dm)) )
#define vpmax_u8(Dn, Dm)                         ( __neon_DdDnDm( 0xf3000a00, (Dn), (Dm)) )
#define vpmin_s16(Dn, Dm)                        ( __neon_DdDnDm( 0xf2100a10, (Dn), (Dm)) )
#define vpmin_s32(Dn, Dm)                        ( __neon_DdDnDm( 0xf2200a10, (Dn), (Dm)) )
#define vpmin_s8(Dn, Dm)                         ( __neon_DdDnDm( 0xf2000a10, (Dn), (Dm)) )
#define vpmin_u16(Dn, Dm)                        ( __neon_DdDnDm( 0xf3100a10, (Dn), (Dm)) )
#define vpmin_u32(Dn, Dm)                        ( __neon_DdDnDm( 0xf3200a10, (Dn), (Dm)) )
#define vpmin_u8(Dn, Dm)                         ( __neon_DdDnDm( 0xf3000a10, (Dn), (Dm)) )

// VQABS, VQNEG
#define vqabs_s16(Dm)                            ( __neon_DdDm( 0xf3b40700, (Dm)) )
#define vqabs_s32(Dm)                            ( __neon_DdDm( 0xf3b80700, (Dm)) )
#define vqabs_s8(Dm)                             ( __neon_DdDm( 0xf3b00700, (Dm)) )
#define vqneg_s16(Dm)                            ( __neon_DdDm( 0xf3b40780, (Dm)) )
#define vqneg_s32(Dm)                            ( __neon_DdDm( 0xf3b80780, (Dm)) )
#define vqneg_s8(Dm)                             ( __neon_DdDm( 0xf3b00780, (Dm)) )
#define vqabsq_s16(Qm)                           ( __neon_QdQm( 0xf3b40740, (Qm)) )
#define vqabsq_s32(Qm)                           ( __neon_QdQm( 0xf3b80740, (Qm)) )
#define vqabsq_s8(Qm)                            ( __neon_QdQm( 0xf3b00740, (Qm)) )
#define vqnegq_s16(Qm)                           ( __neon_QdQm( 0xf3b407c0, (Qm)) )
#define vqnegq_s32(Qm)                           ( __neon_QdQm( 0xf3b807c0, (Qm)) )
#define vqnegq_s8(Qm)                            ( __neon_QdQm( 0xf3b007c0, (Qm)) )

// VQADD
#define vqadd_s16(Dn, Dm)                        ( __neon_DdDnDm( 0xf2100010, (Dn), (Dm)) )
#define vqadd_s32(Dn, Dm)                        ( __neon_DdDnDm( 0xf2200010, (Dn), (Dm)) )
#define vqadd_s64(Dn, Dm)                        ( __neon_DdDnDm( 0xf2300010, (Dn), (Dm)) )
#define vqadd_s8(Dn, Dm)                         ( __neon_DdDnDm( 0xf2000010, (Dn), (Dm)) )
#define vqadd_u16(Dn, Dm)                        ( __neon_DdDnDm( 0xf3100010, (Dn), (Dm)) )
#define vqadd_u32(Dn, Dm)                        ( __neon_DdDnDm( 0xf3200010, (Dn), (Dm)) )
#define vqadd_u64(Dn, Dm)                        ( __neon_DdDnDm( 0xf3300010, (Dn), (Dm)) )
#define vqadd_u8(Dn, Dm)                         ( __neon_DdDnDm( 0xf3000010, (Dn), (Dm)) )
#define vqaddq_s16(Qn, Qm)                       ( __neon_QdQnQm( 0xf2100050, (Qn), (Qm)) )
#define vqaddq_s32(Qn, Qm)                       ( __neon_QdQnQm( 0xf2200050, (Qn), (Qm)) )
#define vqaddq_s64(Qn, Qm)                       ( __neon_QdQnQm( 0xf2300050, (Qn), (Qm)) )
#define vqaddq_s8(Qn, Qm)                        ( __neon_QdQnQm( 0xf2000050, (Qn), (Qm)) )
#define vqaddq_u16(Qn, Qm)                       ( __neon_QdQnQm( 0xf3100050, (Qn), (Qm)) )
#define vqaddq_u32(Qn, Qm)                       ( __neon_QdQnQm( 0xf3200050, (Qn), (Qm)) )
#define vqaddq_u64(Qn, Qm)                       ( __neon_QdQnQm( 0xf3300050, (Qn), (Qm)) )
#define vqaddq_u8(Qn, Qm)                        ( __neon_QdQnQm( 0xf3000050, (Qn), (Qm)) )

// VQDMLAL, VQDMLSL
#define vqdmlal_s16(Qd, Dn, Dm)                  ( __neon_QdDnDm_acc( 0xf2900900, (Qd), (Dn), (Dm)) )
#define vqdmlal_s32(Qd, Dn, Dm)                  ( __neon_QdDnDm_acc( 0xf2a00900, (Qd), (Dn), (Dm)) )
#define vqdmlsl_s16(Qd, Dn, Dm)                  ( __neon_QdDnDm_acc( 0xf2900b00, (Qd), (Dn), (Dm)) )
#define vqdmlsl_s32(Qd, Dn, Dm)                  ( __neon_QdDnDm_acc( 0xf2a00b00, (Qd), (Dn), (Dm)) )

// VQDMLAL, VQDMLSL (by scalar)
#define vqdmlal_lane_s16(Qd, Dn, Dm, lane)       ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_QdDnDmx_acc( 0xf2900340 | _NENC_5x3(lane), (Qd), (Dn), (Dm)) )
#define vqdmlal_lane_s32(Qd, Dn, Dm, lane)       ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_QdDnDmx_acc( 0xf2a00340 | _NENC_5(lane), (Qd), (Dn), (Dm)) )
#define vqdmlsl_lane_s16(Qd, Dn, Dm, lane)       ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_QdDnDmx_acc( 0xf2900740 | _NENC_5x3(lane), (Qd), (Dn), (Dm)) )
#define vqdmlsl_lane_s32(Qd, Dn, Dm, lane)       ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_QdDnDmx_acc( 0xf2a00740 | _NENC_5(lane), (Qd), (Dn), (Dm)) )

// VQDMULH (by scalar)
#define vqdmulh_lane_s16(Dn, Dm, lane)           ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_DdDnDmx( 0xf2900c40 | _NENC_5x3(lane), (Dn), (Dm)) )
#define vqdmulh_lane_s32(Dn, Dm, lane)           ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_DdDnDmx( 0xf2a00c40 | _NENC_5(lane), (Dn), (Dm)) )
#define vqrdmulh_lane_s16(Dn, Dm, lane)          ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_DdDnDmx( 0xf2900d40 | _NENC_5x3(lane), (Dn), (Dm)) )
#define vqrdmulh_lane_s32(Dn, Dm, lane)          ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_DdDnDmx( 0xf2a00d40 | _NENC_5(lane), (Dn), (Dm)) )
#define vqdmulhq_lane_s16(Qn, Dm, lane)          ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_QdQnDmx( 0xf3900c40 | _NENC_5x3(lane), (Qn), (Dm)) )
#define vqdmulhq_lane_s32(Qn, Dm, lane)          ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_QdQnDmx( 0xf3a00c40 | _NENC_5(lane), (Qn), (Dm)) )
#define vqrdmulhq_lane_s16(Qn, Dm, lane)         ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_QdQnDmx( 0xf3900d40 | _NENC_5x3(lane), (Qn), (Dm)) )
#define vqrdmulhq_lane_s32(Qn, Dm, lane)         ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_QdQnDmx( 0xf3a00d40 | _NENC_5(lane), (Qn), (Dm)) )

// VQDMULH, VQRDMULH
#define vqdmulh_s16(Dn, Dm)                      ( __neon_DdDnDm( 0xf2100b00, (Dn), (Dm)) )
#define vqdmulh_s32(Dn, Dm)                      ( __neon_DdDnDm( 0xf2200b00, (Dn), (Dm)) )
#define vqrdmulh_s16(Dn, Dm)                     ( __neon_DdDnDm( 0xf3100b00, (Dn), (Dm)) )
#define vqrdmulh_s32(Dn, Dm)                     ( __neon_DdDnDm( 0xf3200b00, (Dn), (Dm)) )
#define vqdmulhq_s16(Qn, Qm)                     ( __neon_QdQnQm( 0xf2100b40, (Qn), (Qm)) )
#define vqdmulhq_s32(Qn, Qm)                     ( __neon_QdQnQm( 0xf2200b40, (Qn), (Qm)) )
#define vqrdmulhq_s16(Qn, Qm)                    ( __neon_QdQnQm( 0xf3100b40, (Qn), (Qm)) )
#define vqrdmulhq_s32(Qn, Qm)                    ( __neon_QdQnQm( 0xf3200b40, (Qn), (Qm)) )

// VQDMULL
#define vqdmull_s16(Dn, Dm)                      ( __neon_QdDnDm( 0xf2900d00, (Dn), (Dm)) )
#define vqdmull_s32(Dn, Dm)                      ( __neon_QdDnDm( 0xf2a00d00, (Dn), (Dm)) )

// VQDMULL (by scalar)
#define vqdmull_lane_s16(Dn, Dm, lane)           ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_QdDnDmx( 0xf2900b40 | _NENC_5x3(lane), (Dn), (Dm)) )
#define vqdmull_lane_s32(Dn, Dm, lane)           ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_QdDnDmx( 0xf2a00b40 | _NENC_5(lane), (Dn), (Dm)) )

// VQMOVN, VQMOVUN
#define vqmovn_s16(Qm)                           ( __neon_DdQm( 0xf3b20280, (Qm)) )
#define vqmovn_s32(Qm)                           ( __neon_DdQm( 0xf3b60280, (Qm)) )
#define vqmovn_s64(Qm)                           ( __neon_DdQm( 0xf3ba0280, (Qm)) )
#define vqmovn_u16(Qm)                           ( __neon_DdQm( 0xf3b202c0, (Qm)) )
#define vqmovn_u32(Qm)                           ( __neon_DdQm( 0xf3b602c0, (Qm)) )
#define vqmovn_u64(Qm)                           ( __neon_DdQm( 0xf3ba02c0, (Qm)) )
#define vqmovun_s16(Qm)                          ( __neon_DdQm( 0xf3b20240, (Qm)) )
#define vqmovun_s32(Qm)                          ( __neon_DdQm( 0xf3b60240, (Qm)) )
#define vqmovun_s64(Qm)                          ( __neon_DdQm( 0xf3ba0240, (Qm)) )

// VQSHL, VQSHLU (immediate)
#define vqshl_n_s16(Dm, shift_ammount)           ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 16, "invalid shift ammount"), __neon_DdDm( 0xf2800710 | _NENC_21_16((shift_ammount) + 16), (Dm)) )
#define vqshl_n_s32(Dm, shift_ammount)           ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 32, "invalid shift ammount"), __neon_DdDm( 0xf2800710 | _NENC_21_16((shift_ammount) + 32), (Dm)) )
#define vqshl_n_s64(Dm, shift_ammount)           ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 64, "invalid shift ammount"), __neon_DdDm( 0xf2800790 | _NENC_21_16(shift_ammount), (Dm)) )
#define vqshl_n_s8(Dm, shift_ammount)            ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 8, "invalid shift ammount"), __neon_DdDm( 0xf2800710 | _NENC_21_16((shift_ammount) + 8), (Dm)) )
#define vqshl_n_u16(Dm, shift_ammount)           ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 16, "invalid shift ammount"), __neon_DdDm( 0xf3800710 | _NENC_21_16((shift_ammount) + 16), (Dm)) )
#define vqshl_n_u32(Dm, shift_ammount)           ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 32, "invalid shift ammount"), __neon_DdDm( 0xf3800710 | _NENC_21_16((shift_ammount) + 32), (Dm)) )
#define vqshl_n_u64(Dm, shift_ammount)           ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 64, "invalid shift ammount"), __neon_DdDm( 0xf3800790 | _NENC_21_16(shift_ammount), (Dm)) )
#define vqshl_n_u8(Dm, shift_ammount)            ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 8, "invalid shift ammount"), __neon_DdDm( 0xf3800710 | _NENC_21_16((shift_ammount) + 8), (Dm)) )
#define vqshlu_n_s16(Dm, shift_ammount)          ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 16, "invalid shift ammount"), __neon_DdDm( 0xf3800610 | _NENC_21_16((shift_ammount) + 16), (Dm)) )
#define vqshlu_n_s32(Dm, shift_ammount)          ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 32, "invalid shift ammount"), __neon_DdDm( 0xf3800610 | _NENC_21_16((shift_ammount) + 32), (Dm)) )
#define vqshlu_n_s64(Dm, shift_ammount)          ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 64, "invalid shift ammount"), __neon_DdDm( 0xf3800690 | _NENC_21_16(shift_ammount), (Dm)) )
#define vqshlu_n_s8(Dm, shift_ammount)           ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 8, "invalid shift ammount"), __neon_DdDm( 0xf3800610 | _NENC_21_16((shift_ammount) + 8), (Dm)) )
#define vqshlq_n_s16(Qm, shift_ammount)          ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 16, "invalid shift ammount"), __neon_QdQm( 0xf2800750 | _NENC_21_16((shift_ammount) + 16), (Qm)) )
#define vqshlq_n_s32(Qm, shift_ammount)          ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 32, "invalid shift ammount"), __neon_QdQm( 0xf2800750 | _NENC_21_16((shift_ammount) + 32), (Qm)) )
#define vqshlq_n_s64(Qm, shift_ammount)          ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 64, "invalid shift ammount"), __neon_QdQm( 0xf28007d0 | _NENC_21_16(shift_ammount), (Qm)) )
#define vqshlq_n_s8(Qm, shift_ammount)           ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 8, "invalid shift ammount"), __neon_QdQm( 0xf2800750 | _NENC_21_16((shift_ammount) + 8), (Qm)) )
#define vqshlq_n_u16(Qm, shift_ammount)          ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 16, "invalid shift ammount"), __neon_QdQm( 0xf3800750 | _NENC_21_16((shift_ammount) + 16), (Qm)) )
#define vqshlq_n_u32(Qm, shift_ammount)          ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 32, "invalid shift ammount"), __neon_QdQm( 0xf3800750 | _NENC_21_16((shift_ammount) + 32), (Qm)) )
#define vqshlq_n_u64(Qm, shift_ammount)          ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 64, "invalid shift ammount"), __neon_QdQm( 0xf38007d0 | _NENC_21_16(shift_ammount), (Qm)) )
#define vqshlq_n_u8(Qm, shift_ammount)           ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 8, "invalid shift ammount"), __neon_QdQm( 0xf3800750 | _NENC_21_16((shift_ammount) + 8), (Qm)) )
#define vqshluq_n_s16(Qm, shift_ammount)         ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 16, "invalid shift ammount"), __neon_QdQm( 0xf3800650 | _NENC_21_16((shift_ammount) + 16), (Qm)) )
#define vqshluq_n_s32(Qm, shift_ammount)         ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 32, "invalid shift ammount"), __neon_QdQm( 0xf3800650 | _NENC_21_16((shift_ammount) + 32), (Qm)) )
#define vqshluq_n_s64(Qm, shift_ammount)         ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 64, "invalid shift ammount"), __neon_QdQm( 0xf38006d0 | _NENC_21_16(shift_ammount), (Qm)) )
#define vqshluq_n_s8(Qm, shift_ammount)          ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 8, "invalid shift ammount"), __neon_QdQm( 0xf3800650 | _NENC_21_16((shift_ammount) + 8), (Qm)) )

// VQSHRN, VQSHRUN, VQRSHRN, VQRSHRUN (immediate)
#define vqrshrn_n_s16(Qm, shift_ammount)         ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 8, "invalid shift ammount"), __neon_DdQm( 0xf2800950 | _NENC_21_16(16 - (shift_ammount)), (Qm)) )
#define vqrshrn_n_s32(Qm, shift_ammount)         ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 16, "invalid shift ammount"), __neon_DdQm( 0xf2800950 | _NENC_21_16(32 - (shift_ammount)), (Qm)) )
#define vqrshrn_n_s64(Qm, shift_ammount)         ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 32, "invalid shift ammount"), __neon_DdQm( 0xf2800950 | _NENC_21_16(64 - (shift_ammount)), (Qm)) )
#define vqrshrn_n_u16(Qm, shift_ammount)         ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 8, "invalid shift ammount"), __neon_DdQm( 0xf3800950 | _NENC_21_16(16 - (shift_ammount)), (Qm)) )
#define vqrshrn_n_u32(Qm, shift_ammount)         ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 16, "invalid shift ammount"), __neon_DdQm( 0xf3800950 | _NENC_21_16(32 - (shift_ammount)), (Qm)) )
#define vqrshrn_n_u64(Qm, shift_ammount)         ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 32, "invalid shift ammount"), __neon_DdQm( 0xf3800950 | _NENC_21_16(64 - (shift_ammount)), (Qm)) )
#define vqrshrun_n_s16(Qm, shift_ammount)        ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 8, "invalid shift ammount"), __neon_DdQm( 0xf3800850 | _NENC_21_16(16 - (shift_ammount)), (Qm)) )
#define vqrshrun_n_s32(Qm, shift_ammount)        ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 16, "invalid shift ammount"), __neon_DdQm( 0xf3800850 | _NENC_21_16(32 - (shift_ammount)), (Qm)) )
#define vqrshrun_n_s64(Qm, shift_ammount)        ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 32, "invalid shift ammount"), __neon_DdQm( 0xf3800850 | _NENC_21_16(64 - (shift_ammount)), (Qm)) )
#define vqshrn_n_s16(Qm, shift_ammount)          ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 8, "invalid shift ammount"), __neon_DdQm( 0xf2800910 | _NENC_21_16(16 - (shift_ammount)), (Qm)) )
#define vqshrn_n_s32(Qm, shift_ammount)          ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 16, "invalid shift ammount"), __neon_DdQm( 0xf2800910 | _NENC_21_16(32 - (shift_ammount)), (Qm)) )
#define vqshrn_n_s64(Qm, shift_ammount)          ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 32, "invalid shift ammount"), __neon_DdQm( 0xf2800910 | _NENC_21_16(64 - (shift_ammount)), (Qm)) )
#define vqshrn_n_u16(Qm, shift_ammount)          ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 8, "invalid shift ammount"), __neon_DdQm( 0xf3800910 | _NENC_21_16(16 - (shift_ammount)), (Qm)) )
#define vqshrn_n_u32(Qm, shift_ammount)          ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 16, "invalid shift ammount"), __neon_DdQm( 0xf3800910 | _NENC_21_16(32 - (shift_ammount)), (Qm)) )
#define vqshrn_n_u64(Qm, shift_ammount)          ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 32, "invalid shift ammount"), __neon_DdQm( 0xf3800910 | _NENC_21_16(64 - (shift_ammount)), (Qm)) )
#define vqshrun_n_s16(Qm, shift_ammount)         ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 8, "invalid shift ammount"), __neon_DdQm( 0xf3800810 | _NENC_21_16(16 - (shift_ammount)), (Qm)) )
#define vqshrun_n_s32(Qm, shift_ammount)         ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 16, "invalid shift ammount"), __neon_DdQm( 0xf3800810 | _NENC_21_16(32 - (shift_ammount)), (Qm)) )
#define vqshrun_n_s64(Qm, shift_ammount)         ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 32, "invalid shift ammount"), __neon_DdQm( 0xf3800810 | _NENC_21_16(64 - (shift_ammount)), (Qm)) )

// VQSUB
#define vqsub_s16(Dn, Dm)                        ( __neon_DdDnDm( 0xf2100210, (Dn), (Dm)) )
#define vqsub_s32(Dn, Dm)                        ( __neon_DdDnDm( 0xf2200210, (Dn), (Dm)) )
#define vqsub_s64(Dn, Dm)                        ( __neon_DdDnDm( 0xf2300210, (Dn), (Dm)) )
#define vqsub_s8(Dn, Dm)                         ( __neon_DdDnDm( 0xf2000210, (Dn), (Dm)) )
#define vqsub_u16(Dn, Dm)                        ( __neon_DdDnDm( 0xf3100210, (Dn), (Dm)) )
#define vqsub_u32(Dn, Dm)                        ( __neon_DdDnDm( 0xf3200210, (Dn), (Dm)) )
#define vqsub_u64(Dn, Dm)                        ( __neon_DdDnDm( 0xf3300210, (Dn), (Dm)) )
#define vqsub_u8(Dn, Dm)                         ( __neon_DdDnDm( 0xf3000210, (Dn), (Dm)) )
#define vqsubq_s16(Qn, Qm)                       ( __neon_QdQnQm( 0xf2100250, (Qn), (Qm)) )
#define vqsubq_s32(Qn, Qm)                       ( __neon_QdQnQm( 0xf2200250, (Qn), (Qm)) )
#define vqsubq_s64(Qn, Qm)                       ( __neon_QdQnQm( 0xf2300250, (Qn), (Qm)) )
#define vqsubq_s8(Qn, Qm)                        ( __neon_QdQnQm( 0xf2000250, (Qn), (Qm)) )
#define vqsubq_u16(Qn, Qm)                       ( __neon_QdQnQm( 0xf3100250, (Qn), (Qm)) )
#define vqsubq_u32(Qn, Qm)                       ( __neon_QdQnQm( 0xf3200250, (Qn), (Qm)) )
#define vqsubq_u64(Qn, Qm)                       ( __neon_QdQnQm( 0xf3300250, (Qn), (Qm)) )
#define vqsubq_u8(Qn, Qm)                        ( __neon_QdQnQm( 0xf3000250, (Qn), (Qm)) )

// VRECPE, VRSQRTE
#define vrecpe_f32(Dm)                           ( __neon_DdDm( 0xf3bb0500, (Dm)) )
#define vrecpe_u32(Dm)                           ( __neon_DdDm( 0xf3bb0400, (Dm)) )
#define vrsqrte_f32(Dm)                          ( __neon_DdDm( 0xf3bb0580, (Dm)) )
#define vrsqrte_u32(Dm)                          ( __neon_DdDm( 0xf3bb0480, (Dm)) )
#define vrecpeq_f32(Qm)                          ( __neon_QdQm( 0xf3bb0540, (Qm)) )
#define vrecpeq_u32(Qm)                          ( __neon_QdQm( 0xf3bb0440, (Qm)) )
#define vrsqrteq_f32(Qm)                         ( __neon_QdQm( 0xf3bb05c0, (Qm)) )
#define vrsqrteq_u32(Qm)                         ( __neon_QdQm( 0xf3bb04c0, (Qm)) )

// VRECPS
#define vrecps_f32(Dn, Dm)                       ( __neon_DdDnDm( 0xf2000f10, (Dn), (Dm)) )
#define vrecpsq_f32(Qn, Qm)                      ( __neon_QdQnQm( 0xf2000f50, (Qn), (Qm)) )

// VREV
#define vrev16_p8(Dm)                            ( __neon_DdDm( 0xf3b00100, (Dm)) )
#define vrev16_s8(Dm)                            ( __neon_DdDm( 0xf3b00100, (Dm)) )
#define vrev16_u8(Dm)                            ( __neon_DdDm( 0xf3b00100, (Dm)) )
#define vrev32_p16(Dm)                           ( __neon_DdDm( 0xf3b40080, (Dm)) )
#define vrev32_p8(Dm)                            ( __neon_DdDm( 0xf3b00080, (Dm)) )
#define vrev32_s16(Dm)                           ( __neon_DdDm( 0xf3b40080, (Dm)) )
#define vrev32_s8(Dm)                            ( __neon_DdDm( 0xf3b00080, (Dm)) )
#define vrev32_u16(Dm)                           ( __neon_DdDm( 0xf3b40080, (Dm)) )
#define vrev32_u8(Dm)                            ( __neon_DdDm( 0xf3b00080, (Dm)) )
#define vrev64_f32(Dm)                           ( __neon_DdDm( 0xf3b80000, (Dm)) )
#define vrev64_p16(Dm)                           ( __neon_DdDm( 0xf3b40000, (Dm)) )
#define vrev64_p8(Dm)                            ( __neon_DdDm( 0xf3b00000, (Dm)) )
#define vrev64_s16(Dm)                           ( __neon_DdDm( 0xf3b40000, (Dm)) )
#define vrev64_s32(Dm)                           ( __neon_DdDm( 0xf3b80000, (Dm)) )
#define vrev64_s8(Dm)                            ( __neon_DdDm( 0xf3b00000, (Dm)) )
#define vrev64_u16(Dm)                           ( __neon_DdDm( 0xf3b40000, (Dm)) )
#define vrev64_u32(Dm)                           ( __neon_DdDm( 0xf3b80000, (Dm)) )
#define vrev64_u8(Dm)                            ( __neon_DdDm( 0xf3b00000, (Dm)) )
#define vrev16q_p8(Qm)                           ( __neon_QdQm( 0xf3b00140, (Qm)) )
#define vrev16q_s8(Qm)                           ( __neon_QdQm( 0xf3b00140, (Qm)) )
#define vrev16q_u8(Qm)                           ( __neon_QdQm( 0xf3b00140, (Qm)) )
#define vrev32q_p16(Qm)                          ( __neon_QdQm( 0xf3b400c0, (Qm)) )
#define vrev32q_p8(Qm)                           ( __neon_QdQm( 0xf3b000c0, (Qm)) )
#define vrev32q_s16(Qm)                          ( __neon_QdQm( 0xf3b400c0, (Qm)) )
#define vrev32q_s8(Qm)                           ( __neon_QdQm( 0xf3b000c0, (Qm)) )
#define vrev32q_u16(Qm)                          ( __neon_QdQm( 0xf3b400c0, (Qm)) )
#define vrev32q_u8(Qm)                           ( __neon_QdQm( 0xf3b000c0, (Qm)) )
#define vrev64q_f32(Qm)                          ( __neon_QdQm( 0xf3b80040, (Qm)) )
#define vrev64q_p16(Qm)                          ( __neon_QdQm( 0xf3b40040, (Qm)) )
#define vrev64q_p8(Qm)                           ( __neon_QdQm( 0xf3b00040, (Qm)) )
#define vrev64q_s16(Qm)                          ( __neon_QdQm( 0xf3b40040, (Qm)) )
#define vrev64q_s32(Qm)                          ( __neon_QdQm( 0xf3b80040, (Qm)) )
#define vrev64q_s8(Qm)                           ( __neon_QdQm( 0xf3b00040, (Qm)) )
#define vrev64q_u16(Qm)                          ( __neon_QdQm( 0xf3b40040, (Qm)) )
#define vrev64q_u32(Qm)                          ( __neon_QdQm( 0xf3b80040, (Qm)) )
#define vrev64q_u8(Qm)                           ( __neon_QdQm( 0xf3b00040, (Qm)) )

// VRSQRTS
#define vrsqrts_f32(Dn, Dm)                      ( __neon_DdDnDm( 0xf2200f10, (Dn), (Dm)) )
#define vrsqrtsq_f32(Qn, Qm)                     ( __neon_QdQnQm( 0xf2200f50, (Qn), (Qm)) )

// VSHL (immediate)
#define vshl_n_s16(Dm, shift_ammount)            ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 16, "invalid shift ammount"), __neon_DdDm( 0xf2800510 | _NENC_21_16((shift_ammount) + 16), (Dm)) )
#define vshl_n_s32(Dm, shift_ammount)            ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 32, "invalid shift ammount"), __neon_DdDm( 0xf2800510 | _NENC_21_16((shift_ammount) + 32), (Dm)) )
#define vshl_n_s64(Dm, shift_ammount)            ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 64, "invalid shift ammount"), __neon_DdDm( 0xf2800590 | _NENC_21_16(shift_ammount), (Dm)) )
#define vshl_n_s8(Dm, shift_ammount)             ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 8, "invalid shift ammount"), __neon_DdDm( 0xf2800510 | _NENC_21_16((shift_ammount) + 8), (Dm)) )
#define vshl_n_u16(Dm, shift_ammount)            ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 16, "invalid shift ammount"), __neon_DdDm( 0xf2800510 | _NENC_21_16((shift_ammount) + 16), (Dm)) )
#define vshl_n_u32(Dm, shift_ammount)            ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 32, "invalid shift ammount"), __neon_DdDm( 0xf2800510 | _NENC_21_16((shift_ammount) + 32), (Dm)) )
#define vshl_n_u64(Dm, shift_ammount)            ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 64, "invalid shift ammount"), __neon_DdDm( 0xf2800590 | _NENC_21_16(shift_ammount), (Dm)) )
#define vshl_n_u8(Dm, shift_ammount)             ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 8, "invalid shift ammount"), __neon_DdDm( 0xf2800510 | _NENC_21_16((shift_ammount) + 8), (Dm)) )
#define vshlq_n_s16(Qm, shift_ammount)           ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 16, "invalid shift ammount"), __neon_QdQm( 0xf2800550 | _NENC_21_16((shift_ammount) + 16), (Qm)) )
#define vshlq_n_s32(Qm, shift_ammount)           ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 32, "invalid shift ammount"), __neon_QdQm( 0xf2800550 | _NENC_21_16((shift_ammount) + 32), (Qm)) )
#define vshlq_n_s64(Qm, shift_ammount)           ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 64, "invalid shift ammount"), __neon_QdQm( 0xf28005d0 | _NENC_21_16(shift_ammount), (Qm)) )
#define vshlq_n_s8(Qm, shift_ammount)            ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 8, "invalid shift ammount"), __neon_QdQm( 0xf2800550 | _NENC_21_16((shift_ammount) + 8), (Qm)) )
#define vshlq_n_u16(Qm, shift_ammount)           ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 16, "invalid shift ammount"), __neon_QdQm( 0xf2800550 | _NENC_21_16((shift_ammount) + 16), (Qm)) )
#define vshlq_n_u32(Qm, shift_ammount)           ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 32, "invalid shift ammount"), __neon_QdQm( 0xf2800550 | _NENC_21_16((shift_ammount) + 32), (Qm)) )
#define vshlq_n_u64(Qm, shift_ammount)           ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 64, "invalid shift ammount"), __neon_QdQm( 0xf28005d0 | _NENC_21_16(shift_ammount), (Qm)) )
#define vshlq_n_u8(Qm, shift_ammount)            ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 8, "invalid shift ammount"), __neon_QdQm( 0xf2800550 | _NENC_21_16((shift_ammount) + 8), (Qm)) )

// VSHL, VQSHL, VRSHL, VQRSHL (register)
#define vqrshl_s16(Dn, Dm)                       ( __neon_DdDnDm( 0xf2100510, (Dm), (Dn)) )
#define vqrshl_s32(Dn, Dm)                       ( __neon_DdDnDm( 0xf2200510, (Dm), (Dn)) )
#define vqrshl_s64(Dn, Dm)                       ( __neon_DdDnDm( 0xf2300510, (Dm), (Dn)) )
#define vqrshl_s8(Dn, Dm)                        ( __neon_DdDnDm( 0xf2000510, (Dm), (Dn)) )
#define vqrshl_u16(Dn, Dm)                       ( __neon_DdDnDm( 0xf3100510, (Dm), (Dn)) )
#define vqrshl_u32(Dn, Dm)                       ( __neon_DdDnDm( 0xf3200510, (Dm), (Dn)) )
#define vqrshl_u64(Dn, Dm)                       ( __neon_DdDnDm( 0xf3300510, (Dm), (Dn)) )
#define vqrshl_u8(Dn, Dm)                        ( __neon_DdDnDm( 0xf3000510, (Dm), (Dn)) )
#define vqshl_s16(Dn, Dm)                        ( __neon_DdDnDm( 0xf2100410, (Dm), (Dn)) )
#define vqshl_s32(Dn, Dm)                        ( __neon_DdDnDm( 0xf2200410, (Dm), (Dn)) )
#define vqshl_s64(Dn, Dm)                        ( __neon_DdDnDm( 0xf2300410, (Dm), (Dn)) )
#define vqshl_s8(Dn, Dm)                         ( __neon_DdDnDm( 0xf2000410, (Dm), (Dn)) )
#define vqshl_u16(Dn, Dm)                        ( __neon_DdDnDm( 0xf3100410, (Dm), (Dn)) )
#define vqshl_u32(Dn, Dm)                        ( __neon_DdDnDm( 0xf3200410, (Dm), (Dn)) )
#define vqshl_u64(Dn, Dm)                        ( __neon_DdDnDm( 0xf3300410, (Dm), (Dn)) )
#define vqshl_u8(Dn, Dm)                         ( __neon_DdDnDm( 0xf3000410, (Dm), (Dn)) )
#define vrshl_s16(Dn, Dm)                        ( __neon_DdDnDm( 0xf2100500, (Dm), (Dn)) )
#define vrshl_s32(Dn, Dm)                        ( __neon_DdDnDm( 0xf2200500, (Dm), (Dn)) )
#define vrshl_s64(Dn, Dm)                        ( __neon_DdDnDm( 0xf2300500, (Dm), (Dn)) )
#define vrshl_s8(Dn, Dm)                         ( __neon_DdDnDm( 0xf2000500, (Dm), (Dn)) )
#define vrshl_u16(Dn, Dm)                        ( __neon_DdDnDm( 0xf3100500, (Dm), (Dn)) )
#define vrshl_u32(Dn, Dm)                        ( __neon_DdDnDm( 0xf3200500, (Dm), (Dn)) )
#define vrshl_u64(Dn, Dm)                        ( __neon_DdDnDm( 0xf3300500, (Dm), (Dn)) )
#define vrshl_u8(Dn, Dm)                         ( __neon_DdDnDm( 0xf3000500, (Dm), (Dn)) )
#define vshl_s16(Dn, Dm)                         ( __neon_DdDnDm( 0xf2100400, (Dm), (Dn)) )
#define vshl_s32(Dn, Dm)                         ( __neon_DdDnDm( 0xf2200400, (Dm), (Dn)) )
#define vshl_s64(Dn, Dm)                         ( __neon_DdDnDm( 0xf2300400, (Dm), (Dn)) )
#define vshl_s8(Dn, Dm)                          ( __neon_DdDnDm( 0xf2000400, (Dm), (Dn)) )
#define vshl_u16(Dn, Dm)                         ( __neon_DdDnDm( 0xf3100400, (Dm), (Dn)) )
#define vshl_u32(Dn, Dm)                         ( __neon_DdDnDm( 0xf3200400, (Dm), (Dn)) )
#define vshl_u64(Dn, Dm)                         ( __neon_DdDnDm( 0xf3300400, (Dm), (Dn)) )
#define vshl_u8(Dn, Dm)                          ( __neon_DdDnDm( 0xf3000400, (Dm), (Dn)) )
#define vqrshlq_s16(Qn, Qm)                      ( __neon_QdQnQm( 0xf2100550, (Qm), (Qn)) )
#define vqrshlq_s32(Qn, Qm)                      ( __neon_QdQnQm( 0xf2200550, (Qm), (Qn)) )
#define vqrshlq_s64(Qn, Qm)                      ( __neon_QdQnQm( 0xf2300550, (Qm), (Qn)) )
#define vqrshlq_s8(Qn, Qm)                       ( __neon_QdQnQm( 0xf2000550, (Qm), (Qn)) )
#define vqrshlq_u16(Qn, Qm)                      ( __neon_QdQnQm( 0xf3100550, (Qm), (Qn)) )
#define vqrshlq_u32(Qn, Qm)                      ( __neon_QdQnQm( 0xf3200550, (Qm), (Qn)) )
#define vqrshlq_u64(Qn, Qm)                      ( __neon_QdQnQm( 0xf3300550, (Qm), (Qn)) )
#define vqrshlq_u8(Qn, Qm)                       ( __neon_QdQnQm( 0xf3000550, (Qm), (Qn)) )
#define vqshlq_s16(Qn, Qm)                       ( __neon_QdQnQm( 0xf2100450, (Qm), (Qn)) )
#define vqshlq_s32(Qn, Qm)                       ( __neon_QdQnQm( 0xf2200450, (Qm), (Qn)) )
#define vqshlq_s64(Qn, Qm)                       ( __neon_QdQnQm( 0xf2300450, (Qm), (Qn)) )
#define vqshlq_s8(Qn, Qm)                        ( __neon_QdQnQm( 0xf2000450, (Qm), (Qn)) )
#define vqshlq_u16(Qn, Qm)                       ( __neon_QdQnQm( 0xf3100450, (Qm), (Qn)) )
#define vqshlq_u32(Qn, Qm)                       ( __neon_QdQnQm( 0xf3200450, (Qm), (Qn)) )
#define vqshlq_u64(Qn, Qm)                       ( __neon_QdQnQm( 0xf3300450, (Qm), (Qn)) )
#define vqshlq_u8(Qn, Qm)                        ( __neon_QdQnQm( 0xf3000450, (Qm), (Qn)) )
#define vrshlq_s16(Qn, Qm)                       ( __neon_QdQnQm( 0xf2100540, (Qm), (Qn)) )
#define vrshlq_s32(Qn, Qm)                       ( __neon_QdQnQm( 0xf2200540, (Qm), (Qn)) )
#define vrshlq_s64(Qn, Qm)                       ( __neon_QdQnQm( 0xf2300540, (Qm), (Qn)) )
#define vrshlq_s8(Qn, Qm)                        ( __neon_QdQnQm( 0xf2000540, (Qm), (Qn)) )
#define vrshlq_u16(Qn, Qm)                       ( __neon_QdQnQm( 0xf3100540, (Qm), (Qn)) )
#define vrshlq_u32(Qn, Qm)                       ( __neon_QdQnQm( 0xf3200540, (Qm), (Qn)) )
#define vrshlq_u64(Qn, Qm)                       ( __neon_QdQnQm( 0xf3300540, (Qm), (Qn)) )
#define vrshlq_u8(Qn, Qm)                        ( __neon_QdQnQm( 0xf3000540, (Qm), (Qn)) )
#define vshlq_s16(Qn, Qm)                        ( __neon_QdQnQm( 0xf2100440, (Qm), (Qn)) )
#define vshlq_s32(Qn, Qm)                        ( __neon_QdQnQm( 0xf2200440, (Qm), (Qn)) )
#define vshlq_s64(Qn, Qm)                        ( __neon_QdQnQm( 0xf2300440, (Qm), (Qn)) )
#define vshlq_s8(Qn, Qm)                         ( __neon_QdQnQm( 0xf2000440, (Qm), (Qn)) )
#define vshlq_u16(Qn, Qm)                        ( __neon_QdQnQm( 0xf3100440, (Qm), (Qn)) )
#define vshlq_u32(Qn, Qm)                        ( __neon_QdQnQm( 0xf3200440, (Qm), (Qn)) )
#define vshlq_u64(Qn, Qm)                        ( __neon_QdQnQm( 0xf3300440, (Qm), (Qn)) )
#define vshlq_u8(Qn, Qm)                         ( __neon_QdQnQm( 0xf3000440, (Qm), (Qn)) )

// VSHLL (shift_ammount != size)
#define vshll_n_s16(Dm, shift_ammount)           ( __static_assert((shift_ammount) >= 1 && (shift_ammount) < 16, "invalid shift ammount"), __neon_QdDm( 0xf2800a10 | _NENC_21_16((shift_ammount) + 16), (Dm)) )
#define vshll_n_s32(Dm, shift_ammount)           ( __static_assert((shift_ammount) >= 1 && (shift_ammount) < 32, "invalid shift ammount"), __neon_QdDm( 0xf2800a10 | _NENC_21_16((shift_ammount) + 32), (Dm)) )
#define vshll_n_s8(Dm, shift_ammount)            ( __static_assert((shift_ammount) >= 1 && (shift_ammount) < 8, "invalid shift ammount"), __neon_QdDm( 0xf2800a10 | _NENC_21_16((shift_ammount) + 8), (Dm)) )
#define vshll_n_u16(Dm, shift_ammount)           ( __static_assert((shift_ammount) >= 1 && (shift_ammount) < 16, "invalid shift ammount"), __neon_QdDm( 0xf3800a10 | _NENC_21_16((shift_ammount) + 16), (Dm)) )
#define vshll_n_u32(Dm, shift_ammount)           ( __static_assert((shift_ammount) >= 1 && (shift_ammount) < 32, "invalid shift ammount"), __neon_QdDm( 0xf3800a10 | _NENC_21_16((shift_ammount) + 32), (Dm)) )
#define vshll_n_u8(Dm, shift_ammount)            ( __static_assert((shift_ammount) >= 1 && (shift_ammount) < 8, "invalid shift ammount"), __neon_QdDm( 0xf3800a10 | _NENC_21_16((shift_ammount) + 8), (Dm)) )

// VSHR, VRSHR (immediate)
#define vrshr_n_s16(Dm, shift_ammount)           ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 16, "invalid shift ammount"), __neon_DdDm( 0xf2800210 | _NENC_21_16(32 - (shift_ammount)), (Dm)) )
#define vrshr_n_s32(Dm, shift_ammount)           ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 32, "invalid shift ammount"), __neon_DdDm( 0xf2800210 | _NENC_21_16(64 - (shift_ammount)), (Dm)) )
#define vrshr_n_s64(Dm, shift_ammount)           ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 64, "invalid shift ammount"), __neon_DdDm( 0xf2800290 | _NENC_21_16(64 - (shift_ammount)), (Dm)) )
#define vrshr_n_s8(Dm, shift_ammount)            ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 8, "invalid shift ammount"), __neon_DdDm( 0xf2800210 | _NENC_21_16(16 - (shift_ammount)), (Dm)) )
#define vrshr_n_u16(Dm, shift_ammount)           ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 16, "invalid shift ammount"), __neon_DdDm( 0xf3800210 | _NENC_21_16(32 - (shift_ammount)), (Dm)) )
#define vrshr_n_u32(Dm, shift_ammount)           ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 32, "invalid shift ammount"), __neon_DdDm( 0xf3800210 | _NENC_21_16(64 - (shift_ammount)), (Dm)) )
#define vrshr_n_u64(Dm, shift_ammount)           ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 64, "invalid shift ammount"), __neon_DdDm( 0xf3800290 | _NENC_21_16(64 - (shift_ammount)), (Dm)) )
#define vrshr_n_u8(Dm, shift_ammount)            ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 8, "invalid shift ammount"), __neon_DdDm( 0xf3800210 | _NENC_21_16(16 - (shift_ammount)), (Dm)) )
#define vshr_n_s16(Dm, shift_ammount)            ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 16, "invalid shift ammount"), __neon_DdDm( 0xf2800010 | _NENC_21_16(32 - (shift_ammount)), (Dm)) )
#define vshr_n_s32(Dm, shift_ammount)            ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 32, "invalid shift ammount"), __neon_DdDm( 0xf2800010 | _NENC_21_16(64 - (shift_ammount)), (Dm)) )
#define vshr_n_s64(Dm, shift_ammount)            ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 64, "invalid shift ammount"), __neon_DdDm( 0xf2800090 | _NENC_21_16(64 - (shift_ammount)), (Dm)) )
#define vshr_n_s8(Dm, shift_ammount)             ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 8, "invalid shift ammount"), __neon_DdDm( 0xf2800010 | _NENC_21_16(16 - (shift_ammount)), (Dm)) )
#define vshr_n_u16(Dm, shift_ammount)            ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 16, "invalid shift ammount"), __neon_DdDm( 0xf3800010 | _NENC_21_16(32 - (shift_ammount)), (Dm)) )
#define vshr_n_u32(Dm, shift_ammount)            ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 32, "invalid shift ammount"), __neon_DdDm( 0xf3800010 | _NENC_21_16(64 - (shift_ammount)), (Dm)) )
#define vshr_n_u64(Dm, shift_ammount)            ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 64, "invalid shift ammount"), __neon_DdDm( 0xf3800090 | _NENC_21_16(64 - (shift_ammount)), (Dm)) )
#define vshr_n_u8(Dm, shift_ammount)             ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 8, "invalid shift ammount"), __neon_DdDm( 0xf3800010 | _NENC_21_16(16 - (shift_ammount)), (Dm)) )
#define vrshrq_n_s16(Qm, shift_ammount)          ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 16, "invalid shift ammount"), __neon_QdQm( 0xf2800250 | _NENC_21_16(32 - (shift_ammount)), (Qm)) )
#define vrshrq_n_s32(Qm, shift_ammount)          ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 32, "invalid shift ammount"), __neon_QdQm( 0xf2800250 | _NENC_21_16(64 - (shift_ammount)), (Qm)) )
#define vrshrq_n_s64(Qm, shift_ammount)          ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 64, "invalid shift ammount"), __neon_QdQm( 0xf28002d0 | _NENC_21_16(64 - (shift_ammount)), (Qm)) )
#define vrshrq_n_s8(Qm, shift_ammount)           ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 8, "invalid shift ammount"), __neon_QdQm( 0xf2800250 | _NENC_21_16(16 - (shift_ammount)), (Qm)) )
#define vrshrq_n_u16(Qm, shift_ammount)          ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 16, "invalid shift ammount"), __neon_QdQm( 0xf3800250 | _NENC_21_16(32 - (shift_ammount)), (Qm)) )
#define vrshrq_n_u32(Qm, shift_ammount)          ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 32, "invalid shift ammount"), __neon_QdQm( 0xf3800250 | _NENC_21_16(64 - (shift_ammount)), (Qm)) )
#define vrshrq_n_u64(Qm, shift_ammount)          ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 64, "invalid shift ammount"), __neon_QdQm( 0xf38002d0 | _NENC_21_16(64 - (shift_ammount)), (Qm)) )
#define vrshrq_n_u8(Qm, shift_ammount)           ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 8, "invalid shift ammount"), __neon_QdQm( 0xf3800250 | _NENC_21_16(16 - (shift_ammount)), (Qm)) )
#define vshrq_n_s16(Qm, shift_ammount)           ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 16, "invalid shift ammount"), __neon_QdQm( 0xf2800050 | _NENC_21_16(32 - (shift_ammount)), (Qm)) )
#define vshrq_n_s32(Qm, shift_ammount)           ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 32, "invalid shift ammount"), __neon_QdQm( 0xf2800050 | _NENC_21_16(64 - (shift_ammount)), (Qm)) )
#define vshrq_n_s64(Qm, shift_ammount)           ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 64, "invalid shift ammount"), __neon_QdQm( 0xf28000d0 | _NENC_21_16(64 - (shift_ammount)), (Qm)) )
#define vshrq_n_s8(Qm, shift_ammount)            ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 8, "invalid shift ammount"), __neon_QdQm( 0xf2800050 | _NENC_21_16(16 - (shift_ammount)), (Qm)) )
#define vshrq_n_u16(Qm, shift_ammount)           ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 16, "invalid shift ammount"), __neon_QdQm( 0xf3800050 | _NENC_21_16(32 - (shift_ammount)), (Qm)) )
#define vshrq_n_u32(Qm, shift_ammount)           ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 32, "invalid shift ammount"), __neon_QdQm( 0xf3800050 | _NENC_21_16(64 - (shift_ammount)), (Qm)) )
#define vshrq_n_u64(Qm, shift_ammount)           ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 64, "invalid shift ammount"), __neon_QdQm( 0xf38000d0 | _NENC_21_16(64 - (shift_ammount)), (Qm)) )
#define vshrq_n_u8(Qm, shift_ammount)            ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 8, "invalid shift ammount"), __neon_QdQm( 0xf3800050 | _NENC_21_16(16 - (shift_ammount)), (Qm)) )

// VSHRN, VRSHRN (immediate)
#define vrshrn_n_s16(Qm, shift_ammount)          ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 8, "invalid shift ammount"), __neon_DdQm( 0xf2800850 | _NENC_21_16(16 - (shift_ammount)), (Qm)) )
#define vrshrn_n_s32(Qm, shift_ammount)          ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 16, "invalid shift ammount"), __neon_DdQm( 0xf2800850 | _NENC_21_16(32 - (shift_ammount)), (Qm)) )
#define vrshrn_n_s64(Qm, shift_ammount)          ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 32, "invalid shift ammount"), __neon_DdQm( 0xf2800850 | _NENC_21_16(64 - (shift_ammount)), (Qm)) )
#define vrshrn_n_u16(Qm, shift_ammount)          ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 8, "invalid shift ammount"), __neon_DdQm( 0xf2800850 | _NENC_21_16(16 - (shift_ammount)), (Qm)) )
#define vrshrn_n_u32(Qm, shift_ammount)          ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 16, "invalid shift ammount"), __neon_DdQm( 0xf2800850 | _NENC_21_16(32 - (shift_ammount)), (Qm)) )
#define vrshrn_n_u64(Qm, shift_ammount)          ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 32, "invalid shift ammount"), __neon_DdQm( 0xf2800850 | _NENC_21_16(64 - (shift_ammount)), (Qm)) )
#define vshrn_n_s16(Qm, shift_ammount)           ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 8, "invalid shift ammount"), __neon_DdQm( 0xf2800810 | _NENC_21_16(16 - (shift_ammount)), (Qm)) )
#define vshrn_n_s32(Qm, shift_ammount)           ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 16, "invalid shift ammount"), __neon_DdQm( 0xf2800810 | _NENC_21_16(32 - (shift_ammount)), (Qm)) )
#define vshrn_n_s64(Qm, shift_ammount)           ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 32, "invalid shift ammount"), __neon_DdQm( 0xf2800810 | _NENC_21_16(64 - (shift_ammount)), (Qm)) )
#define vshrn_n_u16(Qm, shift_ammount)           ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 8, "invalid shift ammount"), __neon_DdQm( 0xf2800810 | _NENC_21_16(16 - (shift_ammount)), (Qm)) )
#define vshrn_n_u32(Qm, shift_ammount)           ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 16, "invalid shift ammount"), __neon_DdQm( 0xf2800810 | _NENC_21_16(32 - (shift_ammount)), (Qm)) )
#define vshrn_n_u64(Qm, shift_ammount)           ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 32, "invalid shift ammount"), __neon_DdQm( 0xf2800810 | _NENC_21_16(64 - (shift_ammount)), (Qm)) )

// VSLI (immediate)
#define vsli_n_p16(Dd, Dm, shift_ammount)        ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 16, "invalid shift ammount"), __neon_DdDm_acc( 0xf3800510 | _NENC_21_16((shift_ammount) + 16), (Dd), (Dm)) )
#define vsli_n_p8(Dd, Dm, shift_ammount)         ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 8, "invalid shift ammount"), __neon_DdDm_acc( 0xf3800510 | _NENC_21_16((shift_ammount) + 8), (Dd), (Dm)) )
#define vsli_n_s16(Dd, Dm, shift_ammount)        ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 16, "invalid shift ammount"), __neon_DdDm_acc( 0xf3800510 | _NENC_21_16((shift_ammount) + 16), (Dd), (Dm)) )
#define vsli_n_s32(Dd, Dm, shift_ammount)        ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 32, "invalid shift ammount"), __neon_DdDm_acc( 0xf3800510 | _NENC_21_16((shift_ammount) + 32), (Dd), (Dm)) )
#define vsli_n_s64(Dd, Dm, shift_ammount)        ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 64, "invalid shift ammount"), __neon_DdDm_acc( 0xf3800590 | _NENC_21_16(shift_ammount), (Dd), (Dm)) )
#define vsli_n_s8(Dd, Dm, shift_ammount)         ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 8, "invalid shift ammount"), __neon_DdDm_acc( 0xf3800510 | _NENC_21_16((shift_ammount) + 8), (Dd), (Dm)) )
#define vsli_n_u16(Dd, Dm, shift_ammount)        ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 16, "invalid shift ammount"), __neon_DdDm_acc( 0xf3800510 | _NENC_21_16((shift_ammount) + 16), (Dd), (Dm)) )
#define vsli_n_u32(Dd, Dm, shift_ammount)        ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 32, "invalid shift ammount"), __neon_DdDm_acc( 0xf3800510 | _NENC_21_16((shift_ammount) + 32), (Dd), (Dm)) )
#define vsli_n_u64(Dd, Dm, shift_ammount)        ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 64, "invalid shift ammount"), __neon_DdDm_acc( 0xf3800590 | _NENC_21_16(shift_ammount), (Dd), (Dm)) )
#define vsli_n_u8(Dd, Dm, shift_ammount)         ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 8, "invalid shift ammount"), __neon_DdDm_acc( 0xf3800510 | _NENC_21_16((shift_ammount) + 8), (Dd), (Dm)) )
#define vsliq_n_p16(Qd, Qm, shift_ammount)       ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 16, "invalid shift ammount"), __neon_QdQm_acc( 0xf3800550 | _NENC_21_16((shift_ammount) + 16), (Qd), (Qm)) )
#define vsliq_n_p8(Qd, Qm, shift_ammount)        ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 8, "invalid shift ammount"), __neon_QdQm_acc( 0xf3800550 | _NENC_21_16((shift_ammount) + 8), (Qd), (Qm)) )
#define vsliq_n_s16(Qd, Qm, shift_ammount)       ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 16, "invalid shift ammount"), __neon_QdQm_acc( 0xf3800550 | _NENC_21_16((shift_ammount) + 16), (Qd), (Qm)) )
#define vsliq_n_s32(Qd, Qm, shift_ammount)       ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 32, "invalid shift ammount"), __neon_QdQm_acc( 0xf3800550 | _NENC_21_16((shift_ammount) + 32), (Qd), (Qm)) )
#define vsliq_n_s64(Qd, Qm, shift_ammount)       ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 64, "invalid shift ammount"), __neon_QdQm_acc( 0xf38005d0 | _NENC_21_16(shift_ammount), (Qd), (Qm)) )
#define vsliq_n_s8(Qd, Qm, shift_ammount)        ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 8, "invalid shift ammount"), __neon_QdQm_acc( 0xf3800550 | _NENC_21_16((shift_ammount) + 8), (Qd), (Qm)) )
#define vsliq_n_u16(Qd, Qm, shift_ammount)       ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 16, "invalid shift ammount"), __neon_QdQm_acc( 0xf3800550 | _NENC_21_16((shift_ammount) + 16), (Qd), (Qm)) )
#define vsliq_n_u32(Qd, Qm, shift_ammount)       ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 32, "invalid shift ammount"), __neon_QdQm_acc( 0xf3800550 | _NENC_21_16((shift_ammount) + 32), (Qd), (Qm)) )
#define vsliq_n_u64(Qd, Qm, shift_ammount)       ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 64, "invalid shift ammount"), __neon_QdQm_acc( 0xf38005d0 | _NENC_21_16(shift_ammount), (Qd), (Qm)) )
#define vsliq_n_u8(Qd, Qm, shift_ammount)        ( __static_assert((shift_ammount) >= 0 && (shift_ammount) < 8, "invalid shift ammount"), __neon_QdQm_acc( 0xf3800550 | _NENC_21_16((shift_ammount) + 8), (Qd), (Qm)) )

// VSRA, VRSRA (immediate)
#define vrsra_n_s16(Dd, Dm, shift_ammount)       ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 16, "invalid shift ammount"), __neon_DdDm_acc( 0xf2800310 | _NENC_21_16(32 - (shift_ammount)), (Dd), (Dm)) )
#define vrsra_n_s32(Dd, Dm, shift_ammount)       ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 32, "invalid shift ammount"), __neon_DdDm_acc( 0xf2800310 | _NENC_21_16(64 - (shift_ammount)), (Dd), (Dm)) )
#define vrsra_n_s64(Dd, Dm, shift_ammount)       ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 64, "invalid shift ammount"), __neon_DdDm_acc( 0xf2800390 | _NENC_21_16(64 - (shift_ammount)), (Dd), (Dm)) )
#define vrsra_n_s8(Dd, Dm, shift_ammount)        ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 8, "invalid shift ammount"), __neon_DdDm_acc( 0xf2800310 | _NENC_21_16(16 - (shift_ammount)), (Dd), (Dm)) )
#define vrsra_n_u16(Dd, Dm, shift_ammount)       ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 16, "invalid shift ammount"), __neon_DdDm_acc( 0xf3800310 | _NENC_21_16(32 - (shift_ammount)), (Dd), (Dm)) )
#define vrsra_n_u32(Dd, Dm, shift_ammount)       ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 32, "invalid shift ammount"), __neon_DdDm_acc( 0xf3800310 | _NENC_21_16(64 - (shift_ammount)), (Dd), (Dm)) )
#define vrsra_n_u64(Dd, Dm, shift_ammount)       ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 64, "invalid shift ammount"), __neon_DdDm_acc( 0xf3800390 | _NENC_21_16(64 - (shift_ammount)), (Dd), (Dm)) )
#define vrsra_n_u8(Dd, Dm, shift_ammount)        ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 8, "invalid shift ammount"), __neon_DdDm_acc( 0xf3800310 | _NENC_21_16(16 - (shift_ammount)), (Dd), (Dm)) )
#define vsra_n_s16(Dd, Dm, shift_ammount)        ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 16, "invalid shift ammount"), __neon_DdDm_acc( 0xf2800110 | _NENC_21_16(32 - (shift_ammount)), (Dd), (Dm)) )
#define vsra_n_s32(Dd, Dm, shift_ammount)        ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 32, "invalid shift ammount"), __neon_DdDm_acc( 0xf2800110 | _NENC_21_16(64 - (shift_ammount)), (Dd), (Dm)) )
#define vsra_n_s64(Dd, Dm, shift_ammount)        ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 64, "invalid shift ammount"), __neon_DdDm_acc( 0xf2800190 | _NENC_21_16(64 - (shift_ammount)), (Dd), (Dm)) )
#define vsra_n_s8(Dd, Dm, shift_ammount)         ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 8, "invalid shift ammount"), __neon_DdDm_acc( 0xf2800110 | _NENC_21_16(16 - (shift_ammount)), (Dd), (Dm)) )
#define vsra_n_u16(Dd, Dm, shift_ammount)        ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 16, "invalid shift ammount"), __neon_DdDm_acc( 0xf3800110 | _NENC_21_16(32 - (shift_ammount)), (Dd), (Dm)) )
#define vsra_n_u32(Dd, Dm, shift_ammount)        ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 32, "invalid shift ammount"), __neon_DdDm_acc( 0xf3800110 | _NENC_21_16(64 - (shift_ammount)), (Dd), (Dm)) )
#define vsra_n_u64(Dd, Dm, shift_ammount)        ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 64, "invalid shift ammount"), __neon_DdDm_acc( 0xf3800190 | _NENC_21_16(64 - (shift_ammount)), (Dd), (Dm)) )
#define vsra_n_u8(Dd, Dm, shift_ammount)         ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 8, "invalid shift ammount"), __neon_DdDm_acc( 0xf3800110 | _NENC_21_16(16 - (shift_ammount)), (Dd), (Dm)) )
#define vrsraq_n_s16(Qd, Qm, shift_ammount)      ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 16, "invalid shift ammount"), __neon_QdQm_acc( 0xf2800350 | _NENC_21_16(32 - (shift_ammount)), (Qd), (Qm)) )
#define vrsraq_n_s32(Qd, Qm, shift_ammount)      ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 32, "invalid shift ammount"), __neon_QdQm_acc( 0xf2800350 | _NENC_21_16(64 - (shift_ammount)), (Qd), (Qm)) )
#define vrsraq_n_s64(Qd, Qm, shift_ammount)      ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 64, "invalid shift ammount"), __neon_QdQm_acc( 0xf28003d0 | _NENC_21_16(64 - (shift_ammount)), (Qd), (Qm)) )
#define vrsraq_n_s8(Qd, Qm, shift_ammount)       ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 8, "invalid shift ammount"), __neon_QdQm_acc( 0xf2800350 | _NENC_21_16(16 - (shift_ammount)), (Qd), (Qm)) )
#define vrsraq_n_u16(Qd, Qm, shift_ammount)      ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 16, "invalid shift ammount"), __neon_QdQm_acc( 0xf3800350 | _NENC_21_16(32 - (shift_ammount)), (Qd), (Qm)) )
#define vrsraq_n_u32(Qd, Qm, shift_ammount)      ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 32, "invalid shift ammount"), __neon_QdQm_acc( 0xf3800350 | _NENC_21_16(64 - (shift_ammount)), (Qd), (Qm)) )
#define vrsraq_n_u64(Qd, Qm, shift_ammount)      ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 64, "invalid shift ammount"), __neon_QdQm_acc( 0xf38003d0 | _NENC_21_16(64 - (shift_ammount)), (Qd), (Qm)) )
#define vrsraq_n_u8(Qd, Qm, shift_ammount)       ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 8, "invalid shift ammount"), __neon_QdQm_acc( 0xf3800350 | _NENC_21_16(16 - (shift_ammount)), (Qd), (Qm)) )
#define vsraq_n_s16(Qd, Qm, shift_ammount)       ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 16, "invalid shift ammount"), __neon_QdQm_acc( 0xf2800150 | _NENC_21_16(32 - (shift_ammount)), (Qd), (Qm)) )
#define vsraq_n_s32(Qd, Qm, shift_ammount)       ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 32, "invalid shift ammount"), __neon_QdQm_acc( 0xf2800150 | _NENC_21_16(64 - (shift_ammount)), (Qd), (Qm)) )
#define vsraq_n_s64(Qd, Qm, shift_ammount)       ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 64, "invalid shift ammount"), __neon_QdQm_acc( 0xf28001d0 | _NENC_21_16(64 - (shift_ammount)), (Qd), (Qm)) )
#define vsraq_n_s8(Qd, Qm, shift_ammount)        ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 8, "invalid shift ammount"), __neon_QdQm_acc( 0xf2800150 | _NENC_21_16(16 - (shift_ammount)), (Qd), (Qm)) )
#define vsraq_n_u16(Qd, Qm, shift_ammount)       ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 16, "invalid shift ammount"), __neon_QdQm_acc( 0xf3800150 | _NENC_21_16(32 - (shift_ammount)), (Qd), (Qm)) )
#define vsraq_n_u32(Qd, Qm, shift_ammount)       ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 32, "invalid shift ammount"), __neon_QdQm_acc( 0xf3800150 | _NENC_21_16(64 - (shift_ammount)), (Qd), (Qm)) )
#define vsraq_n_u64(Qd, Qm, shift_ammount)       ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 64, "invalid shift ammount"), __neon_QdQm_acc( 0xf38001d0 | _NENC_21_16(64 - (shift_ammount)), (Qd), (Qm)) )
#define vsraq_n_u8(Qd, Qm, shift_ammount)        ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 8, "invalid shift ammount"), __neon_QdQm_acc( 0xf3800150 | _NENC_21_16(16 - (shift_ammount)), (Qd), (Qm)) )

// VSRI (immediate)
#define vsri_n_p16(Dd, Dm, shift_ammount)        ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 16, "invalid shift ammount"), __neon_DdDm_acc( 0xf3800410 | _NENC_21_16(32 - (shift_ammount)), (Dd), (Dm)) )
#define vsri_n_p8(Dd, Dm, shift_ammount)         ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 8, "invalid shift ammount"), __neon_DdDm_acc( 0xf3800410 | _NENC_21_16(16 - (shift_ammount)), (Dd), (Dm)) )
#define vsri_n_s16(Dd, Dm, shift_ammount)        ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 16, "invalid shift ammount"), __neon_DdDm_acc( 0xf3800410 | _NENC_21_16(32 - (shift_ammount)), (Dd), (Dm)) )
#define vsri_n_s32(Dd, Dm, shift_ammount)        ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 32, "invalid shift ammount"), __neon_DdDm_acc( 0xf3800410 | _NENC_21_16(64 - (shift_ammount)), (Dd), (Dm)) )
#define vsri_n_s64(Dd, Dm, shift_ammount)        ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 64, "invalid shift ammount"), __neon_DdDm_acc( 0xf3800490 | _NENC_21_16(64 - (shift_ammount)), (Dd), (Dm)) )
#define vsri_n_s8(Dd, Dm, shift_ammount)         ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 8, "invalid shift ammount"), __neon_DdDm_acc( 0xf3800410 | _NENC_21_16(16 - (shift_ammount)), (Dd), (Dm)) )
#define vsri_n_u16(Dd, Dm, shift_ammount)        ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 16, "invalid shift ammount"), __neon_DdDm_acc( 0xf3800410 | _NENC_21_16(32 - (shift_ammount)), (Dd), (Dm)) )
#define vsri_n_u32(Dd, Dm, shift_ammount)        ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 32, "invalid shift ammount"), __neon_DdDm_acc( 0xf3800410 | _NENC_21_16(64 - (shift_ammount)), (Dd), (Dm)) )
#define vsri_n_u64(Dd, Dm, shift_ammount)        ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 64, "invalid shift ammount"), __neon_DdDm_acc( 0xf3800490 | _NENC_21_16(64 - (shift_ammount)), (Dd), (Dm)) )
#define vsri_n_u8(Dd, Dm, shift_ammount)         ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 8, "invalid shift ammount"), __neon_DdDm_acc( 0xf3800410 | _NENC_21_16(16 - (shift_ammount)), (Dd), (Dm)) )
#define vsriq_n_p16(Qd, Qm, shift_ammount)       ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 16, "invalid shift ammount"), __neon_QdQm_acc( 0xf3800450 | _NENC_21_16(32 - (shift_ammount)), (Qd), (Qm)) )
#define vsriq_n_p8(Qd, Qm, shift_ammount)        ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 8, "invalid shift ammount"), __neon_QdQm_acc( 0xf3800450 | _NENC_21_16(16 - (shift_ammount)), (Qd), (Qm)) )
#define vsriq_n_s16(Qd, Qm, shift_ammount)       ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 16, "invalid shift ammount"), __neon_QdQm_acc( 0xf3800450 | _NENC_21_16(32 - (shift_ammount)), (Qd), (Qm)) )
#define vsriq_n_s32(Qd, Qm, shift_ammount)       ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 32, "invalid shift ammount"), __neon_QdQm_acc( 0xf3800450 | _NENC_21_16(64 - (shift_ammount)), (Qd), (Qm)) )
#define vsriq_n_s64(Qd, Qm, shift_ammount)       ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 64, "invalid shift ammount"), __neon_QdQm_acc( 0xf38004d0 | _NENC_21_16(64 - (shift_ammount)), (Qd), (Qm)) )
#define vsriq_n_s8(Qd, Qm, shift_ammount)        ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 8, "invalid shift ammount"), __neon_QdQm_acc( 0xf3800450 | _NENC_21_16(16 - (shift_ammount)), (Qd), (Qm)) )
#define vsriq_n_u16(Qd, Qm, shift_ammount)       ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 16, "invalid shift ammount"), __neon_QdQm_acc( 0xf3800450 | _NENC_21_16(32 - (shift_ammount)), (Qd), (Qm)) )
#define vsriq_n_u32(Qd, Qm, shift_ammount)       ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 32, "invalid shift ammount"), __neon_QdQm_acc( 0xf3800450 | _NENC_21_16(64 - (shift_ammount)), (Qd), (Qm)) )
#define vsriq_n_u64(Qd, Qm, shift_ammount)       ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 64, "invalid shift ammount"), __neon_QdQm_acc( 0xf38004d0 | _NENC_21_16(64 - (shift_ammount)), (Qd), (Qm)) )
#define vsriq_n_u8(Qd, Qm, shift_ammount)        ( __static_assert((shift_ammount) >= 1 && (shift_ammount) <= 8, "invalid shift ammount"), __neon_QdQm_acc( 0xf3800450 | _NENC_21_16(16 - (shift_ammount)), (Qd), (Qm)) )

// VST1 (multiple single elements)
#define vst1_f32(pD, D)                          ( __neon_AdrD1( 0xf400078f, (pD), (D)) )
#define vst1_p16(pD, D)                          ( __neon_AdrD1( 0xf400074f, (pD), (D)) )
#define vst1_p8(pD, D)                           ( __neon_AdrD1( 0xf400070f, (pD), (D)) )
#define vst1_s16(pD, D)                          ( __neon_AdrD1( 0xf400074f, (pD), (D)) )
#define vst1_s32(pD, D)                          ( __neon_AdrD1( 0xf400078f, (pD), (D)) )
#define vst1_s64(pD, D)                          ( __neon_AdrD1( 0xf40007cf, (pD), (D)) )
#define vst1_s8(pD, D)                           ( __neon_AdrD1( 0xf400070f, (pD), (D)) )
#define vst1_u16(pD, D)                          ( __neon_AdrD1( 0xf400074f, (pD), (D)) )
#define vst1_u32(pD, D)                          ( __neon_AdrD1( 0xf400078f, (pD), (D)) )
#define vst1_u64(pD, D)                          ( __neon_AdrD1( 0xf40007cf, (pD), (D)) )
#define vst1_u8(pD, D)                           ( __neon_AdrD1( 0xf400070f, (pD), (D)) )
#define vst1_f32_ex(pD, D, align)                ( __static_assert(_NEON_ALIGN1(align) >= 0, "invalid align"), __neon_AdrD1( 0xf400078f | _NENC_5_4(_NEON_ALIGN1(align)), (pD), (D)) )
#define vst1_p16_ex(pD, D, align)                ( __static_assert(_NEON_ALIGN1(align) >= 0, "invalid align"), __neon_AdrD1( 0xf400074f | _NENC_5_4(_NEON_ALIGN1(align)), (pD), (D)) )
#define vst1_p8_ex(pD, D, align)                 ( __static_assert(_NEON_ALIGN1(align) >= 0, "invalid align"), __neon_AdrD1( 0xf400070f | _NENC_5_4(_NEON_ALIGN1(align)), (pD), (D)) )
#define vst1_s16_ex(pD, D, align)                ( __static_assert(_NEON_ALIGN1(align) >= 0, "invalid align"), __neon_AdrD1( 0xf400074f | _NENC_5_4(_NEON_ALIGN1(align)), (pD), (D)) )
#define vst1_s32_ex(pD, D, align)                ( __static_assert(_NEON_ALIGN1(align) >= 0, "invalid align"), __neon_AdrD1( 0xf400078f | _NENC_5_4(_NEON_ALIGN1(align)), (pD), (D)) )
#define vst1_s64_ex(pD, D, align)                ( __static_assert(_NEON_ALIGN1(align) >= 0, "invalid align"), __neon_AdrD1( 0xf40007cf | _NENC_5_4(_NEON_ALIGN1(align)), (pD), (D)) )
#define vst1_s8_ex(pD, D, align)                 ( __static_assert(_NEON_ALIGN1(align) >= 0, "invalid align"), __neon_AdrD1( 0xf400070f | _NENC_5_4(_NEON_ALIGN1(align)), (pD), (D)) )
#define vst1_u16_ex(pD, D, align)                ( __static_assert(_NEON_ALIGN1(align) >= 0, "invalid align"), __neon_AdrD1( 0xf400074f | _NENC_5_4(_NEON_ALIGN1(align)), (pD), (D)) )
#define vst1_u32_ex(pD, D, align)                ( __static_assert(_NEON_ALIGN1(align) >= 0, "invalid align"), __neon_AdrD1( 0xf400078f | _NENC_5_4(_NEON_ALIGN1(align)), (pD), (D)) )
#define vst1_u64_ex(pD, D, align)                ( __static_assert(_NEON_ALIGN1(align) >= 0, "invalid align"), __neon_AdrD1( 0xf40007cf | _NENC_5_4(_NEON_ALIGN1(align)), (pD), (D)) )
#define vst1_u8_ex(pD, D, align)                 ( __static_assert(_NEON_ALIGN1(align) >= 0, "invalid align"), __neon_AdrD1( 0xf400070f | _NENC_5_4(_NEON_ALIGN1(align)), (pD), (D)) )
#define vst1q_f32(pQ, Q)                         ( __neon_AdrQ1( 0xf4000a8f, (pQ), (Q)) )
#define vst1q_p16(pQ, Q)                         ( __neon_AdrQ1( 0xf4000a4f, (pQ), (Q)) )
#define vst1q_p8(pQ, Q)                          ( __neon_AdrQ1( 0xf4000a0f, (pQ), (Q)) )
#define vst1q_s16(pQ, Q)                         ( __neon_AdrQ1( 0xf4000a4f, (pQ), (Q)) )
#define vst1q_s32(pQ, Q)                         ( __neon_AdrQ1( 0xf4000a8f, (pQ), (Q)) )
#define vst1q_s64(pQ, Q)                         ( __neon_AdrQ1( 0xf4000acf, (pQ), (Q)) )
#define vst1q_s8(pQ, Q)                          ( __neon_AdrQ1( 0xf4000a0f, (pQ), (Q)) )
#define vst1q_u16(pQ, Q)                         ( __neon_AdrQ1( 0xf4000a4f, (pQ), (Q)) )
#define vst1q_u32(pQ, Q)                         ( __neon_AdrQ1( 0xf4000a8f, (pQ), (Q)) )
#define vst1q_u64(pQ, Q)                         ( __neon_AdrQ1( 0xf4000acf, (pQ), (Q)) )
#define vst1q_u8(pQ, Q)                          ( __neon_AdrQ1( 0xf4000a0f, (pQ), (Q)) )
#define vst1q_f32_ex(pQ, Q, align)               ( __static_assert(_NEON_ALIGN2(align) >= 0, "invalid align"), __neon_AdrQ1( 0xf4000a8f | _NENC_5_4(_NEON_ALIGN2(align)), (pQ), (Q)) )
#define vst1q_p16_ex(pQ, Q, align)               ( __static_assert(_NEON_ALIGN2(align) >= 0, "invalid align"), __neon_AdrQ1( 0xf4000a4f | _NENC_5_4(_NEON_ALIGN2(align)), (pQ), (Q)) )
#define vst1q_p8_ex(pQ, Q, align)                ( __static_assert(_NEON_ALIGN2(align) >= 0, "invalid align"), __neon_AdrQ1( 0xf4000a0f | _NENC_5_4(_NEON_ALIGN2(align)), (pQ), (Q)) )
#define vst1q_s16_ex(pQ, Q, align)               ( __static_assert(_NEON_ALIGN2(align) >= 0, "invalid align"), __neon_AdrQ1( 0xf4000a4f | _NENC_5_4(_NEON_ALIGN2(align)), (pQ), (Q)) )
#define vst1q_s32_ex(pQ, Q, align)               ( __static_assert(_NEON_ALIGN2(align) >= 0, "invalid align"), __neon_AdrQ1( 0xf4000a8f | _NENC_5_4(_NEON_ALIGN2(align)), (pQ), (Q)) )
#define vst1q_s64_ex(pQ, Q, align)               ( __static_assert(_NEON_ALIGN2(align) >= 0, "invalid align"), __neon_AdrQ1( 0xf4000acf | _NENC_5_4(_NEON_ALIGN2(align)), (pQ), (Q)) )
#define vst1q_s8_ex(pQ, Q, align)                ( __static_assert(_NEON_ALIGN2(align) >= 0, "invalid align"), __neon_AdrQ1( 0xf4000a0f | _NENC_5_4(_NEON_ALIGN2(align)), (pQ), (Q)) )
#define vst1q_u16_ex(pQ, Q, align)               ( __static_assert(_NEON_ALIGN2(align) >= 0, "invalid align"), __neon_AdrQ1( 0xf4000a4f | _NENC_5_4(_NEON_ALIGN2(align)), (pQ), (Q)) )
#define vst1q_u32_ex(pQ, Q, align)               ( __static_assert(_NEON_ALIGN2(align) >= 0, "invalid align"), __neon_AdrQ1( 0xf4000a8f | _NENC_5_4(_NEON_ALIGN2(align)), (pQ), (Q)) )
#define vst1q_u64_ex(pQ, Q, align)               ( __static_assert(_NEON_ALIGN2(align) >= 0, "invalid align"), __neon_AdrQ1( 0xf4000acf | _NENC_5_4(_NEON_ALIGN2(align)), (pQ), (Q)) )
#define vst1q_u8_ex(pQ, Q, align)                ( __static_assert(_NEON_ALIGN2(align) >= 0, "invalid align"), __neon_AdrQ1( 0xf4000a0f | _NENC_5_4(_NEON_ALIGN2(align)), (pQ), (Q)) )

// VST1 (single element from one lane)
#define vst1_lane_f32(pD, D, lane)               ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_AdrD1( 0xf480080f | _NENC_7(lane), (pD), (D)) )
#define vst1_lane_p16(pD, D, lane)               ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_AdrD1( 0xf480040f | _NENC_7_6(lane), (pD), (D)) )
#define vst1_lane_p8(pD, D, lane)                ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_AdrD1( 0xf480000f | _NENC_7_5(lane), (pD), (D)) )
#define vst1_lane_s16(pD, D, lane)               ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_AdrD1( 0xf480040f | _NENC_7_6(lane), (pD), (D)) )
#define vst1_lane_s32(pD, D, lane)               ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_AdrD1( 0xf480080f | _NENC_7(lane), (pD), (D)) )
#define vst1_lane_s8(pD, D, lane)                ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_AdrD1( 0xf480000f | _NENC_7_5(lane), (pD), (D)) )
#define vst1_lane_u16(pD, D, lane)               ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_AdrD1( 0xf480040f | _NENC_7_6(lane), (pD), (D)) )
#define vst1_lane_u32(pD, D, lane)               ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_AdrD1( 0xf480080f | _NENC_7(lane), (pD), (D)) )
#define vst1_lane_u8(pD, D, lane)                ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_AdrD1( 0xf480000f | _NENC_7_5(lane), (pD), (D)) )
#define vst1q_lane_f32(pQ, Q, lane)              ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_AdrQ1( 0xf480080f | _NENC_7((lane) % 2) | _NENC_12((lane) >= 2 ? 1 : 0), (pQ), (Q)) )
#define vst1q_lane_p16(pQ, Q, lane)              ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_AdrQ1( 0xf480040f | _NENC_7_6((lane) % 4) | _NENC_12((lane) >= 4 ? 1 : 0), (pQ), (Q)) )
#define vst1q_lane_p8(pQ, Q, lane)               ( __static_assert((lane) >= 0 && (lane) < 16, "invalid lane index"), __neon_AdrQ1( 0xf480000f | _NENC_7_5((lane) % 8) | _NENC_12((lane) >= 8 ? 1 : 0), (pQ), (Q)) )
#define vst1q_lane_s16(pQ, Q, lane)              ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_AdrQ1( 0xf480040f | _NENC_7_6((lane) % 4) | _NENC_12((lane) >= 4 ? 1 : 0), (pQ), (Q)) )
#define vst1q_lane_s32(pQ, Q, lane)              ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_AdrQ1( 0xf480080f | _NENC_7((lane) % 2) | _NENC_12((lane) >= 2 ? 1 : 0), (pQ), (Q)) )
#define vst1q_lane_s8(pQ, Q, lane)               ( __static_assert((lane) >= 0 && (lane) < 16, "invalid lane index"), __neon_AdrQ1( 0xf480000f | _NENC_7_5((lane) % 8) | _NENC_12((lane) >= 8 ? 1 : 0), (pQ), (Q)) )
#define vst1q_lane_u16(pQ, Q, lane)              ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_AdrQ1( 0xf480040f | _NENC_7_6((lane) % 4) | _NENC_12((lane) >= 4 ? 1 : 0), (pQ), (Q)) )
#define vst1q_lane_u32(pQ, Q, lane)              ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_AdrQ1( 0xf480080f | _NENC_7((lane) % 2) | _NENC_12((lane) >= 2 ? 1 : 0), (pQ), (Q)) )
#define vst1q_lane_u8(pQ, Q, lane)               ( __static_assert((lane) >= 0 && (lane) < 16, "invalid lane index"), __neon_AdrQ1( 0xf480000f | _NENC_7_5((lane) % 8) | _NENC_12((lane) >= 8 ? 1 : 0), (pQ), (Q)) )

// VST1 (single element from one lane, aligned)
#define vst1_lane_f32_ex(pD, D, lane)            ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_AdrD1( 0xf480083f | _NENC_7(lane), (pD), (D)) )
#define vst1_lane_p16_ex(pD, D, lane)            ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_AdrD1( 0xf480041f | _NENC_7_6(lane), (pD), (D)) )
#define vst1_lane_p8_ex(pD, D, lane)             ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_AdrD1( 0xf480000f | _NENC_7_5(lane), (pD), (D)) )
#define vst1_lane_s16_ex(pD, D, lane)            ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_AdrD1( 0xf480041f | _NENC_7_6(lane), (pD), (D)) )
#define vst1_lane_s32_ex(pD, D, lane)            ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_AdrD1( 0xf480083f | _NENC_7(lane), (pD), (D)) )
#define vst1_lane_s8_ex(pD, D, lane)             ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_AdrD1( 0xf480000f | _NENC_7_5(lane), (pD), (D)) )
#define vst1_lane_u16_ex(pD, D, lane)            ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_AdrD1( 0xf480041f | _NENC_7_6(lane), (pD), (D)) )
#define vst1_lane_u32_ex(pD, D, lane)            ( __static_assert((lane) >= 0 && (lane) < 2, "invalid lane index"), __neon_AdrD1( 0xf480083f | _NENC_7(lane), (pD), (D)) )
#define vst1_lane_u8_ex(pD, D, lane)             ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_AdrD1( 0xf480000f | _NENC_7_5(lane), (pD), (D)) )
#define vst1q_lane_f32_ex(pQ, Q, lane)           ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_AdrQ1( 0xf480083f | _NENC_7((lane) % 2) | _NENC_12((lane) >= 2 ? 1 : 0), (pQ), (Q)) )
#define vst1q_lane_p16_ex(pQ, Q, lane)           ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_AdrQ1( 0xf480041f | _NENC_7_6((lane) % 4) | _NENC_12((lane) >= 4 ? 1 : 0), (pQ), (Q)) )
#define vst1q_lane_p8_ex(pQ, Q, lane)            ( __static_assert((lane) >= 0 && (lane) < 16, "invalid lane index"), __neon_AdrQ1( 0xf480000f | _NENC_7_5((lane) % 8) | _NENC_12((lane) >= 8 ? 1 : 0), (pQ), (Q)) )
#define vst1q_lane_s16_ex(pQ, Q, lane)           ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_AdrQ1( 0xf480041f | _NENC_7_6((lane) % 4) | _NENC_12((lane) >= 4 ? 1 : 0), (pQ), (Q)) )
#define vst1q_lane_s32_ex(pQ, Q, lane)           ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_AdrQ1( 0xf480083f | _NENC_7((lane) % 2) | _NENC_12((lane) >= 2 ? 1 : 0), (pQ), (Q)) )
#define vst1q_lane_s8_ex(pQ, Q, lane)            ( __static_assert((lane) >= 0 && (lane) < 16, "invalid lane index"), __neon_AdrQ1( 0xf480000f | _NENC_7_5((lane) % 8) | _NENC_12((lane) >= 8 ? 1 : 0), (pQ), (Q)) )
#define vst1q_lane_u16_ex(pQ, Q, lane)           ( __static_assert((lane) >= 0 && (lane) < 8, "invalid lane index"), __neon_AdrQ1( 0xf480041f | _NENC_7_6((lane) % 4) | _NENC_12((lane) >= 4 ? 1 : 0), (pQ), (Q)) )
#define vst1q_lane_u32_ex(pQ, Q, lane)           ( __static_assert((lane) >= 0 && (lane) < 4, "invalid lane index"), __neon_AdrQ1( 0xf480083f | _NENC_7((lane) % 2) | _NENC_12((lane) >= 2 ? 1 : 0), (pQ), (Q)) )
#define vst1q_lane_u8_ex(pQ, Q, lane)            ( __static_assert((lane) >= 0 && (lane) < 16, "invalid lane index"), __neon_AdrQ1( 0xf480000f | _NENC_7_5((lane) % 8) | _NENC_12((lane) >= 8 ? 1 : 0), (pQ), (Q)) )

// VSUB
#define vsub_f32(Dn, Dm)                         ( __neon_DdDnDm( 0xf2200d00, (Dn), (Dm)) )
#define vsub_s16(Dn, Dm)                         ( __neon_DdDnDm( 0xf3100800, (Dn), (Dm)) )
#define vsub_s32(Dn, Dm)                         ( __neon_DdDnDm( 0xf3200800, (Dn), (Dm)) )
#define vsub_s64(Dn, Dm)                         ( __neon_DdDnDm( 0xf3300800, (Dn), (Dm)) )
#define vsub_s8(Dn, Dm)                          ( __neon_DdDnDm( 0xf3000800, (Dn), (Dm)) )
#define vsub_u16(Dn, Dm)                         ( __neon_DdDnDm( 0xf3100800, (Dn), (Dm)) )
#define vsub_u32(Dn, Dm)                         ( __neon_DdDnDm( 0xf3200800, (Dn), (Dm)) )
#define vsub_u64(Dn, Dm)                         ( __neon_DdDnDm( 0xf3300800, (Dn), (Dm)) )
#define vsub_u8(Dn, Dm)                          ( __neon_DdDnDm( 0xf3000800, (Dn), (Dm)) )
#define vsubq_f32(Qn, Qm)                        ( __neon_QdQnQm( 0xf2200d40, (Qn), (Qm)) )
#define vsubq_s16(Qn, Qm)                        ( __neon_QdQnQm( 0xf3100840, (Qn), (Qm)) )
#define vsubq_s32(Qn, Qm)                        ( __neon_QdQnQm( 0xf3200840, (Qn), (Qm)) )
#define vsubq_s64(Qn, Qm)                        ( __neon_QdQnQm( 0xf3300840, (Qn), (Qm)) )
#define vsubq_s8(Qn, Qm)                         ( __neon_QdQnQm( 0xf3000840, (Qn), (Qm)) )
#define vsubq_u16(Qn, Qm)                        ( __neon_QdQnQm( 0xf3100840, (Qn), (Qm)) )
#define vsubq_u32(Qn, Qm)                        ( __neon_QdQnQm( 0xf3200840, (Qn), (Qm)) )
#define vsubq_u64(Qn, Qm)                        ( __neon_QdQnQm( 0xf3300840, (Qn), (Qm)) )
#define vsubq_u8(Qn, Qm)                         ( __neon_QdQnQm( 0xf3000840, (Qn), (Qm)) )

// VSUBHN, VRSUBHN
#define vrsubhn_s16(Qn, Qm)                      ( __neon_DdQnQm( 0xf3800600, (Qn), (Qm)) )
#define vrsubhn_s32(Qn, Qm)                      ( __neon_DdQnQm( 0xf3900600, (Qn), (Qm)) )
#define vrsubhn_s64(Qn, Qm)                      ( __neon_DdQnQm( 0xf3a00600, (Qn), (Qm)) )
#define vrsubhn_u16(Qn, Qm)                      ( __neon_DdQnQm( 0xf3800600, (Qn), (Qm)) )
#define vrsubhn_u32(Qn, Qm)                      ( __neon_DdQnQm( 0xf3900600, (Qn), (Qm)) )
#define vrsubhn_u64(Qn, Qm)                      ( __neon_DdQnQm( 0xf3a00600, (Qn), (Qm)) )
#define vsubhn_s16(Qn, Qm)                       ( __neon_DdQnQm( 0xf2800600, (Qn), (Qm)) )
#define vsubhn_s32(Qn, Qm)                       ( __neon_DdQnQm( 0xf2900600, (Qn), (Qm)) )
#define vsubhn_s64(Qn, Qm)                       ( __neon_DdQnQm( 0xf2a00600, (Qn), (Qm)) )
#define vsubhn_u16(Qn, Qm)                       ( __neon_DdQnQm( 0xf2800600, (Qn), (Qm)) )
#define vsubhn_u32(Qn, Qm)                       ( __neon_DdQnQm( 0xf2900600, (Qn), (Qm)) )
#define vsubhn_u64(Qn, Qm)                       ( __neon_DdQnQm( 0xf2a00600, (Qn), (Qm)) )

// VSUBL, VSUBW
#define vsubl_s16(Dn, Dm)                        ( __neon_QdDnDm( 0xf2900200, (Dn), (Dm)) )
#define vsubl_s32(Dn, Dm)                        ( __neon_QdDnDm( 0xf2a00200, (Dn), (Dm)) )
#define vsubl_s8(Dn, Dm)                         ( __neon_QdDnDm( 0xf2800200, (Dn), (Dm)) )
#define vsubl_u16(Dn, Dm)                        ( __neon_QdDnDm( 0xf3900200, (Dn), (Dm)) )
#define vsubl_u32(Dn, Dm)                        ( __neon_QdDnDm( 0xf3a00200, (Dn), (Dm)) )
#define vsubl_u8(Dn, Dm)                         ( __neon_QdDnDm( 0xf3800200, (Dn), (Dm)) )
#define vsubw_s16(Qn, Dm)                        ( __neon_QdQnDm( 0xf2900300, (Qn), (Dm)) )
#define vsubw_s32(Qn, Dm)                        ( __neon_QdQnDm( 0xf2a00300, (Qn), (Dm)) )
#define vsubw_s8(Qn, Dm)                         ( __neon_QdQnDm( 0xf2800300, (Qn), (Dm)) )
#define vsubw_u16(Qn, Dm)                        ( __neon_QdQnDm( 0xf3900300, (Qn), (Dm)) )
#define vsubw_u32(Qn, Dm)                        ( __neon_QdQnDm( 0xf3a00300, (Qn), (Dm)) )
#define vsubw_u8(Qn, Dm)                         ( __neon_QdQnDm( 0xf3800300, (Qn), (Dm)) )

// VTBL, VTBX
#define vtbl1_p8(Dn, Dm)                         ( __neon_DdDnDm( 0xf3b00800, (Dn), (Dm)) )
#define vtbl1_s8(Dn, Dm)                         ( __neon_DdDnDm( 0xf3b00800, (Dn), (Dm)) )
#define vtbl1_u8(Dn, Dm)                         ( __neon_DdDnDm( 0xf3b00800, (Dn), (Dm)) )
#define vtbx1_p8(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3b00840, (Dd), (Dn), (Dm)) )
#define vtbx1_s8(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3b00840, (Dd), (Dn), (Dm)) )
#define vtbx1_u8(Dd, Dn, Dm)                     ( __neon_DdDnDm_acc( 0xf3b00840, (Dd), (Dn), (Dm)) )
#define vtbl2_p8(Qn, Dm)                         ( __neon_DdQnDm( 0xf3b00900, (Qn), (Dm)) )
#define vtbl2_s8(Qn, Dm)                         ( __neon_DdQnDm( 0xf3b00900, (Qn), (Dm)) )
#define vtbl2_u8(Qn, Dm)                         ( __neon_DdQnDm( 0xf3b00900, (Qn), (Dm)) )
#define vtbx2_p8(Dd, Qn, Dm)                     ( __neon_DdQnDm_acc( 0xf3b00940, (Dd), (Qn), (Dm)) )
#define vtbx2_s8(Dd, Qn, Dm)                     ( __neon_DdQnDm_acc( 0xf3b00940, (Dd), (Qn), (Dm)) )
#define vtbx2_u8(Dd, Qn, Dm)                     ( __neon_DdQnDm_acc( 0xf3b00940, (Dd), (Qn), (Dm)) )

// VTST
#define vtst_p8(Dn, Dm)                          ( __neon_DdDnDm( 0xf2000810, (Dn), (Dm)) )
#define vtst_s16(Dn, Dm)                         ( __neon_DdDnDm( 0xf2100810, (Dn), (Dm)) )
#define vtst_s32(Dn, Dm)                         ( __neon_DdDnDm( 0xf2200810, (Dn), (Dm)) )
#define vtst_s8(Dn, Dm)                          ( __neon_DdDnDm( 0xf2000810, (Dn), (Dm)) )
#define vtst_u16(Dn, Dm)                         ( __neon_DdDnDm( 0xf2100810, (Dn), (Dm)) )
#define vtst_u32(Dn, Dm)                         ( __neon_DdDnDm( 0xf2200810, (Dn), (Dm)) )
#define vtst_u8(Dn, Dm)                          ( __neon_DdDnDm( 0xf2000810, (Dn), (Dm)) )
#define vtstq_p8(Qn, Qm)                         ( __neon_QdQnQm( 0xf2000850, (Qn), (Qm)) )
#define vtstq_s16(Qn, Qm)                        ( __neon_QdQnQm( 0xf2100850, (Qn), (Qm)) )
#define vtstq_s32(Qn, Qm)                        ( __neon_QdQnQm( 0xf2200850, (Qn), (Qm)) )
#define vtstq_s8(Qn, Qm)                         ( __neon_QdQnQm( 0xf2000850, (Qn), (Qm)) )
#define vtstq_u16(Qn, Qm)                        ( __neon_QdQnQm( 0xf2100850, (Qn), (Qm)) )
#define vtstq_u32(Qn, Qm)                        ( __neon_QdQnQm( 0xf2200850, (Qn), (Qm)) )
#define vtstq_u8(Qn, Qm)                         ( __neon_QdQnQm( 0xf2000850, (Qn), (Qm)) )

// } +++ auto-generated code ends (Neon macros)


///////////////////////////////////////////////////////////////////////////////
//
// { +++ auto-generated code begins (vreinterpret macros)

#define vreinterpret_f32_i8(a)         (a)
#define vreinterpret_f32_i16(a)        (a)
#define vreinterpret_f32_i32(a)        (a)
#define vreinterpret_f32_i64(a)        (a)
#define vreinterpret_f32_p8(a)         (a)
#define vreinterpret_f32_p16(a)        (a)
#define vreinterpret_f32_u8(a)         (a)
#define vreinterpret_f32_u16(a)        (a)
#define vreinterpret_f32_u32(a)        (a)
#define vreinterpret_f32_u64(a)        (a)
#define vreinterpret_i8_f32(a)         (a)
#define vreinterpret_i8_i16(a)         (a)
#define vreinterpret_i8_i32(a)         (a)
#define vreinterpret_i8_i64(a)         (a)
#define vreinterpret_i8_p8(a)          (a)
#define vreinterpret_i8_p16(a)         (a)
#define vreinterpret_i8_u8(a)          (a)
#define vreinterpret_i8_u16(a)         (a)
#define vreinterpret_i8_u32(a)         (a)
#define vreinterpret_i8_u64(a)         (a)
#define vreinterpret_i16_f32(a)        (a)
#define vreinterpret_i16_i8(a)         (a)
#define vreinterpret_i16_i32(a)        (a)
#define vreinterpret_i16_i64(a)        (a)
#define vreinterpret_i16_p8(a)         (a)
#define vreinterpret_i16_p16(a)        (a)
#define vreinterpret_i16_u8(a)         (a)
#define vreinterpret_i16_u16(a)        (a)
#define vreinterpret_i16_u32(a)        (a)
#define vreinterpret_i16_u64(a)        (a)
#define vreinterpret_i32_f32(a)        (a)
#define vreinterpret_i32_i8(a)         (a)
#define vreinterpret_i32_i16(a)        (a)
#define vreinterpret_i32_i64(a)        (a)
#define vreinterpret_i32_p8(a)         (a)
#define vreinterpret_i32_p16(a)        (a)
#define vreinterpret_i32_u8(a)         (a)
#define vreinterpret_i32_u16(a)        (a)
#define vreinterpret_i32_u32(a)        (a)
#define vreinterpret_i32_u64(a)        (a)
#define vreinterpret_i64_f32(a)        (a)
#define vreinterpret_i64_i8(a)         (a)
#define vreinterpret_i64_i16(a)        (a)
#define vreinterpret_i64_i32(a)        (a)
#define vreinterpret_i64_p8(a)         (a)
#define vreinterpret_i64_p16(a)        (a)
#define vreinterpret_i64_u8(a)         (a)
#define vreinterpret_i64_u16(a)        (a)
#define vreinterpret_i64_u32(a)        (a)
#define vreinterpret_i64_u64(a)        (a)
#define vreinterpret_p8_f32(a)         (a)
#define vreinterpret_p8_i8(a)          (a)
#define vreinterpret_p8_i16(a)         (a)
#define vreinterpret_p8_i32(a)         (a)
#define vreinterpret_p8_i64(a)         (a)
#define vreinterpret_p8_p16(a)         (a)
#define vreinterpret_p8_u8(a)          (a)
#define vreinterpret_p8_u16(a)         (a)
#define vreinterpret_p8_u32(a)         (a)
#define vreinterpret_p8_u64(a)         (a)
#define vreinterpret_p16_f32(a)        (a)
#define vreinterpret_p16_i8(a)         (a)
#define vreinterpret_p16_i16(a)        (a)
#define vreinterpret_p16_i32(a)        (a)
#define vreinterpret_p16_i64(a)        (a)
#define vreinterpret_p16_p8(a)         (a)
#define vreinterpret_p16_u8(a)         (a)
#define vreinterpret_p16_u16(a)        (a)
#define vreinterpret_p16_u32(a)        (a)
#define vreinterpret_p16_u64(a)        (a)
#define vreinterpret_u8_f32(a)         (a)
#define vreinterpret_u8_i8(a)          (a)
#define vreinterpret_u8_i16(a)         (a)
#define vreinterpret_u8_i32(a)         (a)
#define vreinterpret_u8_i64(a)         (a)
#define vreinterpret_u8_p8(a)          (a)
#define vreinterpret_u8_p16(a)         (a)
#define vreinterpret_u8_u16(a)         (a)
#define vreinterpret_u8_u32(a)         (a)
#define vreinterpret_u8_u64(a)         (a)
#define vreinterpret_u16_f32(a)        (a)
#define vreinterpret_u16_i8(a)         (a)
#define vreinterpret_u16_i16(a)        (a)
#define vreinterpret_u16_i32(a)        (a)
#define vreinterpret_u16_i64(a)        (a)
#define vreinterpret_u16_p8(a)         (a)
#define vreinterpret_u16_p16(a)        (a)
#define vreinterpret_u16_u8(a)         (a)
#define vreinterpret_u16_u32(a)        (a)
#define vreinterpret_u16_u64(a)        (a)
#define vreinterpret_u32_f32(a)        (a)
#define vreinterpret_u32_i8(a)         (a)
#define vreinterpret_u32_i16(a)        (a)
#define vreinterpret_u32_i32(a)        (a)
#define vreinterpret_u32_i64(a)        (a)
#define vreinterpret_u32_p8(a)         (a)
#define vreinterpret_u32_p16(a)        (a)
#define vreinterpret_u32_u8(a)         (a)
#define vreinterpret_u32_u16(a)        (a)
#define vreinterpret_u32_u64(a)        (a)
#define vreinterpret_u64_f32(a)        (a)
#define vreinterpret_u64_i8(a)         (a)
#define vreinterpret_u64_i16(a)        (a)
#define vreinterpret_u64_i32(a)        (a)
#define vreinterpret_u64_i64(a)        (a)
#define vreinterpret_u64_p8(a)         (a)
#define vreinterpret_u64_p16(a)        (a)
#define vreinterpret_u64_u8(a)         (a)
#define vreinterpret_u64_u16(a)        (a)
#define vreinterpret_u64_u32(a)        (a)
#define vreinterpretq_f32_i8(a)        (a)
#define vreinterpretq_f32_i16(a)       (a)
#define vreinterpretq_f32_i32(a)       (a)
#define vreinterpretq_f32_i64(a)       (a)
#define vreinterpretq_f32_p8(a)        (a)
#define vreinterpretq_f32_p16(a)       (a)
#define vreinterpretq_f32_u8(a)        (a)
#define vreinterpretq_f32_u16(a)       (a)
#define vreinterpretq_f32_u32(a)       (a)
#define vreinterpretq_f32_u64(a)       (a)
#define vreinterpretq_i8_f32(a)        (a)
#define vreinterpretq_i8_i16(a)        (a)
#define vreinterpretq_i8_i32(a)        (a)
#define vreinterpretq_i8_i64(a)        (a)
#define vreinterpretq_i8_p8(a)         (a)
#define vreinterpretq_i8_p16(a)        (a)
#define vreinterpretq_i8_u8(a)         (a)
#define vreinterpretq_i8_u16(a)        (a)
#define vreinterpretq_i8_u32(a)        (a)
#define vreinterpretq_i8_u64(a)        (a)
#define vreinterpretq_i16_f32(a)       (a)
#define vreinterpretq_i16_i8(a)        (a)
#define vreinterpretq_i16_i32(a)       (a)
#define vreinterpretq_i16_i64(a)       (a)
#define vreinterpretq_i16_p8(a)        (a)
#define vreinterpretq_i16_p16(a)       (a)
#define vreinterpretq_i16_u8(a)        (a)
#define vreinterpretq_i16_u16(a)       (a)
#define vreinterpretq_i16_u32(a)       (a)
#define vreinterpretq_i16_u64(a)       (a)
#define vreinterpretq_i32_f32(a)       (a)
#define vreinterpretq_i32_i8(a)        (a)
#define vreinterpretq_i32_i16(a)       (a)
#define vreinterpretq_i32_i64(a)       (a)
#define vreinterpretq_i32_p8(a)        (a)
#define vreinterpretq_i32_p16(a)       (a)
#define vreinterpretq_i32_u8(a)        (a)
#define vreinterpretq_i32_u16(a)       (a)
#define vreinterpretq_i32_u32(a)       (a)
#define vreinterpretq_i32_u64(a)       (a)
#define vreinterpretq_i64_f32(a)       (a)
#define vreinterpretq_i64_i8(a)        (a)
#define vreinterpretq_i64_i16(a)       (a)
#define vreinterpretq_i64_i32(a)       (a)
#define vreinterpretq_i64_p8(a)        (a)
#define vreinterpretq_i64_p16(a)       (a)
#define vreinterpretq_i64_u8(a)        (a)
#define vreinterpretq_i64_u16(a)       (a)
#define vreinterpretq_i64_u32(a)       (a)
#define vreinterpretq_i64_u64(a)       (a)
#define vreinterpretq_p8_f32(a)        (a)
#define vreinterpretq_p8_i8(a)         (a)
#define vreinterpretq_p8_i16(a)        (a)
#define vreinterpretq_p8_i32(a)        (a)
#define vreinterpretq_p8_i64(a)        (a)
#define vreinterpretq_p8_p16(a)        (a)
#define vreinterpretq_p8_u8(a)         (a)
#define vreinterpretq_p8_u16(a)        (a)
#define vreinterpretq_p8_u32(a)        (a)
#define vreinterpretq_p8_u64(a)        (a)
#define vreinterpretq_p16_f32(a)       (a)
#define vreinterpretq_p16_i8(a)        (a)
#define vreinterpretq_p16_i16(a)       (a)
#define vreinterpretq_p16_i32(a)       (a)
#define vreinterpretq_p16_i64(a)       (a)
#define vreinterpretq_p16_p8(a)        (a)
#define vreinterpretq_p16_u8(a)        (a)
#define vreinterpretq_p16_u16(a)       (a)
#define vreinterpretq_p16_u32(a)       (a)
#define vreinterpretq_p16_u64(a)       (a)
#define vreinterpretq_u8_f32(a)        (a)
#define vreinterpretq_u8_i8(a)         (a)
#define vreinterpretq_u8_i16(a)        (a)
#define vreinterpretq_u8_i32(a)        (a)
#define vreinterpretq_u8_i64(a)        (a)
#define vreinterpretq_u8_p8(a)         (a)
#define vreinterpretq_u8_p16(a)        (a)
#define vreinterpretq_u8_u16(a)        (a)
#define vreinterpretq_u8_u32(a)        (a)
#define vreinterpretq_u8_u64(a)        (a)
#define vreinterpretq_u16_f32(a)       (a)
#define vreinterpretq_u16_i8(a)        (a)
#define vreinterpretq_u16_i16(a)       (a)
#define vreinterpretq_u16_i32(a)       (a)
#define vreinterpretq_u16_i64(a)       (a)
#define vreinterpretq_u16_p8(a)        (a)
#define vreinterpretq_u16_p16(a)       (a)
#define vreinterpretq_u16_u8(a)        (a)
#define vreinterpretq_u16_u32(a)       (a)
#define vreinterpretq_u16_u64(a)       (a)
#define vreinterpretq_u32_f32(a)       (a)
#define vreinterpretq_u32_i8(a)        (a)
#define vreinterpretq_u32_i16(a)       (a)
#define vreinterpretq_u32_i32(a)       (a)
#define vreinterpretq_u32_i64(a)       (a)
#define vreinterpretq_u32_p8(a)        (a)
#define vreinterpretq_u32_p16(a)       (a)
#define vreinterpretq_u32_u8(a)        (a)
#define vreinterpretq_u32_u16(a)       (a)
#define vreinterpretq_u32_u64(a)       (a)
#define vreinterpretq_u64_f32(a)       (a)
#define vreinterpretq_u64_i8(a)        (a)
#define vreinterpretq_u64_i16(a)       (a)
#define vreinterpretq_u64_i32(a)       (a)
#define vreinterpretq_u64_i64(a)       (a)
#define vreinterpretq_u64_p8(a)        (a)
#define vreinterpretq_u64_p16(a)       (a)
#define vreinterpretq_u64_u8(a)        (a)
#define vreinterpretq_u64_u16(a)       (a)
#define vreinterpretq_u64_u32(a)       (a)

// } +++ auto-generated code ends (vreinterpret macros)


